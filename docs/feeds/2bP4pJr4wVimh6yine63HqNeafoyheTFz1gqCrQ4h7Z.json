{"id":"2bP4pJr4wVimh6yine63HqNeafoyheTFz1gqCrQ4h7Z","title":"AlgoMaster Newsletter","displayTitle":"Dev - Algomaster","url":"https://blog.algomaster.io/feed","feedLink":"https://blog.algomaster.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":8,"items":[{"title":"Design a Web Crawler - System Design Interview","url":"https://blog.algomaster.io/p/design-a-web-crawler-system-design-interview","date":1749358563,"author":"Ashish Pratap Singh","guid":710,"unread":true,"content":"<p>A  (also known as a ) is an automated bot that systematically browses the internet, following links from page to page to discover and collect web content.</p><p>Traditionally, web crawlers have been used by  to discover and index web pages. In recent years, they’ve also become essential for training <strong>large language models (LLMs)</strong> by collecting massive amounts of publicly available text data from across the internet.</p><p>At its core, crawling seems simple:</p><ol><li><p>Start with a list of known URLs (called )</p></li></ol><p>However, designing a crawler that can operate at , processing billions or even trillions of pages, is anything but simple. It introduces several complex engineering challenges like:</p><ul><li><p>How do we prioritize which pages to crawl first?</p></li><li><p>How do we ensure we don’t overload the target servers?</p></li><li><p>How do we avoid redundant crawling of the same URL or content?</p></li><li><p>How do we split the work across hundreds or thousands of crawler nodes?</p></li></ul><p>In this article, we’ll walk through the end-to-end design of a <strong>scalable, distributed web crawler</strong>. We’ll start with the requirements, map out the high-level architecture, explore database and storage options, and dive deep into the core components.</p><p>Before we start drawing boxes and arrows, let's define what our crawler needs to do.</p><h3>1.1 Functional Requirements</h3><ol><li><p> Given a URL, the crawler should be able to download the corresponding content.</p></li><li><p> Save the fetched content for downstream use.</p></li><li><p> Parse the HTML to discover hyperlinks and identify new URLs to crawl.</p></li><li><p> Prevent redundant crawling and storage of the same URL or content. Both URL-level and content-level deduplication should be supported.</p></li><li><p> Follow site-specific crawling rules defined in  files, including disallowed paths and crawl delays.</p></li><li><p><strong>Handle Diverse Content Types:</strong> Support HTML as a primary format, but also be capable of recognizing and handling other formats such as PDFs, XML, images, and scripts.</p></li><li><p> Support recrawling of pages based on content volatility. Frequently updated pages should be revisited more often than static ones.</p></li></ol><h3>1.2 Non-Functional Requirements</h3><ol><li><p> The system should scale horizontally to crawl billions of pages across a large number of domains.</p></li><li><p> The crawler should avoid overwhelming target servers by limiting the rate of requests to each domain.</p></li><li><p> The architecture should allow for easy integration of new modules, such as custom parsers, content filters, storage backends, or processing pipelines.</p></li><li><p><strong>Robustness &amp; Fault Tolerance:</strong> The crawler should gracefully handle failures whether it's a bad URL, a timeout, or a crashing worker node without disrupting the overall system.</p></li><li><p> The crawler should maintain high throughput (pages per second), while also minimizing fetch latency.</p></li></ol><blockquote><p>In a real system design interview, you may only be expected to address a subset of these requirements. Focus on what’s relevant to the problem you’re asked to solve, and clarify assumptions early in the discussion.</p></blockquote><h3>2.1 Number of Pages to Crawl</h3><p>Assume we aim to crawl a subset of the web, not the entire internet, but a meaningful slice. This includes pages across blogs, news sites, e-commerce platforms, documentation pages, and forums.</p><ul><li><p><strong>Additional Metadata (headers, timestamps, etc.)</strong>: ~10 KB</p></li></ul><blockquote><p><strong>Total Data Volume = 1 billion pages × 110 KB = ~110 TB</strong></p></blockquote><p>This estimate covers only the raw HTML and metadata. If we store additional data like structured metadata, embedded files, or full-text search indexes, the storage requirements could grow meaningfully.</p><p>Let’s assume we want to complete the crawl in .</p><ul><li><p> = 1 billion / 10 ≈ </p></li><li><p> ≈ 1150 pages/sec</p></li></ul><blockquote><p> 110 KB/page × 1150 pages/sec = ~126 MB/sec</p></blockquote><p>This means our system must be capable of:</p><ul><li><p>Making over <strong>1150 HTTP requests per second</strong></p></li><li><p>Parsing and storing content at the same rate</p></li></ul><p>Every page typically contains several outbound links, many of which are unique. This causes the  (queue of URLs to visit) to grow rapidly. </p><ul><li><p><strong>Average outbound links per page:</strong> 5</p></li><li><p><strong>New links discovered per second = </strong> 1150 (pages per second) * 5 = 5750</p></li></ul><p>The URL Frontier's needs to handle thousands of new URL submissions per second. We’ll need efficient , , and  to handle this at scale.</p><p>Lets start the . Later, we’ll dive into the internals of each module.</p><p>Let’s break it down component by component:</p>","contentLength":4155,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c560e48-2f72-4659-9f69-f4269895b979_2452x1640.png","enclosureMime":"","commentsUrl":null},{"title":"Strong vs. Eventual Consistency","url":"https://blog.algomaster.io/p/strong-vs-eventual-consistency","date":1748924813,"author":"Ashish Pratap Singh","guid":709,"unread":true,"content":"<p>In today’s distributed systems, data is almost never stored in a single place. It’s replicated across , often spread across <strong>data centers around the world</strong> to ensure high availability, fault tolerance, and performance.</p><p>This global distribution enables scalability but it comes with a critical challenge:</p><blockquote><p><strong>How do we ensure every user and system component sees a consistent and accurate view of the data, especially when updates are happening all the time?</strong></p></blockquote><p>This is where  come into play. They define the rules for how and when changes to data become visible across the system.</p><p>Two of the most widely discussed models are:</p><ul><li><p> – where everyone sees the latest value, always.</p></li><li><p> – where updates eventually propagate, but temporary inconsistencies are allowed.</p></li></ul><p>In this article, we'll break down these two models, explore their trade-offs, when to use each, and how to choose the right model depending on your application's goals.</p><h2>What is Data Consistency in Distributed System?</h2><p>In a , consistency is straightforward: when you write data, any subsequent read returns the most recent value. There’s only one copy of the data.</p><p>But in , where data is replicated across multiple nodes (for <strong>availability, fault tolerance, or low latency)</strong>, things become more complex.</p><p>Imagine you write an update to . That change takes time to  to  and . During this , different nodes may temporarily hold  of the same data.</p><p>Consistency Models define the  a system provides about:</p><ul><li><p> an update becomes visible</p></li><li><p> it becomes visible (to which users or replicas)</p></li><li><p> operations are seen across the system</p></li></ul><p>In essence, consistency models answer the question:</p><blockquote><p><strong>“If I write a piece of data, when and under what conditions will others (or even I) see that new value?”</strong></p></blockquote><p> guarantees that once a write is successfully completed, <strong>any read operation from any client or replica will reflect that write or a newer one</strong>. </p><p>This means that the system always returns the most up-to-date value, regardless of where the read is performed.</p><p>It behaves as if there is a <strong>single, globally synchronized copy of the data</strong>, and all operations occur in a clear and consistent global order.</p><p>To achieve strong consistency, the system performs <strong>coordinated communication between replicas</strong> before confirming a write:</p><ol><li><p>A client sends a write request, such as updating a value in the database.</p></li><li><p>The primary node (e.g., Node A) propagates this write to a group of replicas.</p></li><li><p>Each replica (e.g., Node B, Node C) applies the write and sends an acknowledgment.</p></li><li><p>Only after receiving acknowledgments from all (or enough) replicas does the system confirm the write as complete to the client.</p></li><li><p>From that point onward, any read will return the updated value.</p></li></ol><p>This behavior is made possible through  that ensure all replicas agree on the order of operations. Common protocols used include Paxos and Raft.</p><ul><li><p><strong>Simpler Application Logic</strong>: You don’t need to worry about stale data or implementing custom conflict resolution.</p></li><li><p>: Easy to reason about; data reads always reflect the latest confirmed writes.</p></li><li><p>: Ensures the highest level of data integrity and reliability.</p></li></ul><ul><li><p>: Writes (and sometimes reads) may be slower because they require coordination between nodes, often across regions.</p></li><li><p>: During network partitions or node failures, the system may reject requests to preserve consistency (as per the ).</p></li><li><p>: Implementing strong consistency at scale requires sophisticated protocols and distributed coordination mechanisms.</p></li></ul><h3>When to Use Strong Consistency</h3><p>Strong consistency is the right choice when your application needs <strong>immediate correctness and absolute accuracy</strong>. </p><p>It is especially important when inconsistencies could result in lost data, incorrect decisions, or broken trust such as:</p><ul><li><p><strong>Banking and financial systems</strong>: Balances and transactions must always be accurate.</p></li><li><p>: You can’t afford to sell the same item to two people.</p></li><li><p>: Locks must be exclusive and reflect the latest state.</p></li><li><p>: Duplicate IDs must be prevented across replicas.</p></li></ul><p> is a  that guarantees all replicas in a distributed system will <strong>converge to the same value</strong>,  as long as no new updates are made.</p><blockquote><p>If you stop writing to a piece of data, and wait long enough, <strong>everyone will eventually see the same (latest) version of that data.</strong></p></blockquote><p>There’s <strong>no guarantee about how soon</strong> this convergence happens. In the meantime, different replicas may serve  of the data leading to temporary inconsistencies.</p><p>This model is often used in distributed systems where  and  are prioritized over immediate consistency.</p><ol><li><p>A client sends a  to a node, such as updating a data item on Node A.</p></li><li><p>Node A <strong>acknowledges the write immediately</strong>, without waiting for other replicas to be updated.</p></li><li><p>The updated value is <strong>asynchronously propagated</strong> to other replicas, such as Node B and Node C.</p></li><li><p>While this propagation is in progress, <strong>reads from other replicas</strong> may return stale or outdated data.</p></li><li><p>Once all replicas have received and applied the update, the system is .</p></li></ol><p>This replication process allows the system to <strong>stay highly available and responsive</strong>, even in the presence of network partitions or server failures.</p><p>A temporary inconsistency is acceptable in many scenarios, especially when the data being read is <strong>not critical to business correctness</strong> and <strong>a slight delay in consistency does not harm the user experience</strong>.</p><p>Let’s say you're using a social media platform and you update your profile picture.</p><ul><li><p>You immediately see the new photo on your profile.</p></li><li><p>A friend on the other side of the world might still see your old photo for a few seconds due to .</p></li><li><p>After the system finishes syncing, everyone sees the updated photo.</p></li></ul><p>This temporary inconsistency is acceptable because <strong>the correctness of the system doesn't depend on everyone seeing the same thing at the exact same moment</strong>.</p><ul><li><p>: Writes don’t need to wait for global coordination; responses are fast.</p></li><li><p>: Nodes can accept reads and writes independently even during network partitions.</p></li><li><p>: Ideal for large-scale, globally distributed systems that need to stay responsive under heavy load.</p></li></ul><ul><li><p>Clients may read outdated data until replicas are fully synchronized.</p></li><li><p><strong>Increased Application Complexity</strong>: Developers must handle inconsistency in code especially in read-after-write scenarios.</p></li><li><p>: If multiple replicas accept concurrent writes, you need a conflict resolution strategy (e.g., Last Write Wins, CRDTs).</p></li></ul><h3>When to Use Eventual Consistency</h3><p>Eventual consistency is a good fit for <strong>applications that require high availability and can tolerate temporary inconsistencies</strong>. It is especially effective when systems are distributed globally or need to operate at massive scale.</p><p>Common use cases include:</p><ul><li><p> Like counts, shares, and view counters can tolerate brief inconsistencies without impacting user experience.</p></li><li><p><strong>Product view counts or analytics: </strong>Tracking page visits, clicks, or user events can tolerate minor delays in consistency.</p></li><li><p>Personalized suggestions such as products, movies, or articles do not require real-time accuracy and benefit from fast, large-scale reads.</p></li><li><p><strong>DNS (Domain Name System): </strong>DNS records are cached at multiple layers worldwide. Slight delays in propagation are acceptable and help maintain global availability.</p></li><li><p><strong>Content delivery networks (CDNs): </strong>Static assets like images, stylesheets, and scripts are served from edge locations. Updates propagate gradually to balance performance and consistency.</p></li><li><p> Eventual consistency is suitable when users add items from different devices. Conflict resolution strategies (such as merging or last-write-wins) help maintain a coherent experience.</p></li></ul><h2>Variations and Client-Centric Models</h2><p>It's important to note that \"eventual consistency\" is a broad term. There are stronger forms of eventual consistency that provide better guarantees:</p><ul><li><p> If operation A causally precedes operation B (e.g., B reads a value written by A), then all processes see A before B. Operations that are not causally related can be seen in different orders.</p><ul><li><p>In a comment thread, replies should appear after the comment they respond to even if the system is eventually consistent overall.</p></li></ul></li><li><p><strong>Read-Your-Writes Consistency:</strong> After a client performs a write, any subsequent reads by that same client will always reflect the write (or a newer version). Other clients may still see stale data.</p><ul><li><p>You update your profile bio. When you refresh the page, your new bio appears immediately even if it takes a few seconds for others to see it.</p></li></ul></li><li><p><strong>Monotonic Reads Consistency:</strong> If a client reads a value, any future reads by the same client will return the same or a newer value. The client will never see an older version of the data.</p><ul><li><p>You see your post has 10 likes. After refreshing, you might see 10 or 11 likes but never fewer than 10.</p></li></ul></li><li><p><strong>Monotonic Writes Consistency:</strong> Writes from the same client are executed in the order they were issued by that client.</p><ul><li><p>You post two comments: “Hello” followed by “World.” Other users will always see “Hello” before “World.”</p></li></ul></li></ul><p>Even in an <strong>eventually consistent system</strong>, applying <strong>client-centric guarantees</strong> helps preserve a sense of order, responsiveness, and trust for individual users.</p><h2>Choosing the Right Consistency Model</h2><p>There's no \"best\" consistency model. The right choice depends heavily on your application's specific requirements:</p><p><strong>How critical is it that all users see the most up-to-date, correct data at all times?</strong></p><ul><li><p>High (e.g., financial transactions, inventory management)</p></li><li><p>Lower (e.g., social likes, view counts, analytics)</p><p>→ <strong>Eventual consistency is often sufficient.</strong></p></li></ul><h4>2. User Experience Expectations:</h4><p><strong>Will users notice or care about stale data? Can the UI manage it gracefully?</strong></p><p>For systems where users expect immediate feedback, you can often use:</p><ul><li><p> updates (e.g., show the change immediately, then sync)</p></li><li><p> (e.g., “Syncing…” or “Updating…”)</p></li></ul><p> reduces confusion, but  can be acceptable with the right UX design.</p><h4>3. Performance (Latency) Requirements</h4><p><strong>How important is low latency for reads and writes?</strong></p><ul><li><p> often delivers , since writes don’t block on global coordination.</p></li><li><p> adds overhead especially in geo-distributed systems.</p></li></ul><h4>4. Availability Requirements</h4><p><strong>Can the system tolerate downtime or errors during network partitions?</strong></p><ul><li><p> may sacrifice  to preserve correctness (as per the <a href=\"https://blog.algomaster.io/p/cap-theorem-explained\">CAP theorem</a>).</p></li><li><p> allows systems to  even during partial failures or network splits.</p></li></ul><p><strong>Does the system need to support massive scale across regions or data centers?</strong></p><ul><li><p><strong>Eventually consistent systems</strong> scale more easily especially for .</p></li><li><p> may limit scaling options due to coordination overhead.</p></li></ul><h4>6. Development Complexity</h4><p><strong>How much complexity are you willing to handle at the application layer?</strong></p><ul><li><p> simplifies application logic. Developers can assume they’re always reading the latest state.</p></li><li><p> requires handling: stale data, idempotent operations and conflict resolution.</p></li></ul><p><strong>What happens if two users write conflicting data at the same time?</strong></p><p>In <strong>eventually consistent systems</strong>, concurrent writes to different replicas must be reconciled when replicas sync.</p><ul><li><p>CRDTs (Conflict-Free Replicated Data Types)</p></li><li><p>Custom merge logic based on application semantics</p></li></ul><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/strong-vs-eventual-consistency?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":11141,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fee02cd-828c-4f69-8184-401751cf819c_2070x1398.png","enclosureMime":"","commentsUrl":null},{"title":"Top 10 WebSocket Use Cases in System Design","url":"https://blog.algomaster.io/p/websocket-use-cases-system-design","date":1748524411,"author":"Ashish Pratap Singh","guid":708,"unread":true,"content":"<p>Real-time features are everywhere—chat apps, live dashboards, collaborative editors, multiplayer games. Behind the scenes, one technology powers these seamless interactions: .</p><p>Unlike traditional request-response models, WebSockets enable <strong>full-duplex, low-latency communication</strong> over a single persistent connection. This makes them the ideal choice when both the client and server need to exchange data continuously and without delay.</p><p>In this article, we’ll explore the <strong>top 10 real-world use cases of WebSockets</strong>, many of which are also relevant for .</p><blockquote><p> Each use case includes a simplified system design overview. To stay focused on WebSockets, we'll cover only the components involved in enabling real-time features rather than the full end-to-end system.</p></blockquote><p>In real-time chat applications, users expect —whether it's new messages, typing indicators, delivery receipts, or presence status.</p><p>But delivering this seamless experience at scale is , especially with  and <strong>thousands of concurrent group chats</strong>.</p><p> the server every few seconds to check for new messages isn’t efficient due to: </p><ul><li><p>Wasted bandwidth when no new data is available</p></li><li><p>Server overload, as every client repeatedly hits the backend</p></li></ul><p> solve this by maintaining a <strong>persistent, full-duplex connection</strong> between client and server, enabling:</p><ul><li><p>Real-time typing and presence updates</p></li><li><p>A single connection for sending and receiving, eliminating repeated HTTP overhead</p></li></ul><ul><li><p>: Maintains persistent connections with clients, handles incoming events and messages, and delivers real-time updates to users.</p></li><li><p>: Keeps track of active WebSocket sessions and maps users to their current connections.</p></li><li><p>: Temporarily buffers incoming messages for asynchronous processing, enabling system decoupling and ensuring reliable persistence.</p></li><li><p>: Consumes messages from the queue, attaches relevant metadata, and stores them in the message database.</p></li><li><p>: A durable and scalable database that holds all messages, metadata, and conversation history.</p></li></ul><h4>1. Client Establishes WebSocket Connection</h4><p>When a user opens the app:</p><ul><li><p>The client authenticates with the backend using a </p></li><li><p>Then establishes a  to a chat server:</p><pre><code>{\n  \"action\": \"connect\",\n  \"userId\": \"user-123\",\n  \"device\": \"ios\"\n}</code></pre></li><li><p>This connection remains open for the entire session</p></li></ul><p>When the user sends a message:</p><ul><li><p>The client sends it over the WebSocket connection:</p><pre><code>{\n  \"type\": \"message\",\n  \"conversationId\": \"conv-456\",\n  \"from\": \"user-123\",\n  \"to\": \"user-789\",\n  \"text\": \"Hey! How’s it going?\",\n  \"timestamp\": 1716601000\n}</code></pre></li><li><ul><li><p>Pushes it to a </p></li></ul></li></ul><ul><li><p>The chat server checks the  to find active WebSocket sessions</p></li><li><p>If the user is online, the message is pushed instantly:</p><pre><code>{\n  \"type\": \"incoming_message\",\n  \"conversationId\": \"conv-456\",\n  \"from\": \"user-123\",\n  \"text\": \"Hey! How’s it going?\",\n  \"timestamp\": 1716601000\n}</code></pre></li></ul><ul><li><ul><li><p>The message is stored for delivery upon reconnect</p></li><li><p>Or sent as a </p></li></ul></li></ul><p>The <strong>same WebSocket connection</strong> used for messaging can also be leveraged to send  like typing indicators and presence status.</p><p>When a user begins typing in a conversation:</p><ul><li><p>The client sends a lightweight event over the open WebSocket connection:</p><pre><code>{\n  \"type\": \"typing\",\n  \"conversationId\": \"conv-456\",\n  \"userId\": \"user-123\"\n}</code></pre></li><li><p>The server receives the event and checks the  to determine if the recipient(s) are currently online.</p></li><li><p>If they are online, the typing event is  to their active WebSocket sessions.</p></li><li><p>If they are offline, the event is discarded (not stored), since it's ephemeral and has no long-term value.</p></li></ul><p>Typing events are typically throttled (e.g., once every few seconds) to avoid flooding the system.</p><p>In fast-paced online multiplayer games, <strong>every millisecond matters. </strong></p><ul><li><p> across all players</p></li></ul><p>If you rely on HTTP polling where clients keep asking the server for updates, you introduce:</p><ul><li><p>: Updates arrive late.</p></li><li><p>: One player sees a different state than another.</p></li><li><p>: Too many requests = stressed servers.</p></li></ul><p> allow game clients and servers to send frequent updates (player positions, moves, game events) in both directions. This keeps all players’ views in sync during gameplay.</p><ul><li><p>Maintain persistent, low-latency connections with players to send and receive real-time game events.</p></li><li><p>Tracks which players are connected, which match they belong to, and which WebSocket server is managing their session.</p></li><li><p>Runs the core game logic including input processing, physics, and rules. Ensures a consistent, authoritative game state.</p></li><li><p>Holds the live, up-to-date snapshot of all game entities (e.g., players, objects, projectiles) for a specific match, updated on every game tick.</p></li></ul><p>Each active game instance (e.g., match, room, arena) is represented as a  identified by a session ID.</p><ul></ul><h4>2. Client Input Streaming</h4><ul><li><p>The client establishes a <strong>persistent WebSocket connection</strong> to one of the WebSocket servers.</p></li><li><p>Sends a  message with its  and session info:</p><pre><code>{\n  \"action\": \"join\",\n  \"playerId\": \"p789\",\n  \"session\": \"arena:1234\"\n}</code></pre></li><li><p>From that point on, the client <strong>streams user input events</strong> (movement, actions, abilities) every :</p><pre><code>{\n  \"type\": \"input\",\n  \"playerId\": \"p789\",\n  \"input\": {\n    \"moveX\": 1,\n    \"moveY\": 0,\n    \"shoot\": true\n  },\n  \"timestamp\": 1716600000\n}</code></pre></li></ul><p>This approach ensures the server receives a continuous stream of player actions in real-time.</p><h4>3. Game Match Server and State Propagation</h4><p>Each session is managed by an <strong>authoritative Game Match Server</strong> that runs a  at a fixed tick rate—typically .</p><p><strong>On each tick, the server:</strong></p><ul><li><p>Collects and queues all input events received from connected players</p></li><li><p>Processes core game logic, including:</p><ul><li><p>Movement and velocity updates</p></li><li><p>Health and damage calculations</p></li></ul></li><li><p>Applies physics and collision detection</p></li><li><p>Updates the in-memory game state</p></li><li><p>Computes a  and sends it to all WebSocket servers managing players in that session:</p><pre><code>{\n  \"type\": \"stateUpdate\",\n  \"tick\": 1441,\n  \"players\": [\n    { \"id\": \"p789\", \"x\": 55, \"y\": 89, \"health\": 92 },\n    { \"id\": \"p231\", \"x\": 52, \"y\": 91, \"health\": 80 }\n  ],\n  \"bullets\": [...],\n  \"timestamp\": 1716600012\n}</code></pre></li></ul><p>WebSocket nodes forward these updates to the connected clients.</p><p>Clients use these updates to  and .</p><p>With efficient engineering practices such as:</p><ul><li><p> and  (e.g., Node.js, Netty)</p></li><li><p> (e.g., quadtrees or grid-based zoning) to broadcast updates only to nearby players</p></li><li><p> to reduce payload size</p></li></ul><p>...a single WebSocket server can support <strong>thousands of concurrent players</strong> across multiple game sessions with .</p><p>On modern social media platforms, users expect to see <strong>new posts, likes, comments, and alerts the moment they happen</strong>.</p><p>But without a real-time mechanism, the client must constantly <strong>poll the server every few seconds</strong> to check for updates. This results in:</p><ul><li><p>High latency for the user (delayed updates)</p></li><li><p>Unnecessary load on servers and wasted bandwidth</p></li></ul><p> enable servers to push these updates instantaneously to connected clients, creating a live feed experience.</p><p>To support <strong>millions of concurrent users</strong>, platforms implement:</p><ul><li><p><strong>Dedicated WebSocket Servers</strong> to manage persistent connections</p></li><li><p> via load balancers (ensuring a user reconnects to the same server)</p></li></ul><ul><li><p><strong>Event Source (Like/Comment): </strong>Captures user interactions (e.g., likes, comments, follows) and generates structured events.</p></li><li><p>Buffers and routes events asynchronously to downstream services for processing (e.g., Kafka, Redis Streams).</p></li><li><p>Consumes events from the queue, determines target users, and routes updates to the appropriate WebSocket servers.</p></li><li><p>Maintain persistent client connections and deliver real-time feed updates to online users.</p></li><li><p>Tracks which users are connected and maps them to their corresponding WebSocket server nodes.</p></li></ul><p>Each user is assigned a <strong>dedicated feed and notification channel</strong>, enabling personalized real-time updates.</p><ul><li><p> – main activity feed</p></li><li><p> – direct notifications (likes, comments, follows)</p></li></ul><p>The system may also support <strong>shared or topic-based channels</strong>:</p><ul><li><p> – AI-related posts</p></li><li><p> – tech hashtag activity</p></li><li><p> – group or community feed</p></li></ul><h4>2. WebSocket Connection &amp; Subscription Flow</h4><p>When a user opens the app:</p><ul><li><p>The client establishes a  to the WebSocket Gateway.</p></li><li><p>After authentication, it sends a  message to specify its feed channels:</p><pre><code>{\n  \"action\": \"subscribe\",\n  \"channels\": [\n    \"feed:user:alice\",\n    \"notifications:user:alice\"\n  ]\n}</code></pre></li><li><p>The WebSocket server registers these channel subscriptions in a , typically in an in-memory registry or <strong>a distributed cache (e.g., Redis).</strong></p></li></ul><h4>3. Event Generation &amp; Push</h4><p>When a new event occurs such as a comment on a user’s post:</p><ul><li><p>The backend service (e.g., ) emits the event:</p><pre><code>{\n  \"event\": \"new_comment\",\n  \"postId\": \"abc123\",\n  \"commentId\": \"xyz789\",\n  \"byUser\": \"alice\",\n  \"toUser\": \"bob\"\n}</code></pre></li></ul><ul><li><p>The event is <strong>published to a message broker</strong> (e.g., Kafka, Redis Pub/Sub).</p></li><li><p>The Event broadcaster service  and:</p><ul><li><p>Looks up the WebSocket server holding the user’s connection</p></li><li><p>Routes the event to that server, which pushes it to the correct client over the live WebSocket connection</p></li></ul></li></ul><p>The client receives the message and updates the UI immediately (e.g., displaying a toast notification or updating the comment count in real time).</p>","contentLength":8764,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5634c993-d806-4857-881d-59efe68fb5e7_1616x1432.png","enclosureMime":"","commentsUrl":null},{"title":"10 Must-Know Database Types for System Design Interviews","url":"https://blog.algomaster.io/p/database-types-for-system-design-interviews","date":1748343638,"author":"Ashish Pratap Singh","guid":707,"unread":true,"content":"<p><strong>Choosing the right database</strong> is one of the most critical decisions you’ll make in a .</p><p>The database you pick often dictates how well your system performs under load, how easily it scales, and how gracefully it handles complexity in real-world scenarios.</p><p>That’s why a strong understanding of different database types and  is a key part of acing system design interviews.</p><p>In this article, we’ll walk through the <strong>10 must-know database types</strong> for system design interviews. For each one, we’ll explain:</p><ul><li><p>When to use it (with examples)</p></li><li><p>Key design considerations</p></li><li><p>Popular databases you can mention in interviews</p></li></ul><blockquote><p>A  stores data in structured tables with rows and columns. It’s like an Excel sheet, but much more powerful.</p></blockquote><p>Each table represents an entity (like , , or ), and relationships between tables are defined using . It uses <strong>SQL (Structured Query Language)</strong> to query and manipulate data.</p><h4>Your data is structured and relational</h4><p>Relational databases are ideal when your data consists of clearly defined entities with strong relationships.</p><p>In an :</p><ul><li><p> can contain multiple </p></li><li><p> have associated  and belong to a </p></li></ul><p>A relational schema with foreign keys makes it easy to model and enforce these relationships, and SQL allows efficient querying via joins.</p><h4>You need strong consistency</h4><p>Relational databases provide full <a href=\"https://blog.algomaster.io/p/what-are-acid-transactions-in-databases\">ACID</a> compliance, ensuring reliable transactions.</p><p>In a digital payments system, transferring money between accounts requires atomic updates. If one step fails, the entire transaction rolls back, preserving data integrity.</p><p>This level of <strong>data integrity and transactional safety</strong> is what relational databases excel at.</p><h4>You require complex queries and reporting</h4><p>Relational databases provide a powerful and expressive query language. SQL supports filtering, aggregation, grouping, and multi-table joins.</p><p>Indexes speed up  by allowing the database to quickly locate rows.</p><p>Create indexes on frequently queried columns (e.g., , ). Use composite indexes for multi-column filters. Avoid  in write-heavy systems, as it can slow down inserts and updates.</p><h4>Normalization vs Denormalization</h4><p>Normalize to reduce redundancy and ensure consistency. Denormalize in read-heavy systems to reduce join overhead.</p><p>Joins are powerful for analytics and reporting. However,  on large tables as they can become performance bottlenecks. Never design for  unless absolutely necessary.</p><p>Sharding enables  but introduces complexity. </p><p>Choose high-cardinality shard keys (e.g., ) to distribute load evenly. Be mindful that cross-shard queries and transactions are difficult to implement.</p><ul><li><p> (adding more CPU/RAM to a single machine): Easy but limited.</p></li><li><p>H: Add read replicas, partition large tables, and use caching (e.g., Redis) for frequently accessed data</p></li></ul><ul><li><p> – Open-source, feature-rich, ACID-compliant</p></li><li><p> – Widely used, especially in LAMP stack applications</p></li><li><p> – Enterprise-grade RDBMS</p></li></ul><blockquote><p>An  stores data directly in RAM instead of disk. This makes it  for read and write operations</p></blockquote><h4>You need ultra-low latency</h4><p>In-memory databases are ideal for applications that demand <strong>near-instantaneous responses</strong>.</p><p> A real-time leaderboard in a gaming app where scores are updated and ranked instantly.</p><h4>The data is temporary or can be regenerated</h4><p>In-memory databases are great for storing data that can be recomputed if lost.</p><p>: Caching trending search results to reduce repeated computation and speed up queries.</p><h4>You want to reduce load on your main database</h4><p>In-memory stores can act as a  to offload frequent reads from your primary database.</p><p>: Caching  in a social media platform to avoid repeated lookups in the main DB.</p><p>Since RAM is volatile, data is lost on crash or restart unless persistence is enabled.</p><p>Tools like  offer optional persistence via:</p><ul><li><p>: Saves data at intervals</p></li><li><p>: Logs each write operation</p></li></ul><p>RAM is fast, but limited. When memory runs out,  is evicted. Common  include ,  and </p><p>Avoid storing large files or infrequently accessed data. Store and such as user sessions and recent activity.</p><p>Replication can improve  and provide  in case the primary instance goes down. Redis supports  and automatic failover using Sentinel or Cluster mode.</p><p>However, replication is typically , so there's a risk of data loss if the primary fails before sync. Always persist critical data in a durable system like a relational database.</p><ul><li><p> – Lightning fast, supports data structures like lists, sets, sorted sets, and pub-sub</p></li><li><p> – Simple key-value store for caching</p></li><li><p> – Distributed in-memory store with SQL support</p></li><li><p> – Often used in Java-based enterprise applications</p></li></ul><blockquote><p>A  is the simplest type of database. It stores data as a collection of , where each key is unique and maps directly to a value. Think of it like a giant, distributed .</p></blockquote><p>There are no tables, schemas, or relationships—just keys and values. This makes key-value stores extremely fast and highly scalable.</p><h4>You need fast lookups by unique key</h4><p>Key-value stores offer  reads and writes, making them ideal for quick access using a unique identifier.</p><ul><li><p>Storing user sessions as  in a web application.</p></li><li><p>Storing  mappings in a URL shortener service.</p></li></ul><h4>You don’t need complex queries or relationships</h4><p>If your data doesn't require joins, filtering, or relational constraints, a key-value store is a simple and scalable choice.</p><h4>You’re dealing with high-volume, low-latency workloads</h4><p>Key-value stores are designed for  and are often used in systems that demand <strong>millions of reads/writes per second</strong> with minimal latency.</p><p>You can only retrieve values by key. They typically don’t provide filtering, sorting, or joining. Secondary indexes are typically .</p><p>Key-value databases are . Values can be strings, JSON, or binary blobs.</p><p>This gives you flexibility but also puts the burden on your application to handle serialization/deserialization and versioning of the data model.</p><p> enables seamless distribution across nodes using  or <strong>range-based partitioning.</strong></p><p>To distribute keys evenly, choose  (avoid  if most users are from one region)</p><ul><li><p> – Also acts as a key-value store with rich data types</p></li><li><p> – Managed, horizontally scalable key-value store</p></li><li><p> – Distributed, fault-tolerant key-value system</p></li><li><p> – High performance for low-latency read/write at scale</p></li></ul>","contentLength":6071,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/88e1ef63-bbce-49f3-bdfd-d1ca56565168_1994x1314.png","enclosureMime":"","commentsUrl":null},{"title":"Why is Redis so Fast and Efficient?","url":"https://blog.algomaster.io/p/why-is-redis-so-fast-and-efficient","date":1747803615,"author":"Ashish Pratap Singh","guid":706,"unread":true,"content":"<p><strong>Redis (Remote Dictionary Server)</strong> is a , , <strong>in-memory key-value store</strong> that’s become a go-to choice for building ,  applications.</p><p>Despite being , a single Redis server can handle <strong>over 100,000 requests per second.</strong></p><blockquote><p><strong>But, how does Redis achieve such incredible performance with a single-threaded architecture?</strong></p></blockquote><p>In this article, we’ll break down the <strong>5 key design choices and architectural optimizations</strong> that make Redis so fast and efficient:</p><ul><li><p>: Data lives entirely in RAM, which is orders of magnitude faster than disk.</p></li><li><p><strong>Single-Threaded Event Loop</strong>: Eliminates concurrency overhead for consistent, low-latency performance.</p></li><li><p><strong>Optimized Data Structures</strong>: Built-in structures like hashes, lists, and sorted sets are implemented with speed and memory in mind.</p></li><li><p>: Event-driven networking, pipelining, and I/O threads help Redis scale to thousands of connections.</p></li><li><p>: Lua scripts allow complex operations to run atomically, without round trips.</p></li></ul><p>The single most important reason Redis is so fast comes down to one design decision:</p><blockquote><p><strong>All data in Redis lives in RAM.</strong></p></blockquote><p>Unlike traditional databases that store their data on disk and read it into memory when needed, Redis keeps the <strong>entire dataset in memory at all times</strong>.</p><p>Even with a fast SSD, reading from disk is <strong>thousands of times slower</strong> than reading from RAM.</p><p>So when Redis performs a , it doesn’t wait for disk I/O. It simply follows a pointer in memory—an operation that completes in , not milliseconds.</p><p>Redis doesn’t just store data in RAM, it stores it .</p><ul><li><p>Small values are packed into compact memory formats (, , )</p></li><li><p>These formats improve , letting Redis touch fewer memory locations per command</p></li></ul><p>While in-memory storage gives Redis its speed, it also introduces two important limitations:</p><p>Your dataset size is limited by how much RAM your machine has. For example:</p><ul><li><p>On a 32 GB server, Redis can only store up to 32 GB of data (minus overhead)</p></li><li><p>If you exceed this, Redis starts evicting keys or rejecting writes unless you scale horizontally</p></li></ul><p>To deal with this, Redis offers  like:</p><ul><li><p>Least Recently Used (LRU)</p></li><li><p>Least Frequently Used (LFU)</p></li><li><p>Volatile TTL-based eviction</p></li></ul><p>You can also  your dataset across a Redis Cluster.</p><h4>2. Volatility &amp; Durability</h4><p>RAM is . It loses data when the server shuts down or crashes. That’s risky if you’re storing anything you care about long term.</p><p>Redis solves this with <strong>optional persistence mechanisms</strong>, allowing you to write data to disk periodically or in real time.</p><p>Redis provides two main persistence models to give you durability without compromising performance:</p><ul><li><p><strong>RDB (Redis Database Snapshot)</strong></p><ul><li><p>Takes point-in-time snapshots of your data</p></li><li><p>Runs in a , so the main thread keeps serving traffic</p></li><li><p>Good for backups or systems that can tolerate some data loss</p></li></ul></li><li><ul><li><p>Logs every write operation to disk</p></li><li><p>Offers configurable  options: </p><ul><li><p>Every write (safe but slow)</p></li></ul></li><li><p>Supports  in the background to reduce file size</p></li></ul></li></ul><p>These persistence methods are designed to , so the main thread never blocks.</p><p>One of Redis’s most surprising design choices is this:</p><blockquote><p><strong>All commands in Redis are executed by a single thread.</strong></p></blockquote><p>In a world where most high-performance systems lean on multi-core CPUs, parallel processing, and thread pools, this seems almost counterintuitive.</p><p><strong>Shouldn’t more threads mean more performance?</strong></p><p>Not necessarily. Redis proves that sometimes, <strong>one well-utilized thread can outperform many</strong>, if the architecture is right.</p><h3>But How Does One Thread Handle Thousands of Clients?</h3><p>The answer lies in Redis’s , powered by .</p><h4>What is I/O Multiplexing?</h4><p>I/O Multiplexing allows a single thread to monitor multiple I/O channels (like network sockets, pipes, files) simultaneously.</p><p>Instead of spinning up a new thread for each client, Redis tells the OS:</p><blockquote><p>\"Watch these client sockets for me and let me know when any of them have data to read or are ready to write.\"</p></blockquote><p>The implementation relies on highly optimized  specifically designed for this purpose:</p><ul><li><p> (Linux): High-performance I/O event notification system. Designed for scalability, it can handle thousands of concurrent connections efficiently.</p></li><li><p> (macOS): BSD-style I/O event notification system. Monitors a wide range of events: file descriptors, sockets, signals, and more.</p></li><li><p> (fallback): Oldest and most portable I/O multiplexing method, supported on almost all platforms.</p></li></ul><p>These interfaces allow Redis to remain dormant, consuming no CPU cycles, until the moment data arrives or a socket becomes writable.</p><p>Redis event loop is a lightweight cycle that efficiently juggles thousands of connections .</p><p>When a client sends a request, the operating system , which then:</p><ol><li><p>Moves to the next ready client</p></li></ol><p>This loop is tight, predictable, and fast. Redis cycles through ready connections, executes commands , and responds quickly without ever waiting on a slow client or thread switch.</p><h3>Internal Flow of a GET Command</h3><p>To understand the simplicity and speed of this model, let’s walk through how Redis handles a simple  command:</p><pre><code><code>1. Client sends: GET user:42\n2. I/O multiplexer wakes the Redis event loop\n3. Redis reads the command from the socket buffer\n4. Parses the command\n5. Looks up the key in an in-memory hash table (O(1))\n6. Formats the response\n7. Writes the response to the socket buffer\n8. Returns to listening for more events</code></code></pre><p>All of this happens , without any locking or waiting.</p><h3>Why Single-Threaded Works So Well</h3><p>By sticking to a single-threaded execution model, Redis <strong>avoids the typical overhead</strong> that comes with multithreaded systems:</p><ul><li><p>No locks, mutexes, or semaphores</p></li><li><p>No race conditions or deadlocks</p></li></ul><p>This means Redis spends almost all its CPU time doing actual work rather than wasting cycles coordinating between threads.</p><p>Since only one thread is modifying Redis’s in-memory data at a time, <strong>operations are inherently atomic</strong>:</p><ul><li><p>No two clients can update the same key at the same time</p></li><li><p>You don’t need locks to ensure safety</p></li><li><p>You don’t get partial updates due to concurrency bugs</p></li></ul><p>This dramatically simplifies the internal logic and improves  and .</p><p>Redis isn’t just fast because it stores everything in memory. It’s also fast because it stores data .</p><p>It doesn’t use generic one-size-fits-all containers. It picks the  for each use case and implements it in , with a focus on speed, memory efficiency, and predictable performance.</p><h3>Adaptive Internal Representations</h3><p>Each data type in Redis has <strong>multiple internal representations</strong>, and Redis  between them based on size and access pattern.</p><ul><li><ul><li><p>Small collections → Stored as compact  or  (memory-efficient and fast)</p></li><li><p>Larger collections → Converted to  or  for scalability</p></li></ul></li><li><ul><li><p>If elements are integers and set is small → Stored as </p></li><li><p>Grows large → Upgraded to a standard </p></li></ul></li><li><ul><li><p>Backed by a hybrid of a  and a , allowing fast score-based queries and O(log N) operations</p></li></ul></li></ul><p>This design makes Redis <strong>fast and memory-efficient</strong> at every scale.</p><h3>Built for Big-O Performance</h3><p>Redis carefully picks and implements data structures to ensure <strong>excellent time complexity</strong>:</p><p>These operations stay fast even as the dataset grows, thanks to efficient internal representations and fine-tuned implementations in C.</p><p>Redis also takes advantage of <strong>low-level programming techniques</strong> to squeeze out every last bit of performance.</p><p>Redis isn’t just fast at executing commands, it’s also extremely efficient at .</p><p>Whether you’re serving a single API call or managing tens of thousands of concurrent clients, Redis keeps up with <strong>minimal latency and maximum throughput</strong>.</p><p><strong>So, what exactly makes Redis’s I/O so efficient?</strong></p><h3>A Lightweight, Fast Protocol</h3><p>Redis uses a custom protocol called <strong>(REdis Serialization Protocol)</strong>, which is:</p><ul><li><p> but </p></li><li><p>Extremely lightweight (much simpler than HTTP or SQL)</p></li><li><p>Designed for </p></li></ul><p><strong>Example of a RESP-formatted command::</strong></p><p>Each part of the message clearly defines the number of elements and their sizes. This structure allows Redis to <strong>read and parse commands with minimal CPU cycles</strong>, unlike parsing full SQL queries or nested JSON structures.</p><h3>Pipelining: Batching to Boost Throughput</h3><p>One of Redis’s most effective I/O optimization features is .</p><p>Normally, a client sends one command, waits for a response, then sends the next. This is fine for a few requests but inefficient when thousands of commands are involved.</p><p>With , the client sends multiple commands in a single request  for intermediate responses.</p><pre><code>SET user:1 \"Alice\"\nGET user:1\nINCR counter</code></pre><p>These three commands can be sent in a single TCP packet. Redis reads and queues them, executes them in order, and returns all responses at once.</p><ul><li><p>Fewer round-trips → reduced latency</p></li><li><p>Less back-and-forth → higher throughput</p></li><li><p>Less context switching → lower CPU overhead</p></li></ul><p>In real-world benchmarks, pipelining can help Redis achieve <strong>1 million+ requests per second</strong>.</p><h3>Redis 6+: Optional I/O Threads</h3><p>While Redis has traditionally used a single thread for both command execution and I/O, <strong>Redis 6 introduced optional I/O threads</strong> to further improve performance—especially in network-heavy scenarios.</p><p>When enabled, I/O threads handle:</p><ul><li><p> client requests from sockets</p></li><li><p> responses back to clients</p></li></ul><p>Command execution still happens on the , preserving Redis’s atomicity and simplicity.</p><p>This hybrid model brings the best of both worlds:</p><ul><li><p><strong>Multi-core network processing</strong></p></li><li><p><strong>Single-threaded command execution</strong></p></li></ul><p>In workloads where clients send or receive large payloads (e.g., big JSON blobs, long lists), I/O threads can .</p><h3>Persistent Connections: Avoiding the Handshake Overhead</h3><p>Redis client libraries typically use <strong>persistent TCP connections</strong>, which means:</p><ul><li><p>No repeated handshakes or reconnects</p></li><li><p>Lower latency for every command</p></li><li><p>More predictable performance under load</p></li></ul><p>Persistent connections also reduce CPU and memory usage on the server, since Redis doesn’t have to reallocate resources for new connections frequently.</p><p>Redis also offers the ability to execute . This allows you to run complex logic  without bouncing back and forth between the client and server.</p><p>Let’s say you want to perform this logic:</p><ol><li><p>If they do, increment their score</p></li><li><p>Add them to a leaderboard</p></li></ol><p>Doing this using multiple client-server requests would involve:</p><ul><li><p>Multiple round trips over the network</p></li><li><p>Race conditions if multiple clients do this concurrently</p></li><li><p>More code on the client to handle logic</p></li></ul><p>With , you can do all of this in , executed entirely on the Redis server.</p><pre><code>-- Lua script to increment score and update leaderboard\nlocal key = \"user:\" .. ARGV[1]\nlocal new_score = redis.call(\"INCRBY\", key, tonumber(ARGV[2]))\nredis.call(\"ZADD\", \"leaderboard\", new_score, ARGV[1])\nreturn new_score</code></pre><p>Run this script using the  command:</p><pre><code>EVAL \"&lt;script&gt;\" 0 user123 50</code></pre><p>This increments the user’s score and updates the leaderboard <strong>in one atomic server-side operation</strong>.</p><h3>Scripting is Powerful, But Use Responsibly</h3><p>While Lua scripting is fast and atomic, there are a few things to watch out for:</p><ul><li><p>Scripts run on the main thread: If your script is slow or CPU-heavy, it can block Redis from serving other requests.</p></li><li><p>Avoid unbounded loops or expensive computations</p></li><li><p>Keep scripts short and predictable</p></li></ul><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/how-i-mastered-data-structures-and-algorithms?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQ1NjU1MjUyLCJpYXQiOjE3MjE1MjE3MzEsImV4cCI6MTcyNDExMzczMSwiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.2cNY811YEugd5iH9XJQhakBzyahGqF7PcATBlFj5J2w&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":11010,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fabdba039-ee21-48f2-b391-d411628067e3_2492x1692.png","enclosureMime":"","commentsUrl":null},{"title":"I Created a GitHub Repository to Learn AI Engineering","url":"https://blog.algomaster.io/p/github-repository-ai-engineering","date":1747545397,"author":"Ashish Pratap Singh","guid":705,"unread":true,"content":"<p>Over the past few months, I’ve been diving deep into AI Engineering.</p><p>To make the journey easier (for both you and me), I’ve put together a <strong>structured GitHub repository</strong>: </p><p>This repo is designed to be a  that curates the best <strong>free courses, articles, tutorials, and videos</strong> across the entire AI engineering stack.</p><p>You’ll find resources to learn:</p><ul><li><p>Deep Learning and Specializations</p></li><li><p>Large Language Models (LLMs)</p></li><li><p>Prompt Engineering Guides</p></li></ul><ul><li><p>If you find the repo valuable, consider giving it a ⭐  and share with others!</p></li><li><p>Know a great resource that should be included? Feel free to create a pull request—contributions are welcome!</p></li></ul><p>I'm also launching a second newsletter called  where I will be sharing my learning and breaking down complex AI topics in a simple, practical way.</p><ul><li><p>🧠 Core AI &amp; ML concepts explained simply</p></li><li><p>🛠️ Hands-on tools, frameworks, and coding tutorials</p></li><li><p>🤖 LLMs, agents, and the evolving landscape of Generative AI</p></li><li><p>🧪 Practical breakdowns of research, trends, and real-world applications</p></li></ul><p><strong>Subscribe here to stay updated:</strong></p>","contentLength":1030,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78de46bc-5e49-4efe-9b06-fe7d044777a0_643x494.png","enclosureMime":"","commentsUrl":null},{"title":"Designing a Proximity Service like Yelp","url":"https://blog.algomaster.io/p/designing-proximity-service-like-yelp","date":1747285597,"author":"Ashish Pratap Singh","guid":704,"unread":true,"content":"<p>Services like , , or  have become an essential part of how we discover the world around us.</p><p>Whether you're looking for the best coffee shop within walking distance, a nearby gym that’s open late, or the top-rated restaurant in your city—these platforms deliver location-aware results , personalized and filtered to your needs.</p><p><strong>But how does such a system work under the hood?</strong></p><p>In this blog, we’ll walk through the <strong>design  of a proximity service like Yelp</strong>—a platform that helps users search for nearby businesses, view detailed listings, read and write reviews, and save favorites for future visits.</p><p>We'll dive deep into the high-level architecture, database design, API design, indexing strategies, caching, scalability, and how to handle billions of data points with milli-seconds latency.</p><p>Before diving into the architecture, lets clearly define  we’re building.</p><h2>1.1 Functional Requirements</h2><ul><li><p>Users can  (restaurants, gyms, salons, etc.) using their . Support filters like:</p><ul><li><p> (e.g., within 2 km, 5 km)</p></li><li><p> (e.g., cafe, spa, grocery store)</p></li><li><p> (e.g., 4 stars and above)</p></li><li><p> (e.g., currently open)</p></li></ul></li><li><p><strong>Business Listing Details: </strong>Each place should have a detailed page showing:</p><ul><li><p>Photos, opening hours, services</p></li></ul></li><li><p>Users can leave  (1 to 5 stars) and .</p></li><li><p>Business owners can create a new listing or update details</p></li><li><p>Users can  or  favorite places for quick access later.</p></li></ul><h2>1.2 Non-Functional Requirements</h2><ul><li><p>The system should handle millions of users and tens of millions of listings and reviews</p></li><li><p>Search results should be returned within &lt;200ms</p></li><li><p>The service should remain operational even if individual components fail. Availability target: </p></li><li><p>For certain operations (e.g., newly posted reviews or updated listings), the system can be  rather than strongly consistent.</p></li></ul><p>Let’s assume this service operates at global scale:</p><ul><li><p><strong>Monthly Active Users (MAU): </strong>50 million</p></li><li><p><strong>Daily Active Users (DAU): </strong>~10 million (20% of MAU)</p></li><li><p><strong>New businesses added/day:</strong> 100,000</p></li></ul><blockquote><p>Peak traffic may be , especially during weekends, holidays, or lunch/dinner hours.</p></blockquote><ul><li><p>Assume 100 million listings</p></li><li><p>Each listing ≈ 2 KB (name, category, address, etc.)</p></li></ul><blockquote><p>Relatively small footprint, fits well in relational or document DBs. Queryable fields (category, rating, location) should be indexed.</p></blockquote><ul><li><p>Assume 1 billion reviews (historical)</p></li><li><p>Data per review ≈ 1 KB (text + metadata)</p></li></ul><ul><li><p>: 5</p></li><li><p>: ~200 KB (optimized JPEG or WebP)</p></li><li><p>100M listings × 5 × 200 KB = </p></li></ul><blockquote><p>Stored in  like Amazon S3, not in databases. Backed by CDN for efficient global delivery.</p></blockquote><ul><li><p>Assume 100M total saved favorites</p></li><li><p>Data per favorite ≈ 50 bytes (user_id, business_id, timestamp)</p></li><li><p>100M × 50bytes </p></li></ul>","contentLength":2537,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/0d68a9de-894a-4d97-96b1-37d4022deec9_2816x2456.png","enclosureMime":"","commentsUrl":null},{"title":"20 Git Commands EVERY Developer Should Know","url":"https://blog.algomaster.io/p/top-20-git-commands","date":1747108882,"author":"Ashish Pratap Singh","guid":703,"unread":true,"content":"<p>If you want to become a great developer, you need to master Git.</p><p>But, Git can feel confusing and overwhelming. With so many commands and options, it’s hard to know which ones actually matter.</p><p>In this article, I’ll walk you through the  every developer should know.</p><p>These are the exact commands I’ve used again and again over the past 11 years as a programmer and they cover almost  of real-world Git use cases.</p><p>If you get comfortable with just these, you will be able to handle almost anything git related while working on projects.</p><p>Let’s start at the very beginning.</p><p>This command turns a regular folder into a Git repository.</p><p>Behind the scenes, it creates a hidden  folder.</p><p>This folder holds everything Git needs—your commit history, branches, logs, and all the metadata that powers version control.</p><p>You’ll typically use  when starting a brand-new project from scratch.</p><p>Once you run , Git starts watching your files for changes.</p><p>From here on, you can commit changes, create branches, roll back mistakes, and more.</p><p>But before you start making commits, there’s one important thing you need to do: </p><p>That’s where  comes in.</p><p>This command lets you set your name and email so Git can tag each of your commits with your identity.</p><pre><code><strong>git config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"</strong></code></pre><p>The  flag means this configuration applies to all your Git projects.</p><p>But if you want to set a different name or email just for one project, you can run the same command without the --global flag, inside that project’s folder.</p><p>This step might seem small, but it’s actually very important, especially when you’re working in a team.</p><p>Git uses this information to track  made  change.</p><p>Now let’s say you’re not starting a new project locally. But, joining an existing one hosted on a , like Github.</p><p>In that case, you use the  command followed by the repository’s URL:</p><pre><code><strong>git clone &lt;https://github.com/user/project.git&gt;</strong></code></pre><p>This command does two things:</p><ol><li><p><strong>It downloads the entire project</strong>, including all files, folders, branches, and commit history—so you get a complete local copy.</p></li><li><p><strong>It sets up a connection to the remote repository</strong>, so you can easily push your changes or pull updates from others.</p></li></ol><p>Once cloned, you’re ready to start working on the project locally.</p><p>Now let’s imagine a different scenario: <strong>your local Git repository isn’t connected to any remote.</strong></p><p>In that case, Git has no idea  to push your changes.</p><p>That’s where the  command comes in.</p><p>Using , you can , , or  remote connections. Remote repository can be hosted anywhere like GitHub, GitLab, Bitbucket, or even your own server.</p><p>To check if your project already has any remotes configured, run:</p><p>This will list all the remote URLs your project can push to or pull from.</p><pre><code><strong>origin  https://github.com/algomaster-io/git-tutorial.git (fetch)\norigin  https://github.com/algomaster-io/git-tutorial.git (push)</strong></code></pre><p>If you need to add a new remote, use:</p><pre><code><strong>git remote add origin &lt;https://github.com/user/project.git&gt;</strong></code></pre><p>Here,  is just an alias for your remote. It’s not special, but it’s a common convention for the primary remote.</p><p>Once you’ve added a remote, Git knows where to send your code when you push it.</p><p>Alright, now that we’ve covered how to set up Git locally and connect it to a remote repository, let’s move on to how you actually start working with your code and tracking changes.</p><p>If there’s one Git command you’ll find yourself using constantly, it’s this one:</p><p>Think of it as your —a quick snapshot of everything happening in your working directory.</p><p>Whenever you’re unsure about what’s changed, what’s staged, or what’s untracked,  gives you the full picture.</p><ul><li><p>Which files have  for commit</p></li><li><p>Which files have <strong>changed but are not staged</strong></p></li></ul><p>Before committing anything, it’s a good habit to run .</p><p>It helps you decide what to stage, what to skip, and what needs attention.</p><p>Once you’ve reviewed the changes, your next step is usually to stage the right files using .</p><p>When you run , you’re telling Git to include this in the next snapshot.</p><p>Just editing a file isn’t enough. Git doesn’t track your changes automatically.</p><p>Until you run , those changes are invisible to Git when it comes time to commit.</p><p>You can stage a specific file by running git add filename:</p><p>Or, if you want to stage everything that’s changed in the current directory, run:</p><blockquote><p> is like packing your code changes into a box and getting them ready for delivery.</p></blockquote><p>Once your changes are staged, it's time to  with a commit with  command.</p><p>Think of  as hitting “Save” in the world of version control.</p><p>It takes a snapshot of your codebase at that point in time along with a message describing .</p><pre><code><strong>git commit -m \"Add login feature\"</strong></code></pre><p>That message isn’t just a label, it’s a log of what you did.</p><p>Later, when you’re looking through your project’s history (or fixing bugs), clear commit messages will save you and your teammates a lot of time.</p><p>If you’re working on files that Git already knows about (i.e., they’ve been committed before), you can skip the  step using:</p><pre><code><strong>git commit -a -m \"Update login error handling\"</strong></code></pre><p>The  flag tells Git to automatically stage all modified tracked files before committing.</p><p>But note—this only works for files Git is already tracking.</p><p>If you’ve created a  or only want to include specific files in the commit, you still need to run  first.</p><p>Sometimes, you commit too early or want to change the commit message.</p><p>In that case, you can use:</p><p>This lets you either edit the commit message or include additional changes (as long as they’ve been staged with ).</p><p> and  are local operations. Your changes are saved on your machine only. No one else can see them yet.</p><p>To share your commits with others, you use:</p><p>The  command sends your commits from your local repository to a remote one, like GitHub or GitLab.</p><p>It’s how you sync your changes with the central project.</p><p>This command pushes your changes to the  branch of your remote repository. Now anyone else working on the project can see your updates.</p><blockquote><p> In most real-world projects, you don’t push directly to the main branch. Instead, you push to a separate feature branch, and then create a pull request or code review before merging into main.</p></blockquote><p>Sometimes, when you're working on a project with others, someone else may have already pushed their changes before you.</p><p>That’s why, before you push your work, it’s a good idea to  so your local copy is up to date and you don’t accidentally overwrite someone else’s work.</p><p>To do that, we use command.</p><p>This command does two things in one step:</p><ol><li><p>It  the most recent commits from the remote repository.</p></li><li><p>Then, it  those commits into your current local branch.</p></li></ol><p>You can also specify the branch that you want to pull from. It will merge them into whatever branch you’re currently on.</p><p>But what if you don’t want to merge anything just yet?</p><p>Maybe you just want to  on the remote without affecting your local work.</p><p>That’s what  is for.</p><p>This command contacts the remote and <strong>downloads all the latest changes</strong>—new commits, branches, etc.—but it doesn’t touch your working directory.</p><p>Your current branch remains exactly the same.</p><p>If you're on the  branch and want to see what new commits are on the remote , run:</p><pre><code><strong>git log HEAD..origin/main</strong></code></pre><p>This shows the commits that exist on the remote but not on your local branch.</p><p> gives you full control—you fetch first, review, and only merge when you're ready.</p><p>Speaking of branches—let’s look at how to create and work with them next.</p><p>When you’re working on a new feature or fixing a bug, it’s best to keep that work separate from the main codebase.</p><p>That’s where  come in.</p><p>To create a new branch we simply run:</p><p>This creates a branch called  , giving you a  where you can make changes without affecting the  branch.</p><p>Once you're done, you can merge your changes back into the main branch or open a pull request for review.</p><p>To see a list of all your , just run:</p><pre><code>  dev\n* main\n  feature/login\n  hotfix/navbar-bug</code></pre><p>The branch you’re currently on will be marked with an asterisk ().</p><p>But if you have multiple branches…How do you switch between them?</p><p>That’s where command comes in.</p><p>This switches your working directory to the  branch. It’s like opening up a different version of your project focused on a specific task.</p><p>If you want to create a new branch and switch to it in one step, you can use:</p><pre><code><strong>git checkout -b new-feature</strong></code></pre><p>But  isn’t just for switching branches. It’s a versatile command you can use in other ways too:</p><ul><li><p> by checking out its hash:</p></li><li><p><strong>Switch back to your previous branch</strong>:</p></li><li><p> and restore a file to its last committed state:</p><ul><li><p><code>git checkout -- style.css</code></p></li></ul></li><li><p> from another branch:</p><ul><li><p><code>git checkout feature-branch src/app.js</code></p></li></ul></li></ul><p>Because  tries to do a lot of different things, Git later introduced a cleaner alternative:</p><p>This command focuses purely on switching branches.</p><p>Once you’re done working on a branch and want to bring your changes back into the main project, you use: .</p><p> combines the changes from one branch into another.</p><p>This tells Git to take the changes from the  branch <strong>and combine them into the branch you are currently on.</strong></p><p>If the changes don’t overlap, Git will merge everything automatically.</p><p>But if <strong>both branches have modified the same parts of a file</strong>, Git won’t know which version to keep.</p><p>That’s called a , and Git will pause and ask  to manually resolve it.</p><p><strong>Sample Merge Conflict Output:</strong></p><p>Git marks the conflicting files. If you open one (e.g., ), you’ll see conflict markers like this:</p><pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nconsole.log(\"Welcome to the main branch\");\n=======\nconsole.log(\"Login feature initialized\");\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/login</code></pre><p>You now have to <strong>manually resolve the conflict</strong>, remove the conflict markers, and keep the correct code.</p><pre><code>git add src/App.js src/components/Header.js\ngit commit</code></pre><p>This final commit will complete the merge.</p><p>Now, while merging is a safe and common way to combine changes, it has a small drawback—<strong>it can clutter your commit history.</strong></p><p>You may end up with lots of merge commits, especially if you’re merging frequently.</p><p>If you like keeping your commit history neat and linear,  is the command for you.</p><p>Unlike , which combines branches and often creates an extra merge commit,  on top of another branch as if you wrote them after the latest changes.</p><pre><code>* 3f9e1b2 Add login form validation\n* 1a2d3f4 Create login page UI\n* a12cd3e Update homepage banner\n* 91b2ea3 Initial commit</code></pre><p>This makes your commit history look like a straight line instead of a mess of merges.</p><p>Let’s say you’re working on a  branch, and  has moved forward.</p><p>You can update your feature branch with the latest changes from  using:</p><p>This takes all the commits from your current branch and places them  the latest commit on , one by one.</p><p>This results into a cleaner, more linear project history that’s easier to read and reason about.</p><blockquote><p>Remember to not rebase branches that others are working on. Because rebase rewrites commit history, it can create confusion or conflicts for your teammates.</p></blockquote><p>When used properly,  is a powerful way to keep your project history clean, clear, and easy to follow.</p><p>Once you’ve made a few commits, you’ll probably want to look back and see <strong>what changed, when, and by whom</strong>.</p><p>That’s where  comes in.</p><p>This command gives you a detailed history of commits showing commit hashes, author names, dates, and commit messages.</p><pre><code>commit 3f9e1b2a8c5b7894dc8d39f12c4f271c51a2f0e4\nAuthor: Jane Doe &lt;jane@example.com&gt;\nDate:   Mon May 13 10:45:12 2024 +0530\n\n    Add login form validation\n\ncommit 1a2d3f4e6b0c34b18ff9d1b8e20d4b6129a15a78\nAuthor: Jane Doe &lt;jane@example.com&gt;\nDate:   Sun May 12 17:08:45 2024 +0530\n\n    Create login page UI\n\ncommit a12cd3e6aa719c3b32c29fd2f1123d2b31434cc9\nAuthor: John Smith &lt;john@example.com&gt;\nDate:   Sat May 11 14:30:59 2024 +0530\n\n    Update homepage banner</code></pre><p>If you want a more compact view, try:</p><pre><code>3f9e1b2 Add login form validation\n1a2d3f4 Create login page UI\na12cd3e Update homepage banner</code></pre><p>This shows the commit timeline in one-line-per-commit format.</p><p>If you want to <strong>visualize branches and merges</strong>, you can run:</p><pre><code>* commit a1b2c3d (HEAD -&gt; main, origin/main)\n| Author: Jane Doe &lt;jane@example.com&gt;\n| Date:   Tue May 13 10:23:45 2025 -0700\n|\n|     Fix pagination bug\n|\n* commit b2c3d4e\n| Author: John Smith &lt;john@example.com&gt;\n|\n|     Add dark mode toggle\n|\n*   commit c3d4e5f\n|\\  Merge: d4e5f6g e5f6g7h\n| | Author: Jane Doe &lt;jane@example.com&gt;\n| |\n| |     Merge feature/api-v2 into main\n| |\n| * commit d4e5f6g (feature/api-v2)\n| |\n| |     Complete API v2\n|/\n* commit f6g7h8i\n|\n|     Update docs</code></pre><p>This adds a branching diagram next to your commits, making it easier to see how your project evolved over time.</p><p> is super useful when you’re tracking down bugs , or just reviewing your work.</p><p>And speaking of reviewing—before you commit, it’s always smart to check what’s changed since your last commit.</p><p>That’s where  command comes in.</p><p> gives you a  between your working directory and the last commit.</p><pre><code>diff --git a/src/App.js b/src/App.js\nindex 3a5b2c1..7d8e9f2 100644\n--- a/src/App.js\n+++ b/src/App.js\n@@ -12,7 +12,10 @@ function App() {\n   return (\n     &lt;div className=\"App\"&gt;\n-      &lt;h1&gt;Welcome to My App&lt;/h1&gt;\n+      &lt;h1&gt;Welcome to My Awesome App&lt;/h1&gt;\n+      &lt;p&gt;Start by logging in to continue.&lt;/p&gt;\n     &lt;/div&gt;\n   );\n }</code></pre><p>It highlights what’s been , , or  so you know exactly what you’re about to commit.</p><p>You can also use git diff to compare changes between branches or specific commits:</p><pre><code><strong>git diff branch1..branch2</strong></code></pre><p>This shows what’s different between two branches—great for code reviews or deciding whether a branch is ready to merge.</p><p>Sometimes you’re in the middle of working on something and suddenly, you need to switch branches or handle a different task.</p><p>But your changes aren’t ready to commit yet.</p><p>You can use  command to temporarily save all your changes <strong>both staged and unstaged.</strong></p><p>It’s like putting your work in a drawer so you can come back to it later.</p><p>When you're ready to pick up where you left off, just run:</p><p>This brings back your stashed changes while keeping the stash in case you need it again.</p><p>But, if you want to apply the stash and remove it at the same time, use:</p><p>Now, let’s say you've made a mistake and want to undo a commit or reset your branch. </p><p>That’s where  comes into play.</p><p> is one of the most powerful commands for rolling back changes.</p><p>It moves the current branch’s  pointer to a previous commit—essentially rewinding your project to an earlier state.</p><p>Depending on how much you want to undo, you can use  in three different ways:</p><p>Moves the HEAD back by one commit, but <strong>keeps all your changes staged</strong>.</p><blockquote><p>Use this when you want to adjust a previous commit or fix a message.</p></blockquote><p>Moves the HEAD back and , but keeps them in your working directory.</p><blockquote><p>Perfect when you want to start over on staging but don’t want to lose your code.</p></blockquote><p>This one completely wipes out changes in both the staging area  working directory.</p><blockquote><p>Use it with caution as it  uncommitted work.</p></blockquote><p>If you’ve already pushed a commit to a shared repository and need to undo something,  is your safer bet.</p><p>If you’ve already pushed a commit to a shared repository and need to undo something,  is the safer option.</p><p>Unlike , which rewrites history,  by creating a new commit that reverses the effects of a previous one.</p><p>This doesn’t delete the original commit—it simply adds a new one that undoes the changes from that specific commit.</p><p>What if you only want to copy a specific commit from one branch to another without merging everything? </p><p>That’s where  command comes in.</p><pre><code><strong>git cherry-pick &lt;commit-hash&gt;</strong></code></pre><p>This command takes a single commit from another branch and <strong>applies it to your current branch</strong>, as if you made that change yourself.</p><p>It’s super useful when you need a bug fix or feature from another branch but don’t want to pull in everything.</p><p>Just be mindful that cherry-picking creates a  with the same changes but a different hash, so it’s not identical to the original.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/how-i-mastered-data-structures-and-algorithms?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQ1NjU1MjUyLCJpYXQiOjE3MjE1MjE3MzEsImV4cCI6MTcyNDExMzczMSwiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.2cNY811YEugd5iH9XJQhakBzyahGqF7PcATBlFj5J2w&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":15974,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8a1b87f-c431-48f0-af52-b4f36825841c_811x976.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}