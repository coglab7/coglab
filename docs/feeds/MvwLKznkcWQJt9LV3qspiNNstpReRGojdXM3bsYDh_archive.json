{"id":"MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh","title":"Kubernetes Blog","displayTitle":"Dev - Kubernetes Blog","url":"https://kubernetes.io/feed.xml","feedLink":"https://kubernetes.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":9,"items":[{"title":"Introducing Gateway API Inference Extension","url":"https://kubernetes.io/blog/2025/06/05/introducing-gateway-api-inference-extension/","date":1749081600,"author":"","guid":719,"unread":true,"content":"<p>Modern generative AI and large language model (LLM) services create unique traffic-routing challenges\non Kubernetes. Unlike typical short-lived, stateless web requests, LLM inference sessions are often\nlong-running, resource-intensive, and partially stateful. For example, a single GPU-backed model server\nmay keep multiple inference sessions active and maintain in-memory token caches.</p><p>Traditional load balancers focused on HTTP path or round-robin lack the specialized capabilities needed\nfor these workloads. They also don’t account for model identity or request criticality (e.g., interactive\nchat vs. batch jobs). Organizations often patch together ad-hoc solutions, but a standardized approach\nis missing.</p><h2>Gateway API Inference Extension</h2><p><a href=\"https://gateway-api-inference-extension.sigs.k8s.io/\">Gateway API Inference Extension</a> was created to address\nthis gap by building on the existing <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API</a>, adding inference-specific\nrouting capabilities while retaining the familiar model of Gateways and HTTPRoutes. By adding an inference\nextension to your existing gateway, you effectively transform it into an , enabling you to\nself-host GenAI/LLMs with a “model-as-a-service” mindset.</p><p>The project’s goal is to improve and standardize routing to inference workloads across the ecosystem. Key\nobjectives include enabling model-aware routing, supporting per-request criticalities, facilitating safe model\nroll-outs, and optimizing load balancing based on real-time model metrics. By achieving these, the project aims\nto reduce latency and improve accelerator (GPU) utilization for AI workloads.</p><p>The design introduces two new Custom Resources (CRDs) with distinct responsibilities, each aligning with a\nspecific user persona in the AI/ML serving workflow​:</p><ol><li><p><a href=\"https://gateway-api-inference-extension.sigs.k8s.io/api-types/inferencepool/\">InferencePool</a>\nDefines a pool of pods (model servers) running on shared compute (e.g., GPU nodes). The platform admin can\nconfigure how these pods are deployed, scaled, and balanced. An InferencePool ensures consistent resource\nusage and enforces platform-wide policies. An InferencePool is similar to a Service but specialized for AI/ML\nserving needs and aware of the model-serving protocol.</p></li><li><p><a href=\"https://gateway-api-inference-extension.sigs.k8s.io/api-types/inferencemodel/\">InferenceModel</a>\nA user-facing model endpoint managed by AI/ML owners. It maps a public name (e.g., \"gpt-4-chat\") to the actual\nmodel within an InferencePool. This lets workload owners specify which models (and optional fine-tuning) they\nwant served, plus a traffic-splitting or prioritization policy.</p></li></ol><p>In summary, the InferenceModel API lets AI/ML owners manage what is served, while the InferencePool lets platform\noperators manage where and how it’s served.</p><p>The flow of a request builds on the Gateway API model (Gateways and HTTPRoutes) with one or more extra inference-aware\nsteps (extensions) in the middle. Here’s a high-level example of the request flow with the\n<a href=\"https://gateway-api-inference-extension.sigs.k8s.io/#endpoint-selection-extension\">Endpoint Selection Extension (ESE)</a>:</p><ol><li><p>\nA client sends a request (e.g., an HTTP POST to /completions). The Gateway (like Envoy) examines the HTTPRoute\nand identifies the matching InferencePool backend.</p></li><li><p>\nInstead of simply forwarding to any available pod, the Gateway consults an inference-specific routing extension—\nthe Endpoint Selection Extension—to pick the best of the available pods. This extension examines live pod metrics\n(queue lengths, memory usage, loaded adapters) to choose the ideal pod for the request.</p></li><li><p><strong>Inference-Aware Scheduling</strong>\nThe chosen pod is the one that can handle the request with the lowest latency or highest efficiency, given the\nuser’s criticality or resource needs. The Gateway then forwards traffic to that specific pod.</p></li></ol><p>This extra step provides a smarter, model-aware routing mechanism that still feels like a normal single request to\nthe client. Additionally, the design is extensible—any Inference Gateway can be enhanced with additional inference-specific\nextensions to handle new routing strategies, advanced scheduling logic, or specialized hardware needs. As the project\ncontinues to grow, contributors are encouraged to develop new extensions that are fully compatible with the same underlying\nGateway API model, further expanding the possibilities for efficient and intelligent GenAI/LLM routing.</p><p>We evaluated ​this extension against a standard Kubernetes Service for a <a href=\"https://docs.vllm.ai/en/latest/\">vLLM</a>‐based model\nserving deployment. The test environment consisted of multiple H100 (80 GB) GPU pods running vLLM (<a href=\"https://blog.vllm.ai/2025/01/27/v1-alpha-release.html\">version 1</a>)\non a Kubernetes cluster, with 10 Llama2 model replicas. The <a href=\"https://github.com/AI-Hypercomputer/inference-benchmark\">Latency Profile Generator (LPG)</a>\ntool was used to generate traffic and measure throughput, latency, and other metrics. The\n<a href=\"https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\">ShareGPT</a>\ndataset served as the workload, and traffic was ramped from 100 Queries per Second (QPS) up to 1000 QPS.</p><ul><li><p>: Throughout the tested QPS range, the ESE delivered throughput roughly on par with a standard\nKubernetes Service.</p></li><li><ul><li>: The ​ESE showed significantly lower p90 latency at higher QPS (500+), indicating that\nits model-aware routing decisions reduce queueing and resource contention as GPU memory approaches saturation.</li><li>: Similar trends emerged, with the ​ESE reducing end‐to‐end tail latencies compared to the\nbaseline, particularly as traffic increased beyond 400–500 QPS.</li></ul></li></ul><p>These results suggest that this extension's model‐aware routing significantly reduced latency for GPU‐backed LLM\nworkloads. By dynamically selecting the least‐loaded or best‐performing model server, it avoids hotspots that can\nappear when using traditional load balancing methods for large, long‐running inference requests.</p><p>As the Gateway API Inference Extension heads toward GA, planned features include:</p><ol><li><strong>Prefix-cache aware load balancing</strong> for remote caches</li><li> for automated rollout</li><li> between workloads in the same criticality band</li><li> for scaling based on aggregate, per-model metrics</li><li><strong>Support for large multi-modal inputs/outputs</strong></li><li> (e.g., diffusion models)</li><li><strong>Heterogeneous accelerators</strong> (serving on multiple accelerator types with latency- and cost-aware load balancing)</li><li> for independently scaling pools</li></ol><p>By aligning model serving with Kubernetes-native tooling, Gateway API Inference Extension aims to simplify\nand standardize how AI/ML traffic is routed. With model-aware routing, criticality-based prioritization, and\nmore, it helps ops teams deliver the right LLM services to the right users—smoothly and efficiently.</p><p> Visit the <a href=\"https://gateway-api-inference-extension.sigs.k8s.io/\">project docs</a> to dive deeper,\ngive an Inference Gateway extension a try with a few <a href=\"https://gateway-api-inference-extension.sigs.k8s.io/guides/\">simple steps</a>,\nand <a href=\"https://gateway-api-inference-extension.sigs.k8s.io/contributing/\">get involved</a> if you’re interested in\ncontributing to the project!</p>","contentLength":6365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Start Sidecar First: How To Avoid Snags","url":"https://kubernetes.io/blog/2025/06/03/start-sidecar-first/","date":1748908800,"author":"","guid":718,"unread":true,"content":"<p>From the <a href=\"https://kubernetes.io/blog/2025/04/22/multi-container-pods-overview/\">Kubernetes Multicontainer Pods: An Overview blog post</a> you know what their job is, what are the main architectural patterns, and how they are implemented in Kubernetes. The main thing I’ll cover in this article is how to ensure that your sidecar containers start before the main app. It’s more complicated than you might think!</p><p>I'd just like to remind readers that the <a href=\"https://kubernetes.io/blog/2023/12/13/kubernetes-v1-29-release/\">v1.29.0 release of Kubernetes</a> added native support for\n<a href=\"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/\">sidecar containers</a>, which can now be defined within the  field,\nbut with . You can see that illustrated in the following example Pod manifest snippet:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>What are the specifics of defining sidecars with a  block, rather than as a legacy multi-container pod with multiple ?\nWell, all  are always launched  the main application. If you define Kubernetes-native sidecars, those are terminated  the main application. Furthermore, when used with <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Jobs</a>, a sidecar container should still be alive and could potentially even restart after the owning Job is complete; Kubernetes-native sidecar containers do not block pod completion.</p><p>Now you know that defining a sidecar with this native approach will always start it before the main application. From the <a href=\"https://github.com/kubernetes/kubernetes/blob/537a602195efdc04cdf2cb0368792afad082d9fd/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L827-L830\">kubelet source code</a>, it's visible that this often means being started almost in parallel, and this is not always what an engineer wants to achieve. What I'm really interested in is whether I can delay the start of the main application until the sidecar is not just started, but fully running and ready to serve.\nIt might be a bit tricky because the problem with sidecars is there’s no obvious success signal, contrary to init containers - designed to run only for a specified period of time. With an init container, exit status 0 is unambiguously \"I succeeded\". With a sidecar, there are lots of points at which you can say \"a thing is running\".\nStarting one container only after the previous one is ready is part of a graceful deployment strategy, ensuring proper sequencing and stability during startup. It’s also actually how I’d expect sidecar containers to work as well, to cover the scenario where the main application is dependent on the sidecar. For example, it may happen that an app errors out if the sidecar isn’t available to serve requests (e.g., logging with DataDog). Sure, one could change the application code (and it would actually be the “best practice” solution), but sometimes they can’t - and this post focuses on this use case.</p><p>I'll explain some ways that you might try, and show you what approaches will really work.</p><p>To check whether Kubernetes native sidecar delays the start of the main application until the sidecar is ready, let’s simulate a short investigation. Firstly, I’ll simulate a sidecar container which will never be ready by implementing a readiness probe which will never succeed. As a reminder, a <a href=\"https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/\">readiness probe</a> checks if the container is ready to start accepting traffic and therefore, if the pod can be used as a backend for services.</p><p>(Unlike standard init containers, sidecar containers can have <a href=\"https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/\">probes</a> so that the kubelet can supervise the sidecar and intervene if there are problems. For example, restarting a sidecar container if it fails a health check.)</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"console\"></code></pre></div><p>From these logs it’s evident that only one container is ready - and I know it can’t be the sidecar, because I’ve defined it so it’ll never be ready (you can also check container statuses in ). I also saw that myapp has been started before the sidecar is ready. That was not the result I wanted to achieve; in this case, the main app container has a hard dependency on its sidecar.</p><p>To ensure that the sidecar is ready before the main app container starts, I can define a . It will delay the start of the main container until the command is successfully executed (returns  exit status). If you’re wondering why I’ve added it to my , let’s analyse what happens If I’d added it to myapp container. I wouldn’t have guaranteed the probe would run before the main application code - and this one, can potentially error out without the sidecar being up and running.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This results in 2/2 containers being ready and running, and from events, it can be inferred that the main application started only after nginx had already been started. But to confirm whether it waited for the sidecar readiness, let’s change the  to the exec type of command:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>and run  to watch in real time whether the readiness of both containers only changes after a 15 second delay. Again, events confirm the main application starts after the sidecar.\nThat means that using the  with a correct  request helps to delay the main application start until the sidecar is ready. It’s not optimal, but it works.</p><h2>What about the postStart lifecycle hook?</h2><p>Fun fact: using the  lifecycle hook block will also do the job, but I’d have to write my own mini-shell script, which is even less efficient.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>An interesting exercise would be to check the sidecar container behavior with a <a href=\"https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/\">liveness probe</a>.\nA liveness probe behaves and is configured similarly to a readiness probe - only with the difference that it doesn’t affect the readiness of the container but restarts it in case the probe fails.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>After adding the liveness probe configured just as the previous readiness probe and checking events of the pod by  it’s visible that the sidecar has a restart count above 0. Nevertheless, the main application is not restarted nor influenced at all, even though I'm aware that (in our imaginary worst-case scenario) it can error out when the sidecar is not there serving requests.\nWhat if I’d used a  without lifecycle ? Both containers will be immediately ready: at the beginning, this behavior will not be different from the one without any additional probes since the liveness probe doesn’t affect readiness at all. After a while, the sidecar will begin to restart itself, but it won’t influence the main container.</p><p>I’ll summarize the startup behavior in the table below:</p><table><thead><tr><th>Sidecar starts before the main app?</th><th>Main app waits for the sidecar to be ready?</th><th>What if the check doesn’t pass?</th></tr></thead><tbody><tr><td>, but it’s almost in parallel (effectively )</td><td>Sidecar is not ready; main app continues running</td></tr><tr><td>Yes, but it’s almost in parallel (effectively )</td><td>Sidecar is restarted, main app continues running</td></tr><tr></tr><tr><td>, main app container starts after  completes</td><td>, but you have to provide custom logic for that</td></tr></tbody></table><p>To summarize: with sidecars often being a dependency of the main application, you may want to delay the start of the latter until the sidecar is healthy.\nThe ideal pattern is to start both containers simultaneously and have the app container logic delay at all levels, but it’s not always possible. If that's what you need, you have to use the right kind of customization to the Pod definition. Thankfully, it’s nice and quick, and you have the recipe ready above.</p>","contentLength":6796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gateway API v1.3.0: Advancements in Request Mirroring, CORS, Gateway Merging, and Retry Budgets","url":"https://kubernetes.io/blog/2025/06/02/gateway-api-v1-3/","date":1748883600,"author":"","guid":717,"unread":true,"content":"<p>Join us in the Kubernetes SIG Network community in celebrating the general\navailability of <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API</a> v1.3.0! We are\nalso pleased to announce that there are already a number of conformant\nimplementations to try, made possible by postponing this blog\nannouncement. Version 1.3.0 of the API was released about a month ago on\nApril 24, 2025.</p><p>Gateway API v1.3.0 brings a new feature to the  channel\n(Gateway API's GA release channel): <em>percentage-based request mirroring</em>, and\nintroduces three new experimental features: cross-origin resource sharing (CORS)\nfilters, a standardized mechanism for listener and gateway merging, and retry\nbudgets.</p><h2>Graduation to Standard channel</h2><p>Graduation to the Standard channel is a notable achievement for Gateway API\nfeatures, as inclusion in the Standard release channel denotes a high level of\nconfidence in the API surface and provides guarantees of backward compatibility.\nOf course, as with any other Kubernetes API, Standard channel features can continue\nto evolve with backward-compatible additions over time, and we (SIG Network)\ncertainly expect\nfurther refinements and improvements in the future. For more information on how\nall of this works, refer to the <a href=\"https://gateway-api.sigs.k8s.io/concepts/versioning/\">Gateway API Versioning Policy</a>.</p><h3>Percentage-based request mirroring</h3><p><em>Percentage-based request mirroring</em> is an enhancement to the\nexisting support for <a href=\"https://gateway-api.sigs.k8s.io/guides/http-request-mirroring/\">HTTP request mirroring</a>, which allows HTTP requests to be duplicated to another backend using the\nRequestMirror filter type. Request mirroring is particularly useful in\nblue-green deployment. It can be used to assess the impact of request scaling on\napplication performance without impacting responses to clients.</p><p>The previous mirroring capability worked on all the requests to a .\nPercentage-based request mirroring allows users to specify a subset of requests\nthey want to be mirrored, either by percentage or fraction. This can be\nparticularly useful when services are receiving a large volume of requests.\nInstead of mirroring all of those requests, this new feature can be used to\nmirror a smaller subset of them.</p><p>Here's an example with 42% of the requests to \"foo-v1\" being mirrored to \"foo-v2\":</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>You can also configure the partial mirroring using a fraction. Here is an example\nwith 5 out of every 1000 requests to \"foo-v1\" being mirrored to \"foo-v2\".</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h2>Additions to Experimental channel</h2><p>The Experimental channel is Gateway API's channel for experimenting with new\nfeatures and gaining confidence with them before allowing them to graduate to\nstandard. Please note: the experimental channel may include features that are\nchanged or removed later.</p><p>Starting in release v1.3.0, in an effort to distinguish Experimental channel\nresources from Standard channel resources, any new experimental API kinds have the\nprefix \"\". For the same reason, experimental resources are now added to the\nAPI group <code>gateway.networking.x-k8s.io</code> instead of <code>gateway.networking.k8s.io</code>.\nBear in mind that using new experimental channel resources means they can coexist\nwith standard channel resources, but migrating these resources to the standard\nchannel will require recreating them with the standard channel names and API\ngroup (both of which lack the \"x-k8s\" designator or \"X\" prefix).</p><p>The v1.3 release introduces two new experimental API kinds: XBackendTrafficPolicy\nand XListenerSet. To be able to use experimental API kinds, you need to install\nthe Experimental channel Gateway API YAMLs from the locations listed below.</p><p>Cross-origin resource sharing (CORS) is an HTTP-header based mechanism that allows\na web page to access restricted resources from a server on an origin (domain,\nscheme, or port) different from the domain that served the web page. This feature\nadds a new HTTPRoute  type, called \"CORS\", to configure the handling of\ncross-origin requests before the response is sent back to the client.</p><p>Here's an example of a simple cross-origin configuration:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>In this case, the Gateway returns an  of \"*\", which means that the\nrequested resource can be referenced from any origin, a \n(<code>Access-Control-Allow-Methods</code>) that permits the , , and \nverbs, and a  allowing , ,\n, , and .</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>The complete list of fields in the new CORS filter:</p><ul></ul><h3>XListenerSets (standardized mechanism for Listener and Gateway merging)</h3><p>This release adds a new experimental API kind, XListenerSet, that allows a\nshared list of  to be attached to one or more parent Gateway(s). In\naddition, it expands upon the existing suggestion that Gateway API implementations\nmay merge configuration from multiple Gateway objects. It also:</p><ul><li>adds a new field  to the  of a Gateway. The\n field defines from which Namespaces to select XListenerSets\nthat are allowed to attach to that Gateway: Same, All, None, or Selector based.</li><li>increases the previous maximum number (64) of listeners with the addition of\nXListenerSets.</li><li>allows the delegation of listener configuration, such as TLS, to applications in\nother namespaces.</li></ul><p>The following example shows a Gateway with an HTTP listener and two child HTTPS\nXListenerSets with unique hostnames and certificates. The combined set of listeners\nattached to the Gateway includes the two additional HTTPS listeners in the\nXListenerSets that attach to the Gateway. This example illustrates the\ndelegation of listener TLS config to application owners in different namespaces\n(\"store\" and \"app\"). The HTTPRoute has both the Gateway listener named \"foo\" and\none XListenerSet listener named \"second\" as .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Each listener in a Gateway must have a unique combination of , ,\n(and  if supported by the protocol) in order for all listeners to be\n and not conflicted over which traffic they should receive.</p><p>Furthermore, implementations can  separate Gateways into a single set of\nlistener addresses if all listeners across those Gateways are compatible. The\nmanagement of merged listeners was under-specified in releases prior to v1.3.0.</p><p>With the new feature, the specification on merging is expanded. Implementations\nmust treat the parent Gateways as having the merged list of all listeners from\nitself and from attached XListenerSets, and validation of this list of listeners\nmust behave the same as if the list were part of a single Gateway. Within a single\nGateway, listeners are ordered using the following precedence:</p><ol><li>Single Listeners (not a part of an XListenerSet) first,</li><li>Remaining listeners ordered by:\n<ul><li>object creation time (oldest first), and if two listeners are defined in\nobjects that have the same timestamp, then</li><li>alphabetically based on \"{namespace}/{name of listener}\"</li></ul></li></ol><h3>Retry budgets (XBackendTrafficPolicy)</h3><p>This feature allows you to configure a  across all endpoints\nof a destination Service. This is used to limit additional client-side retries\nafter reaching a configured threshold. When configuring the budget, the maximum\npercentage of active requests that may consist of retries may be specified, as well as\nthe interval over which requests will be considered when calculating the threshold\nfor retries. The development of this specification changed the existing\nexperimental API kind BackendLBPolicy into a new experimental API kind,\nXBackendTrafficPolicy, in the interest of reducing the proliferation of policy\nresources that had commonalities.</p><p>The following example shows an XBackendTrafficPolicy that applies a\n that represents a budget that limits the retries to a maximum\nof 20% of requests, over a duration of 10 seconds, and to a minimum of 3 retries\nover 1 second.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Unlike other Kubernetes APIs, you don't need to upgrade to the latest version of\nKubernetes to get the latest version of Gateway API. As long as you're running\nKubernetes 1.26 or later, you'll be able to get up and running with this version\nof Gateway API.</p><p>To try out the API, follow the <a href=\"https://gateway-api.sigs.k8s.io/guides/\">Getting Started Guide</a>.\nAs of this writing, four implementations are already conformant with Gateway API\nv1.3 experimental channel features. In alphabetical order:</p><p>Wondering when a feature will be added? There are lots of opportunities to get\ninvolved and help define the future of Kubernetes routing APIs for both ingress\nand service mesh.</p><p>The maintainers would like to thank  who's contributed to Gateway\nAPI, whether in the form of commits to the repo, discussion, ideas, or general\nsupport. We could never have made this kind of progress without the support of\nthis dedicated and active community.</p><h2>Related Kubernetes blog articles</h2>","contentLength":8284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.33: In-Place Pod Resize Graduated to Beta","url":"https://kubernetes.io/blog/2025/05/16/kubernetes-v1-33-in-place-pod-resize-beta/","date":1747420200,"author":"","guid":716,"unread":true,"content":"<p>On behalf of the Kubernetes project, I am excited to announce that the  feature (also known as In-Place Pod Vertical Scaling), first introduced as alpha in Kubernetes v1.27, has graduated to  and will be enabled by default in the Kubernetes v1.33 release! This marks a significant milestone in making resource management for Kubernetes workloads more flexible and less disruptive.</p><h2>What is in-place Pod resize?</h2><p>Traditionally, changing the CPU or memory resources allocated to a container required restarting the Pod. While acceptable for many stateless applications, this could be disruptive for stateful services, batch jobs, or any workloads sensitive to restarts.</p><p>In-place Pod resizing allows you to change the CPU and memory requests and limits assigned to containers within a  Pod, often without requiring a container restart.</p><ul><li>The <code>spec.containers[*].resources</code> field in a Pod specification now represents the  resources and is mutable for CPU and memory.</li><li>The <code>status.containerStatuses[*].resources</code> field reflects the  resources currently configured on a running container.</li><li>You can trigger a resize by updating the desired resources in the Pod spec via the new  subresource.</li></ul><p>You can try it out on a v1.33 Kubernetes cluster by using kubectl to edit a Pod (requires  v1.32+):</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><h2>Why does in-place Pod resize matter?</h2><p>Kubernetes still excels at scaling workloads horizontally (adding or removing replicas), but in-place Pod resizing unlocks several key benefits for vertical scaling:</p><ul><li> Stateful applications, long-running batch jobs, and sensitive workloads can have their resources adjusted without suffering the downtime or state loss associated with a Pod restart.</li><li><strong>Improved Resource Utilization:</strong> Scale down over-provisioned Pods without disruption, freeing up resources in the cluster. Conversely, provide more resources to Pods under heavy load without needing a restart.</li><li> Address transient resource needs more quickly. For example Java applications often need more CPU during startup than during steady-state operation. Start with higher CPU and resize down later.</li></ul><h2>What's changed between Alpha and Beta?</h2><p>Since the alpha release in v1.27, significant work has gone into maturing the feature, improving its stability, and refining the user experience based on feedback and further development. Here are the key changes:</p><h3>Notable user-facing changes</h3><ul><li> Modifying Pod resources must now be done via the Pod's  subresource (<code>kubectl patch pod &lt;name&gt; --subresource resize ...</code>).  versions v1.32+ support this argument.</li><li><strong>Resize Status via Conditions:</strong> The old  field is deprecated. The status of a resize operation is now exposed via two Pod conditions:\n<ul><li>: Indicates the Kubelet cannot grant the resize immediately (e.g.,  if temporarily unable,  if impossible on the node).</li><li>: Indicates the resize is accepted and being applied. Errors encountered during this phase are now reported in this condition's message with .</li></ul></li></ul><h3>Stability and reliability enhancements</h3><ul><li><strong>Refined Allocated Resources Management:</strong> The allocation management logic with the Kubelet was significantly reworked, making it more consistent and robust. The changes eliminated whole classes of bugs, and greatly improved the reliability of in-place Pod resize.</li><li><strong>Improved Checkpointing &amp; State Tracking:</strong> A more robust system for tracking \"allocated\" and \"actuated\" resources was implemented, using new checkpoint files (, ) to reliably manage resize state across Kubelet restarts and handle edge cases where runtime-reported resources differ from requested ones. Several bugs related to checkpointing and state restoration were fixed. Checkpointing efficiency was also improved.</li><li> Enhancements to the Kubelet's Pod Lifecycle Event Generator (PLEG) allow the Kubelet to respond to and complete resizes much more quickly.</li><li><strong>Enhanced CRI Integration:</strong> A new <code>UpdatePodSandboxResources</code> CRI call was added to better inform runtimes and plugins (like NRI) about Pod-level resource changes.</li><li> Addressed issues related to systemd cgroup drivers, handling of containers without limits, CPU minimum share calculations, container restart backoffs, error propagation, test stability, and more.</li></ul><p>Graduating to Beta means the feature is ready for broader adoption, but development doesn't stop here! Here's what the community is focusing on next:</p><ul><li><strong>Stability and Productionization:</strong> Continued focus on hardening the feature, improving performance, and ensuring it is robust for production environments.</li><li> Working towards relaxing some of the current limitations noted in the documentation, such as allowing memory limit decreases.</li><li> Work to enable VPA to leverage in-place Pod resize is already underway. A new  update mode will allow it to attempt non-disruptive resizes first, or fall back to recreation if needed. This will allow users to benefit from VPA's recommendations with significantly less disruption.</li><li> Gathering feedback from users adopting the beta feature is crucial for prioritizing further enhancements and addressing any uncovered issues or bugs.</li></ul><h2>Getting started and providing feedback</h2><p>With the <code>InPlacePodVerticalScaling</code> feature gate enabled by default in v1.33, you can start experimenting with in-place Pod resizing right away!</p><p>As this feature moves through Beta, your feedback is invaluable. Please report any issues or share your experiences via the standard Kubernetes communication channels (GitHub issues, mailing lists, Slack). You can also review the <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1287-in-place-update-pod-resources\">KEP-1287: In-place Update of Pod Resources</a> for the full in-depth design details.</p><p>We look forward to seeing how the community leverages in-place Pod resize to build more efficient and resilient applications on Kubernetes!</p>","contentLength":5578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing etcd v3.6.0","url":"https://kubernetes.io/blog/2025/05/15/announcing-etcd-3.6/","date":1747353600,"author":"","guid":715,"unread":true,"content":"<p><em>This announcement originally <a href=\"https://etcd.io/blog/2025/announcing-etcd-3.6/\">appeared</a> on the etcd blog.</em></p><p>Today, we are releasing <a href=\"https://github.com/etcd-io/etcd/releases/tag/v3.6.0\">etcd v3.6.0</a>, the first minor release since etcd v3.5.0 on June 15, 2021. This release\nintroduces several new features, makes significant progress on long-standing efforts like downgrade support and\nmigration to v3store, and addresses numerous critical &amp; major issues. It also includes major optimizations in\nmemory usage, improving efficiency and performance.</p><p>In addition to the features of v3.6.0, etcd has joined Kubernetes as a SIG (sig-etcd), enabling us to improve\nproject sustainability. We've introduced systematic robustness testing to ensure correctness and reliability.\nThrough the etcd-operator Working Group, we plan to improve usability as well.</p><p>What follows are the most significant changes introduced in etcd v3.6.0, along with the discussion of the\nroadmap for future development. For a detailed list of changes, please refer to the <a href=\"https://github.com/etcd-io/etcd/blob/main/CHANGELOG/CHANGELOG-3.6.md\">CHANGELOG-3.6</a>.</p><p>A heartfelt thank you to all the contributors who made this release possible!</p><p>etcd takes security seriously. To enhance software security in v3.6.0, we have improved our workflow checks by\nintegrating  to scan the source code and  to scan container images. These improvements\nhave also been backported to supported stable releases.</p><p>The v2store has been deprecated since etcd v3.4 but could still be enabled via . It remained the source of\ntruth for membership data. In etcd v3.6.0, v2store can no longer be enabled as the  flag has been removed,\nand v3store has become the sole source of truth for membership data.</p><p>While v2store still exists in v3.6.0, etcd will fail to start if it contains any data other than membership information.\nTo assist with migration, etcd v3.5.18+ provides the  command, which verifies that v2store\ncontains only membership data (see <a href=\"https://github.com/etcd-io/etcd/pull/19113\">PR 19113</a>).</p><p>Compared to v2store, v3store offers better performance and transactional support. It is also the actively maintained\nstorage engine moving forward.</p><p>The removal of v2store is still ongoing and is tracked in <a href=\"https://github.com/etcd-io/etcd/issues/12913\">issues/12913</a>.</p><p>etcd v3.6.0 is the first version to fully support downgrade. The effort for this downgrade task spans\nboth versions 3.5 and 3.6, and all related work is tracked in <a href=\"https://github.com/etcd-io/etcd/issues/11716\">issues/11716</a>.</p><p>At a high level, the process involves migrating the data schema to the target version (e.g., v3.5),\nfollowed by a rolling downgrade.</p><p>Ensure the cluster is healthy, and take a snapshot backup. Validate whether the downgrade is valid:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>If the downgrade is valid, enable downgrade mode:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>etcd will then migrate the data schema in the background. Once complete, proceed with the rolling downgrade.</p><p>In etcd v3.6.0, we introduced Kubernetes-style feature gates for managing new features. Previously, we\nindicated unstable features through the  prefix in feature flag names. The prefix was removed\nonce the feature was stable, causing a breaking change. Now, features will start in Alpha, progress\nto Beta, then GA, or get deprecated. This ensures a much smoother upgrade and downgrade experience for users.</p><p>etcd now supports  and  endpoints, aligning with Kubernetes' Liveness and Readiness probes.\n indicates whether the etcd instance is alive, while  indicates when it is ready to serve requests.\nThis feature has also been backported to release-3.5 (starting from v3.5.11) and release-3.4 (starting from v3.4.29).\nSee <a href=\"https://etcd.io/docs/v3.6/op-guide/monitoring/\">livez/readyz</a> for details.</p><p>The existing  endpoint remains functional.  is similar to <code>/health?serializable=true</code>, while\n is similar to  or <code>/health?serializable=false</code>. Clearly, the  and \nendpoints provide clearer semantics and are easier to understand.</p><p>In etcd v3.6.0, the new discovery protocol <a href=\"https://etcd.io/docs/v3.6/dev-internal/discovery_protocol/\">v3discovery</a> was introduced, based on clientv3.\nIt facilitates the discovery of all cluster members during the bootstrap phase.</p><p>The previous <a href=\"https://etcd.io/docs/v3.5/dev-internal/discovery_protocol/\">v2discovery</a> protocol, based on clientv2, has been deprecated. Additionally,\nthe public discovery service at <a href=\"https://discovery.etcd.io/\">https://discovery.etcd.io/</a>, which relied on v2discovery, is no longer maintained.</p><p>In this release, we reduced average memory consumption by at least 50% (see Figure 1). This improvement is primarily due to two changes:</p><ul><li>The default value of  has been reduced from 100,000 in v3.5 to 10,000 in v3.6. As a result, etcd v3.6 now retains only about 10% of the history records compared to v3.5.</li><li>Raft history is compacted more frequently, as introduced in <a href=\"https://github.com/etcd-io/etcd/pull/18825\">PR/18825</a>.</li></ul><p><em> Memory usage comparison between etcd v3.5.20 and v3.6.0-rc.2 under different read/write ratios.\nEach subplot shows the memory usage over time with a specific read/write ratio. The red line represents etcd\nv3.5.20, while the teal line represents v3.6.0-rc.2. Across all tested ratios, v3.6.0-rc.2 exhibits lower and\nmore stable memory usage.</em></p><p>Compared to v3.5, etcd v3.6 delivers an average performance improvement of approximately 10%\nin both read and write throughput (see Figure 2, 3, 4 and 5). This improvement is not attributed to\nany single major change, but rather the cumulative effect of multiple minor enhancements. One such\nexample is the optimization of the free page queries introduced in <a href=\"https://github.com/etcd-io/bbolt/pull/419\">PR/419</a>.</p><p><em> Read throughput comparison between etcd v3.5.20 and v3.6.0-rc.2 under a high write ratio. The\nread/write ratio is 0.0078, meaning 1 read per 128 writes. The right bar shows the percentage improvement\nin read throughput of v3.6.0-rc.2 over v3.5.20, ranging from 3.21% to 25.59%.</em></p><p><em> Read throughput comparison between etcd v3.5.20 and v3.6.0-rc.2 under a high read ratio.\nThe read/write ratio is 8, meaning 8 reads per write. The right bar shows the percentage improvement in\nread throughput of v3.6.0-rc.2 over v3.5.20, ranging from 4.38% to 27.20%.</em></p><p><em> Write throughput comparison between etcd v3.5.20 and v3.6.0-rc.2 under a high write ratio. The\nread/write ratio is 0.0078, meaning 1 read per 128 writes. The right bar shows the percentage improvement\nin write throughput of v3.6.0-rc.2 over v3.5.20, ranging from 2.95% to 24.24%.</em></p><p><em> Write throughput comparison between etcd v3.5.20 and v3.6.0-rc.2 under a high read ratio.\nThe read/write ratio is 8, meaning 8 reads per write. The right bar shows the percentage improvement in\nwrite throughput of v3.6.0-rc.2 over v3.5.20, ranging from 3.86% to 28.37%.</em></p><p>Old binaries are incompatible with new schema versions</p><p>Old etcd binaries are not compatible with newer data schema versions. For example, etcd 3.5 cannot start with\ndata created by etcd 3.6, and etcd 3.4 cannot start with data created by either 3.5 or 3.6.</p><p>When downgrading etcd, it's important to follow the documented downgrade procedure. Simply replacing\nthe binary or image will result in the incompatibility issue.</p><h3>Peer endpoints no longer serve client requests</h3><p>Client endpoints () are intended to serve client requests only, while peer\nendpoints (<code>--initial-advertise-peer-urls</code>) are intended solely for peer communication. However, due to an implementation\noversight, the peer endpoints were also able to handle client requests in etcd 3.4 and 3.5. This behavior was misleading and\nencouraged incorrect usage patterns. In etcd 3.6, this misleading behavior was corrected via <a href=\"https://github.com/etcd-io/etcd/pull/13565\">PR/13565</a>; peer endpoints\nno longer serve client requests.</p><h3>Clear boundary between etcdctl and etcdutl</h3><p>Both  and  are command line tools.  is an offline utility designed to operate directly on\netcd data files, while  is an online tool that interacts with etcd over a network. Previously, there were some\noverlapping functionalities between the two, but these overlaps were removed in 3.6.0.</p><ul><li><p>Removed <code>etcdctl defrag --data-dir</code></p><p>The  command only support online defragmentation and no longer supports offline defragmentation.\nTo perform offline defragmentation, use the <code>etcdutl defrag --data-dir</code> command instead.</p></li><li><p>Removed </p><p> no longer supports retrieving the status of a snapshot. Use the  command instead.</p></li><li><p>Removed </p><p> no longer supports restoring from a snapshot. Use the  command instead.</p></li></ul><p>Correctness has always been a top priority for the etcd project. In the process of developing 3.6.0, we found and\nfixed a few notable bugs that could lead to data inconsistency in specific cases. These fixes have been backported\nto previous releases, but we believe they deserve special mention here.</p><ul><li>Data Inconsistency when Crashing Under Load</li></ul><p>Previously, when etcd was applying data, it would update the consistent-index first, followed by committing the\ndata. However, these operations were not atomic. If etcd crashed in between, it could lead to data inconsistency\n(see <a href=\"https://github.com/etcd-io/etcd/issues/13766\">issue/13766</a>). The issue was introduced in v3.5.0, and fixed in v3.5.3 with <a href=\"https://github.com/etcd-io/etcd/pull/13854\">PR/13854</a>.</p><ul><li>Durability API guarantee broken in single node cluster</li></ul><p>When a client writes data and receives a success response, the data is expected to be persisted. However, the data might\nbe lost if etcd crashes immediately after sending the success response to the client. This was a legacy issue (see <a href=\"https://github.com/etcd-io/etcd/issues/14370\">issue/14370</a>)\naffecting all previous releases. It was addressed in v3.4.21 and v3.5.5 with <a href=\"https://github.com/etcd-io/etcd/pull/14400\">PR/14400</a>, and fixed in raft side in\nmain branch (now release-3.6) with <a href=\"https://github.com/etcd-io/etcd/pull/14413\">PR/14413</a>.</p><ul><li>Revision Inconsistency when Crashing During Defragmentation</li></ul><p>If etcd crashed during the defragmentation operation, upon restart, it might reapply\nsome entries which had already been applied, accordingly leading to the revision inconsistency issue\n(see the discussions in <a href=\"https://github.com/etcd-io/etcd/pull/14685\">PR/14685</a>). The issue was introduced in v3.5.0, and fixed in v3.5.6 with <a href=\"https://github.com/etcd-io/etcd/pull/14730\">PR/14730</a>.</p><p>The issue was introduced in etcd v3.5.1, and resolved in v3.5.20.</p><p>: users are required to first upgrade to etcd v3.5.20 (or a higher patch version) before upgrading\nto etcd v3.6.0; otherwise, the upgrade may fail.</p><p>We introduced the <a href=\"https://github.com/etcd-io/etcd/tree/main/tests/robustness\">Robustness testing</a> to verify correctness, which has always been our top priority.\nIt plays traffic of various types and volumes against an etcd cluster, concurrently injects a random\nfailpoint, records all operations (including both requests and responses), and finally performs a\nlinearizability check. It also verifies that the <a href=\"https://etcd.io/docs/v3.5/learning/api_guarantees/#watch-apis\">Watch APIs</a> guarantees have not been violated.\nThe robustness test increases our confidence in ensuring the quality of each etcd release.</p><p>We have migrated most of the etcd workflow tests to Kubernetes' Prow testing infrastructure to\ntake advantage of its benefit, such as nice dashboards for viewing test results and the ability\nfor contributors to rerun failed tests themselves.</p><p>While retaining all existing supported platforms, we have promoted Linux/ARM64 to Tier 1 support.\nFor more details, please refer to <a href=\"https://github.com/etcd-io/etcd/issues/15951\">issues/15951</a>. For the complete list of supported platforms,\nsee <a href=\"https://etcd.io/docs/v3.6/op-guide/supported-platform/\">supported-platform</a>.</p><p>We have published an official guide on how to bump dependencies for etcd’s main branch and stable releases.\nIt also covers how to update the Go version. For more details, please refer to <a href=\"https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/dependency_management.md\">dependency_management</a>.\nWith this guide available, any contributors can now help with dependency upgrades.</p><p><a href=\"https://github.com/etcd-io/bbolt\">bbolt</a> and <a href=\"https://github.com/etcd-io/raft\">raft</a> are two core dependencies of etcd.</p><p>Both etcd v3.4 and v3.5 depend on bbolt v1.3, while etcd v3.6 depends on bbolt v1.4.</p><p>For the release-3.4 and release-3.5 branches, raft is included in the etcd repository itself, so etcd v3.4 and v3.5\ndo not depend on an external raft module. Starting from etcd v3.6, raft was moved to a separate repository (<a href=\"https://github.com/etcd-io/raft\">raft</a>),\nand the first standalone raft release is v3.6.0. As a result, etcd v3.6.0 depends on raft v3.6.0.</p><p>Please see the table below for a summary:</p><table><thead><tr></tr></thead><tbody></tbody></table><p>We upgraded <a href=\"https://github.com/grpc-ecosystem/grpc-gateway\">grpc-gateway</a> from v1 to v2 via <a href=\"https://github.com/etcd-io/etcd/pull/16595\">PR/16595</a> in etcd v3.6.0. This is a major step toward\nmigrating to <a href=\"https://github.com/protocolbuffers/protobuf-go\">protobuf-go</a>, the second major version of the Go protocol buffer API implementation.</p><p>grpc-gateway@v2 is designed to work with <a href=\"https://github.com/protocolbuffers/protobuf-go\">protobuf-go</a>. However, etcd v3.6 still depends on the deprecated\n<a href=\"https://github.com/gogo/protobuf\">gogo/protobuf</a>, which is actually protocol buffer v1 implementation. To resolve this incompatibility,\nwe applied a <a href=\"https://github.com/etcd-io/etcd/blob/158b9e0d468d310c3edf4cf13f2458c51b0406fa/scripts/genproto.sh#L151-L184\">patch</a> to the generated *.pb.gw.go files to convert v1 messages to v2 messages.</p><h3>grpc-ecosystem/go-grpc-middleware/providers/prometheus</h3><p>There are exciting developments in the etcd community that reflect our ongoing commitment\nto strengthening collaboration, improving maintainability, and evolving the project’s governance.</p><p>etcd has officially become a Kubernetes Special Interest Group: SIG-etcd. This change reflects\netcd’s critical role as the primary datastore for Kubernetes and establishes a more structured\nand transparent home for long-term stewardship and cross-project collaboration. The new SIG\ndesignation will help streamline decision-making, align roadmaps with Kubernetes needs,\nand attract broader community involvement.</p><h3>New contributors, maintainers, and reviewers</h3><p>We’ve seen increasing engagement from contributors, which has resulted in the addition of three new maintainers:</p><p>Their continued contributions have been instrumental in driving the project forward.</p><p>We also welcome two new reviewers to the project:</p><p>We appreciate their dedication to code quality and their willingness to take on broader review responsibilities\nwithin the community.</p><p>We've formed a new release team led by <a href=\"https://github.com/ivanvc\">ivanvc</a> and <a href=\"https://github.com/jmhbnz\">jmhbnz</a>, streamlining the release process by automating\nmany previously manual steps. Inspired by Kubernetes SIG Release, we've adopted several best practices, including\nclearly defined release team roles and the introduction of release shadows to support knowledge sharing and team\nsustainability. These changes have made our releases smoother and more reliable, allowing us to approach each\nrelease with greater confidence and consistency.</p><h3>Introducing the etcd Operator Working Group</h3><p>To further advance etcd’s operational excellence, we have formed a new working group: <a href=\"https://github.com/kubernetes/community/tree/master/wg-etcd-operator\">WG-etcd-operator</a>.\nThe working group is dedicated to enabling the automatic and efficient operation of etcd clusters that run in\nthe Kubernetes environment using an etcd-operator.</p><p>The legacy v2store has been deprecated since etcd v3.4, and the flag  was removed entirely in v3.6.\nThis means that starting from v3.6, there is no longer a way to enable or use the v2store. However, etcd still\nbootstraps internally from the legacy v2 snapshots. To address this inconsistency, We plan to change etcd to\nbootstrap from the v3store and replay the WAL entries based on the . The work is being tracked\nin <a href=\"https://github.com/etcd-io/etcd/issues/12913\">issues/12913</a>.</p><p>One of the most persistent challenges remains the large range of queries from the kube-apiserver, which can\nlead to process crashes due to their unpredictable nature. The range stream feature, originally outlined in\nthe <a href=\"https://etcd.io/blog/2021/announcing-etcd-3.5/#future-roadmaps\">v3.5 release blog/Future roadmaps</a>, remains an idea worth revisiting to address the challenges of large\nrange queries.</p><p>For more details and upcoming plans, please refer to the <a href=\"https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/roadmap.md\">etcd roadmap</a>.</p>","contentLength":14377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes 1.33: Job's SuccessPolicy Goes GA","url":"https://kubernetes.io/blog/2025/05/15/kubernetes-1-33-jobs-success-policy-goes-ga/","date":1747333800,"author":"","guid":714,"unread":true,"content":"<p>On behalf of the Kubernetes project, I'm pleased to announce that Job  has graduated to General Availability (GA) as part of the v1.33 release.</p><h2>About Job's Success Policy</h2><p>In batch workloads, you might want to use leader-follower patterns like <a href=\"https://en.wikipedia.org/wiki/Message_Passing_Interface\">MPI</a>,\nin which the leader controls the execution, including the followers' lifecycle.</p><p>In this case, you might want to mark it as succeeded\neven if some of the indexes failed. Unfortunately, a leader-follower Kubernetes Job that didn't use a success policy, in most cases, would have to require  Pods to finish successfully\nfor that Job to reach an overall succeeded state.</p><p>For Kubernetes Jobs, the API allows you to specify the early exit criteria using the \nfield (you can only use the  field for an <a href=\"https://kubernetes.io/docs/concept/workloads/controllers/job/#completion-mode\">indexed Job</a>).\nWhich describes a set of rules either using a list of succeeded indexes for a job, or defining a minimal required size of succeeded indexes.</p><p>This newly stable field is especially valuable for scientific simulation, AI/ML and High-Performance Computing (HPC) batch workloads.\nUsers in these areas often run numerous experiments and may only need a specific number to complete successfully, rather than requiring all of them to succeed.\nIn this case, the leader index failure is the only relevant Job exit criteria, and the outcomes for individual follower Pods are handled\nonly indirectly via the status of the leader index.\nMoreover, followers do not know when they can terminate themselves.</p><p>After Job meets any , the Job is marked as succeeded, and all Pods are terminated including the running ones.</p><p>The following excerpt from a Job manifest, using <code>.successPolicy.rules[0].succeededCount</code>, shows an example of\nusing a custom success policy:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Here, the Job is marked as succeeded when one index succeeded regardless of its number.\nAdditionally, you can constrain index numbers against  in <code>.successPolicy.rules[0].succeededCount</code>\nas shown below:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This example shows that the Job will be marked as succeeded once a Pod with a specific index (Pod index 0) has succeeded.</p><p>Once the Job either reaches one of the  rules, or achieves its  criteria based on ,\nthe Job controller within kube-controller-manager adds the  condition to the Job status.\nAfter that, the job-controller initiates cleanup and termination of Pods for Jobs with  condition.\nEventually, Jobs obtain  condition when the job-controller finished cleanup and termination.</p><p>If you are interested in working on new features in the space I recommend\nsubscribing to our <a href=\"https://kubernetes.slack.com/messages/wg-batch\">Slack</a>\nchannel and attending the regular community meetings.</p>","contentLength":2528,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.33: Updates to Container Lifecycle","url":"https://kubernetes.io/blog/2025/05/14/kubernetes-v1-33-updates-to-container-lifecycle/","date":1747247400,"author":"","guid":713,"unread":true,"content":"<p>Kubernetes v1.33 introduces a few updates to the lifecycle of containers. The Sleep action for container lifecycle hooks now supports a zero sleep duration (feature enabled by default).\nThere is also alpha support for customizing the stop signal sent to containers when they are being terminated.</p><p>This blog post goes into the details of these new aspects of the container lifecycle, and how you can use them.</p><h2>Zero value for Sleep action</h2><p>Kubernetes v1.29 introduced the  action for container PreStop and PostStart Lifecycle hooks. The Sleep action lets your containers pause for a specified duration after the container is started or before it is terminated. This was needed to provide a straightforward way to manage graceful shutdowns. Before the Sleep action, folks used to run the  command using the exec action in their container lifecycle hooks. If you wanted to do this you'd need to have the binary for the  command in your container image. This is difficult if you're using third party images.</p><p>The sleep action when it was added initially didn't have support for a sleep duration of zero seconds. The  which the Sleep action uses under the hood supports a duration of zero seconds. Using a negative or a zero value for the sleep returns immediately, resulting in a no-op. We wanted the same behaviour with the sleep action. This support for the zero duration was later added in v1.32, with the <code>PodLifecycleSleepActionAllowZero</code> feature gate.</p><p>The <code>PodLifecycleSleepActionAllowZero</code> feature gate has graduated to beta in v1.33, and is now enabled by default.\nThe original Sleep action for  and  hooks is been enabled by default, starting from Kubernetes v1.30.\nWith a cluster running Kubernetes v1.33, you are able to set a\nzero duration for sleep lifecycle hooks. For a cluster with default configuration, you don't need\nto enable any feature gate to make that possible.</p><p>Container runtimes such as containerd and CRI-O honor a  instruction in the container image definition. This can be used to specify a custom stop signal\nthat the runtime will used to terminate containers based on that image.\nStop signal configuration was not originally part of the Pod API in Kubernetes.\nUntil Kubernetes v1.33, the only way to override the stop signal for containers was by rebuilding your container image with the new custom stop signal\n(for example, specifying  in a  or ).</p><p>The  feature gate which is newly added in Kubernetes v1.33 adds stop signals to the Kubernetes API. This allows users to specify a custom stop signal in the container spec. Stop signals are added to the API as a new lifecycle along with the existing PreStop and PostStart lifecycle handlers. In order to use this feature, we expect the Pod to have the operating system specified with . This is enforced so that we can cross-validate the stop signal against the operating system and make sure that the containers in the Pod are created with a valid stop signal for the operating system the Pod is being scheduled to. For Pods scheduled on Windows nodes, only  and  are allowed as valid stop signals. Find the full list of signals supported in Linux nodes <a href=\"https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/core/v1/types.go#L2985-L3053\">here</a>.</p><p>If a container has a custom stop signal defined in its lifecycle, the container runtime would use the signal defined in the lifecycle to kill the container, given that the container runtime also supports custom stop signals. If there is no custom stop signal defined in the container lifecycle, the runtime would fallback to the stop signal defined in the container image. If there is no stop signal defined in the container image, the default stop signal of the runtime would be used. The default signal is  for both containerd and CRI-O.</p><p>For the feature to work as intended, both the versions of Kubernetes and the container runtime should support container stop signals. The changes to the Kuberentes API and kubelet are available in alpha stage from v1.33, which can be enabled with the  feature gate. The container runtime implementations for containerd and CRI-O are still a work in progress and will be rolled out soon.</p><h3>Using container stop signals</h3><p>To enable this feature, you need to turn on the  feature gate in both the kube-apiserver and the kubelet. Once you have nodes where the feature gate is turned on, you can create Pods with a StopSignal lifecycle and a valid OS name like so:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Do note that the  signal in this example can only be used if the container's Pod is scheduled to a Linux node. Hence we need to specify  as  to be able to use the signal. You will only be able to configure  and  signals if the Pod is being scheduled to a Windows node. You cannot specify a <code>containers[*].lifecycle.stopSignal</code> if the  field is nil or unset either.</p><p>This feature is driven by the <a href=\"https://github.com/Kubernetes/community/blob/master/sig-node/README.md\">SIG Node</a>. If you are interested in helping develop this feature, sharing feedback, or participating in any other ongoing SIG Node projects, please reach out to us!</p><p>You can reach SIG Node by several means:</p><p>You can also contact me directly:</p><ul><li>GitHub: @sreeram-venkitesh</li><li>Slack: @sreeram.venkitesh</li></ul>","contentLength":4992,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.33: Job's Backoff Limit Per Index Goes GA","url":"https://kubernetes.io/blog/2025/05/13/kubernetes-v1-33-jobs-backoff-limit-per-index-goes-ga/","date":1747161000,"author":"","guid":712,"unread":true,"content":"<p>In Kubernetes v1.33, the  feature reaches general\navailability (GA). This blog describes the Backoff Limit Per Index feature and\nits benefits.</p><h2>About backoff limit per index</h2><p>When you run workloads on Kubernetes, you must consider scenarios where Pod\nfailures can affect the completion of your workloads. Ideally, your workload\nshould tolerate transient failures and continue running.</p><p>To achieve failure tolerance in a Kubernetes Job, you can set the\n field. This field specifies the total number of tolerated\nfailures.</p><p>However, for workloads where every index is considered independent, like\n<a href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\">embarassingly parallel</a>\nworkloads - the  field is often not flexible enough.\nFor example, you may choose to run multiple suites of integration tests by\nrepresenting each suite as an index within an <a href=\"https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/\">Indexed Job</a>.\nIn that setup, a fast-failing index (test suite) is likely to consume your\nentire budget for tolerating Pod failures, and you might not be able to run the\nother indexes.</p><p>In order to address this limitation, Kubernetes introduced ,\nwhich allows you to control the number of retries per index.</p><h2>How backoff limit per index works</h2><p>To use Backoff Limit Per Index for Indexed Jobs, specify the number of tolerated\nPod failures per index with the <code>spec.backoffLimitPerIndex</code> field. When you set\nthis field, the Job executes all indexes by default.</p><p>Additionally, to fine-tune the error handling:</p><ul><li>Specify the cap on the total number of failed indexes by setting the\n field. When the limit is exceeded the entire Job is\nterminated.</li><li>Define a short-circuit to detect a failed index by using the  action in the\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy\">Pod Failure Policy</a>\nmechanism.</li></ul><p>When the number of tolerated failures is exceeded, the Job marks that index as\nfailed and lists it in the Job's  field.</p><p>The following Job spec snippet is an example of how to combine backoff limit per\nindex with the  feature:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>In this example, the Job handles Pod failures as follows:</p><ul><li>Ignores any failed Pods that have the built-in\n<a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-conditions\">disruption condition</a>,\ncalled . These Pods don't count towards Job backoff limits.</li><li>Fails the index corresponding to the failed Pod if any of the failed Pod's\ncontainers finished with the exit code 42 - based on the matching \"FailIndex\"\nrule.</li><li>Retries the first failure of any index, unless the index failed due to the\nmatching  rule.</li><li>Fails the entire Job if the number of failed indexes exceeded 5 (set by the\n field).</li></ul><p>If you are interested in working on new features in the space we recommend\nsubscribing to our <a href=\"https://kubernetes.slack.com/messages/wg-batch\">Slack</a>\nchannel and attending the regular community meetings.</p>","contentLength":2509,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.33: Image Pull Policy the way you always thought it worked!","url":"https://kubernetes.io/blog/2025/05/12/kubernetes-v1-33-ensure-secret-pulled-images-alpha/","date":1747074600,"author":"","guid":711,"unread":true,"content":"<h2>Image Pull Policy the way you always thought it worked!</h2><p>Some things in Kubernetes are surprising, and the way  behaves might\nbe one of them. Given Kubernetes is all about running pods, it may be peculiar\nto learn that there has been a caveat to restricting pod access to authenticated images for\nover 10 years in the form of <a href=\"https://github.com/kubernetes/kubernetes/issues/18787\">issue 18787</a>!\nIt is an exciting release when you can resolve a ten-year-old issue.</p><div role=\"alert\">Throughout this blog post, the term \"pod credentials\" will be used often. In this context,\nthe term generally encapsulates the authentication material that is available to a pod\nto authenticate a container image pull.</div><h2>IfNotPresent, even if I'm not supposed to have it</h2><p>The gist of the problem is that the <code>imagePullPolicy: IfNotPresent</code> strategy has done\nprecisely what it says, and nothing more. Let's set up a scenario. To begin,  in  is scheduled to  and requires  from a private repository.\nFor it's image pull authentication material, the pod references  in its .  contains the necessary credentials to pull from the private repository. The Kubelet will utilize the credentials from  as supplied by \nand it will pull  from the registry. This is the intended (and secure)\nbehavior.</p><p>But now things get curious. If  in  happens to also be scheduled to , unexpected (and potentially insecure) things happen.  may reference the same private image, specifying the  image pull policy.  does not reference \n(or in our case, any secret) in its . When the Kubelet tries to run the pod, it honors the  policy. The Kubelet sees that the  is already present locally, and will provide  to .  gets to run the image even though it did not provide credentials authorizing it to pull the image in the first place.</p><figure><img src=\"https://kubernetes.io/blog/2025/05/12/kubernetes-v1-33-ensure-secret-pulled-images-alpha/ensure_secret_image_pulls.svg\" alt=\"Illustration of the process of two pods trying to access a private image, the first one with a pull secret, the second one without it\"><figcaption><p>Using a private image pulled by a different pod</p></figcaption></figure><p>While  should not pull  if it is already present\non the node, it is an incorrect security posture to allow all pods scheduled\nto a node to have access to previously pulled private image. These pods were never\nauthorized to pull the image in the first place.</p><h2>IfNotPresent, but only if I am supposed to have it</h2><p>In Kubernetes v1.33, we - SIG Auth and SIG Node - have finally started to address this (really old) problem and getting the verification right! The basic expected behavior is not changed. If\nan image is not present, the Kubelet will attempt to pull the image. The credentials each pod supplies will be utilized for this task. This matches behavior prior to 1.33.</p><p>If the image is present, then the behavior of the Kubelet changes. The Kubelet will now\nverify the pod's credentials before allowing the pod to use the image.</p><p>Performance and service stability have been a consideration while revising the feature.\nPods utilizing the same credential will not be required to re-authenticate. This is\nalso true when pods source credentials from the same Kubernetes Secret object, even\nwhen the credentials are rotated.</p><h2>Never pull, but use if authorized</h2><p>The  option does not fetch images. However, if the\ncontainer image is already present on the node, any pod attempting to use the private\nimage will be required to provide credentials, and those credentials require verification.</p><p>Pods utilizing the same credential will not be required to re-authenticate.\nPods that do not supply credentials previously used to successfully pull an\nimage will not be allowed to use the private image.</p><h2>Always pull, if authorized</h2><p>The  has always worked as intended. Each time an image\nis requested, the request goes to the registry and the registry will perform an authentication\ncheck.</p><p>In the past, forcing the  image pull policy via pod admission was the only way to ensure\nthat your private container images didn't get reused by other pods on nodes which already pulled the images.</p><p>Fortunately, this was somewhat performant. Only the image manifest was pulled, not the image. However, there was still a cost and a risk. During a new rollout, scale up, or pod restart, the image registry that provided the image MUST be available for the auth check, putting the image registry in the critical path for stability of services running inside of the cluster.</p><p>The feature is based on persistent, file-based caches that are present on each of\nthe nodes. The following is a simplified description of how the feature works.\nFor the complete version, please see <a href=\"https://kep.k8s.io/2535\">KEP-2535</a>.</p><p>The process of requesting an image for the first time goes like this:</p><ol><li>A pod requesting an image from a private registry is scheduled to a node.</li><li>The image is not present on the node.</li><li>The Kubelet makes a record of the intention to pull the image.</li><li>The Kubelet extracts credentials from the Kubernetes Secret referenced by the pod\nas an image pull secret, and uses them to pull the image from the private registry.</li><li>After the image has been successfully pulled, the Kubelet makes a record of\nthe successful pull. This record includes details about credentials used\n(in the form of a hash) as well as the Secret from which they originated.</li><li>The Kubelet removes the original record of intent.</li><li>The Kubelet retains the record of successful pull for later use.</li></ol><p>When future pods scheduled to the same node request the previously pulled private image:</p><ol><li>The Kubelet checks the credentials that the new pod provides for the pull.</li><li>If the hash of these credentials, or the source Secret of the credentials match\nthe hash or source Secret which were recorded for a previous successful pull,\nthe pod is allowed to use the previously pulled image.</li><li>If the credentials or their source Secret are not found in the records of\nsuccessful pulls for that image, the Kubelet will attempt to use\nthese new credentials to request a pull from the remote registry, triggering\nthe authorization flow.</li></ol><p>In Kubernetes v1.33 we shipped the alpha version of this feature. To give it a spin,\nenable the <code>KubeletEnsureSecretPulledImages</code> feature gate for your 1.33 Kubelets.</p><p>You can learn more about the feature and additional optional configuration on the\n<a href=\"https://kubernetes.io/docs/concepts/containers/images/#ensureimagepullcredentialverification\">concept page for Images</a>\nin the official Kubernetes documentation.</p><p>In future releases we are going to:</p><ol><li>Write a benchmarking suite to measure the performance of this feature and assess the impact of\nany future changes.</li><li>Implement an in-memory caching layer so that we don't need to read files for each image\npull request.</li><li>Add support for credential expirations, thus forcing previously validated credentials to\nbe re-authenticated.</li></ol>","contentLength":6307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","k8s"]}