{"id":"2Qhhdda6Qnbf8RCfUPd4nB9sSt2WDQfEpF7H3gCnZZ4AsfbGMy3RmrCa6gigGY6TkbrrJn4wmHXXNYcVj1bK","title":"top scoring links : rust","displayTitle":"Reddit - Rust","url":"https://www.reddit.com/r/rust/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/rust/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"rkyv is awesome","url":"https://www.reddit.com/r/rust/comments/1l6qzqo/rkyv_is_awesome/","date":1749428018,"author":"/u/ChadNauseam_","guid":418,"unread":true,"content":"<p>I recently started using the crate `<a href=\"https://github.com/rkyv/rkyv\">rkyv</a>` to speed up the webapp I'm working on. It's for language learning and it runs entirely locally, meaning a ton of data needs to be loaded into the browser (over 200k example sentences, for example). Previously I was serializing all this data to JSON, storing it in the binary with , then deserializing it with serde_json. But json is obviously not the most efficient-to-parse format, so I looked into alternatives and found rkyv. As soon as I switched to it, the deserialization time improved 6x, and I also believe I'm seeing some improvements in memory locality as well. At this point it's quick enough that i'm not even using the zero-copy deserialization features of rkyv, as it's just not necessary.</p><p>(I likely would have seen similar speedups if I went with another binary format like <a href=\"https://crates.io/crates/bitcode/0.6.6\">bitcode</a>, but I like that rkyv will allow me to switch to zero-copy deserialization later if I need to.)</p>","contentLength":933,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"simply_colored is the simplest crate for printing colored text!","url":"https://github.com/nik-rev/simply-colored","date":1749412489,"author":"/u/nikitarevenco","guid":420,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1l6l8ns/simply_colored_is_the_simplest_crate_for_printing/"},{"title":"Considering replacing GoMobile with Rust uniffi for shared core mobile/desktop/core/wasm","url":"https://www.reddit.com/r/rust/comments/1l6jp8e/considering_replacing_gomobile_with_rust_uniffi/","date":1749408585,"author":"/u/cinemast","guid":419,"unread":true,"content":"<p>We’re working on <a href=\"https://zeitkapsl.eu/en/?utm_source=reddit_rust\">zeitkapsl.eu</a> an end-to-end encrypted alternative to Google photos, offering native apps for Android, iOS, Desktop and the web, with a shared core implemented in Go, using <a href=\"https://pkg.go.dev/golang.org/x/mobile/cmd/gomobile\">GoMobile</a> for FFI to iOS and Android. </p><p>While GoMobile works “okay,” we’ve hit several frustrating limitations that make us looking for alternatives.</p><p>Some of our main pain points with GoMobile:</p><ul><li> across the FFI boundary — no slices, arrays, or complex objects, so we rely heavily on protobuf for data passing. Still, we often need to massage types manually.</li><li>Cross-compilation with  (libwebp, SQLite) is complicated and brittle. Zig came to the rescue here, but it is still a mess.</li><li> binaries are huge and slow to compile; our web client currently has no shared core logic. We looked at , which is cool but would basically also be a rewrite.</li><li><strong>Debugging across FFI barriers</strong> is basically impossible.</li><li><strong>No native async/coroutine support</strong> on Kotlin or Swift sides, so we rely on callbacks and threading workarounds.</li></ul><p>We are currently considering to build a spike prototype in Rust to evaluate the following:</p><ul><li>SQLite CRUD with our schema (media, collections, labels, etc.)</li><li>FFI support for Android, iOS, desktop — cancellable calls, async if feasible</li><li>Image processing: HEIC decode, WebP encode, Lanczos3 resizing</li><li>Protobuf encoding/decoding</li><li>ONNX Runtime for AI inference</li><li>Local webserver to serve media</li><li>MP4 parsing and HLS muxing</li><li>AES-GCM encryption, SHA3, PBKDF2, HKDF, secure key gen</li><li>Configurable worker pool for processing media in parallel</li></ul><p><strong>We’d love to hear from Rust experts:</strong></p><ul><li><a href=\"https://mozilla.github.io/uniffi-rs/latest/Does\">uniffi-rs</a> seems a promising alternative to gomobile, any insights that you can share? Especially with deployment in Android, iOS and WASM environments</li><li>Any recommended crates for above mentioned aspects. </li></ul><p>We’re also considering alternatives like Kotlin Multiplatform or Zig, but currently Rust looks most promising.</p><p>I have looked at <a href=\"https://github.com/bitwarden/sdk-internal\">Bitwarden SDK</a>, they operate in a similar context, except for the media processing. </p><p>Has someone been working on a project with similar requirements? </p>","contentLength":2023,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I wrote a programming language in Rust for procedural art","url":"https://www.reddit.com/r/rust/comments/1l6ja4l/i_wrote_a_programming_language_in_rust_for/","date":1749407527,"author":"/u/masterofgiraffe","guid":417,"unread":true,"content":"<p>I wanted to share that I’ve been working on a functional programming language aimed at generating procedural art. Although it’s still in the early stages, the language has a defined syntax and a comprehensive standard library. I’ve also been documenting the project on GitBook.</p><p>I’m looking for users to help explore its potential use cases. There may be many creative applications I haven’t considered, and I’d appreciate identifying any gaps in its capabilities.</p><p>The language is implemented in Rust and runs an interpreter that compiles code into a collection of shapes, which are then rendered as PNG images. All code is distilled down to a single root function.</p><pre><code>root = hsl (rand * 360) 0.4 0.2 FILL : grid grid_size = 10 grid = t (-width / 2.0) (-height / 2.0) (ss (float width / grid_size) (collect rows)) rows = for i in 0..grid_size collect (cols i) cols i = for j in 0..grid_size hsl (rand * 360) 0.5 0.6 ( t (i + 0.5) (j + 0.5) (r (rand * 360) (ss 0.375 SQUARE))) </code></pre><p>If you’re interested in creative coding, I encourage you to take a look!</p>","contentLength":1055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probably Faster Than You Can Count: Scalable Log Search with Probabilistic Techniques","url":"https://blog.vega.io/posts/probabilistic_techniques/","date":1749388660,"author":"/u/TonTinTon","guid":416,"unread":true,"content":"<p>Imagine you want to build a system that needs to search through petabytes of log data, with new logs streaming in at multiple terabytes per day. Using traditional data structures and <a href=\"https://en.wikipedia.org/wiki/Exact_algorithm\" target=\"_blank\">exact algorithms</a> it’s hard to keep up with the pressure of such scale. Database indices grow unwieldy, memory requirements explode, and query times stretch from milliseconds to minutes or even hours. When working at this scale, the pursuit of 100% precision can become your worst enemy.</p><p>Following up on our exploration of log search engines in <a href=\"https://blog.vega.io/posts/log_search_engines/\" target=\"_blank\">“Search Logs Faster than Sonic”</a>, it’s time to introduce a class of solutions that isn’t very common in the standard software engineer’s toolbox but shines best at extreme scale: probabilistic data structures and approximation algorithms.</p><p>These tools aren’t just a part of theoretical computer science. They’re working behind the scenes in systems you likely use every day. Redis, ElasticSearch, ClickHouse rely on them to optimize lookups and provide estimations in queries that would otherwise crash the servers or take forever to complete.</p><p>The basic idea is simple, there is a trade-off between accuracy and performance. Sometimes a small compromise on accuracy can results in massive performance gains while still producing a sufficient result. Instead of keeping track of everything exactly (which gets expensive fast), these structures / algorithms maintain a good-enough approximation that requires far less memory and processing time. It’s like estimating the number of rubber ducks I have in my collection instead of counting each one – you might be off by a few, but you’ll get a good-enough answer fast, without searching for the ones my cats have “sharded” across the apartment.</p><p>Let’s explore how these techniques can help process massive amounts of logs without breaking your infrastructure budget.</p><h2>The Challenge of Data Sharding \n    </h2><p>When working with massive datasets, high-scale systems often split data into smaller, more manageable horizontal partition of data called <a href=\"https://en.wikipedia.org/wiki/Shard_%28database_architecture%29\" target=\"_blank\">shards</a>.</p><p>When you want to query this data, you need to know which shards contain relevant information. Otherwise, you’re forced to read from all of them leading to many expensive io operations whether the shards should be read from disk or over network (e.g. from s3).</p><p>The simplest pruning approach is time-based filtering. Each shard tracks its minimum and maximum timestamps:</p><pre tabindex=\"0\"><code>Shard_1: 2023-01-01T00:00:00Z to 2023-01-01T06:00:00Z\nShard_2: 2023-01-01T03:00:00Z to 2023-01-01T09:00:00Z\nShard_3: 2023-01-01T06:00:00Z to 2023-01-01T12:00:00Z\n...\n</code></pre><p>When a query comes in requesting data for a specific timeframe:</p><pre tabindex=\"0\"><code data-lang=\"kql\">@Table\n| where timestamp &gt; '2023-01-01T07:00:00Z'\n</code></pre><p>We can immediately eliminate  from consideration.\nThis concept is widely used, for example elasticsearch organizes data into time-based indices and shards within those indices, ClickHouse partitions tables by date ranges and S3-based data lakes organize files into prefixes and time-based partitions.</p><p>But what about other filter conditions? Consider this simple query:</p><pre tabindex=\"0\"><code data-lang=\"kql\">@Table\n| where source.ip = \"192.168.1.1\" AND timestamp &gt; '2023-01-01T07:00:00Z'\n</code></pre><p>Time-based pruning helps with the timestamp condition, but we still need to check all remaining shards for the specific IP.</p><p>A naive approach might be to maintain an exact index of all values for each field using a hashmap. The shard can be skipped if the filtered value isn’t present:</p><pre tabindex=\"0\"><code>Shard_2 contains:\n  source.ip: {\"192.168.1.1\", \"10.0.0.1\", ... 10,000s more IPs}\n</code></pre><p>The problem is for high-cardinality fields like user IDs, request paths or if you’re really unlucky some uuid as storing and checking complete value lists consumes enormous amounts of memory and processing time.</p><p>A Bloom filter solves this by providing a memory efficient way to answer a simple question: “Could this value exist in this shard?” It can tell you with certainty when something is NOT in the dataset (no false-negative), while occasionally producing false positives.</p><p>You can think of Bloom filters like trying to guess what your coworker is heating up in the office microwave just by the smell, so you know if it’s worth asking for a bite.\nSmells carry less information than the full dish, but if you recognize the scent of leftover fried chicken, you can usually make a decent guess.\nThe problem is that scents can overlap so you might think it’s fried chicken, but it’s actually reheated chicken nuggets 😕 (that’s a false positive).\nBut if none of the familiar smells are present, you know for sure it’s not what you’re hoping for (no false negatives).</p><p>Here’s how a Bloom filter works:</p><ul><li>Start with a bit array of  bits, all initially set to 0</li><li>Choose  different hash functions (scents) that each map an input to one of the  array positions</li><li>To add an element, run it through all  hash functions to get  array positions, then set all those positions to 1</li><li>To check if an element exists, run it through all  hash functions to get  positions\nIf ALL positions contain a 1, the element is PROBABLY in the set (it could be a false positive due to hash collisions)</li><li>Otherwise, the element is DEFINITELY not in the set.</li></ul><p>What I like about Bloom filters is that both adding and searching are done in a time-complexity which doesn’t depend on the data, it depends solely on the number of chosen hash function  which of-course affect the false positive rate.\nSo you can control the trade-off between memory usage and false positive rate!\nThe probability of a false positive is approximately:</p><p>$$\np ≈ (1 - e^(\\frac{-kn}{m}))^k\n$$</p><ul><li> is the size of the bit array</li><li> is the number of elements in the set</li><li> is the number of hash functions</li></ul><p>So for our use case, for each shard and each “relevant” field (we’ll touch on when to avoid Bloom filters later on) in the table’s schema, we can maintain a separate Bloom filter that tracks all values for that field in that shard.\nThis lets us quickly eliminate shards that definitely don’t contain our target values.</p><p>So let’s say you estimate a particular field will have  in a shard of data and you’re willing to retrieve shards without relevant data (false positives) at a rate of \\(1\\%\\).\nYou would need approximately:</p><p>$$\nm = -\\frac{n \\cdot \\ln(p)}{(\\ln 2)^2}\n= -\\frac{1000 \\cdot \\ln(0.01)}{(\\ln 2)^2}\n\\approx 9585 \\text{ Bits} \\approx 1198 \\text{ Bytes} \\approx 1.17 \\text{ KB}\n$$</p><p>And you would need approximately:\n$$\nk = \\frac{m}{n} \\cdot \\ln 2\n= \\frac{9585}{1000} \\cdot \\ln 2\n\\approx 6.64 = 7 \\text{ hash functions}\n$$</p><p>The point is that this is dramatically more space-efficient than storing the complete set of elements.\nHere’s a simple implementation:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>As I mentioned you can find them everywhere, for example:</p><ul><li><a href=\"https://lucene.apache.org/core/4_5_0/codecs/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.html\" target=\"_blank\">Elasticsearch</a> is based on Apache Lucene search which uses Bloom filters in engine for efficient term lookups.</li><li><a href=\"https://cassandra.apache.org/doc/4.1/cassandra/operating/bloom_filters.html\" target=\"_blank\">Cassandra</a> uses Bloom filters to avoid checking every SSTable data file for the partition being requested.</li><li><a href=\"https://clickhouse.com/docs/en/optimize/skipping-indexes\" target=\"_blank\">ClickHouse</a> uses Bloom filters them to skip indexes.</li><li><a href=\"https://grafana.com/blog/2024/04/09/grafana-loki-3.0-release-all-the-new-features/#query-acceleration-with-bloom-filters\" target=\"_blank\">Loki</a> uses Bloom filters to accelerate queries by skipping irrelevant logs as well.</li></ul><h2>When Bloom Filters Fall Short \n    </h2><p>Bloom filters shine when you’re looking for something specific and rare, the classic “needle in a haystack” scenario. But they quickly lose their edge when that needle becomes a recurring pattern.</p><p>A classic example is multi-tenancy. When handling logs from many tenants, it’s common to have a  field. In that case most queries if not all will filter on a specific :</p><pre tabindex=\"0\"><code data-lang=\"kql\">@AuthLogs\n| where tenant_name = 'ducks-corp'\n...\n</code></pre><p>As mentioned earlier, shards are often partitioned by time ranges so that we could skip irrelevant data when filtering by timestamp. The problem is that logs from many tenants are usually mixed together across time so their logs are likely to show up in almost every shard. That means a Bloom filter on  will be pretty useless as it will return “maybe” for almost every shard and we’ll still need to scan all of them.</p><p>The  example is a pretty extreme case, let’s take a proper example, say you’re hunting for activity related to a single user “ducker”</p><pre tabindex=\"0\"><code data-lang=\"kql\">@AuthLogs\n| where actor.username == \"ducker\" and timestamp &gt; ago(7d)\n</code></pre><p>You’re in a large organization:</p><ul><li>1TB worth of data is ingested per day.</li><li>Authentication logs make up about 5% of the total → 50 GB/day.</li><li>Each log entry averages around 1 KB → roughly 50 million  entries per day.</li><li>Each shard contains about 1 million entries → 50 shards per day.</li></ul><p>Now assuming our suspect  appears in just  of the logs, that’s 10,000 logs total per day.\n<em>Note that  may be a power IT user that is shared across many people or a user that is being used by some automation.</em>\nIf the data is  then each shard has 200 matching entries. Under a , the chance of a shard having zero matches is:\n$$\nP(\\text{no match}) = (1 - 0.0002)^{1,000,000} \\approx 1.35^{-87}\n$$\nIn both cases, Bloom filters mark every shard as a “maybe”, offering no pruning.\nIt’s important to note that although having large shards have their benefits, the larger the shard the more likely that even low-frequency value will appear at least once. So basically it will be much harder for Bloom filters to prune any shard…</p><p>So now we understand that Bloom filters are optimized for infrequent matches. When the match rate is high, the bit array becomes saturated.</p><p><strong>A More General Rule of Thumb</strong>\nBloom filters become ineffective when:</p><ul><li>The value you’re searching for is not rare so it <strong>appears frequently across many shards</strong>.</li><li>Each shard is  that even rare terms still appear often.</li><li>The field being filtered has , e.g. categorical field like  or .</li></ul><p>So before reaching for a Bloom filter, consider: how rare is the thing you’re looking for? If the answer is “not very” you may just be wasting CPU cycles hashing your way to scanning most of the shards anyway…</p><h3>Alternative Approach: Data Partitioning \n    </h3><p>A simple solution for fields that are too common for Bloom filters is to partition your data by the values of those fields. Instead of probabilistic filtering, you group data by field values into separate shards.</p><p>Going back to our  example, partitioned shards would look like:</p><pre tabindex=\"0\"><code>Shard_1: tenant=ducks-corp, 2023-01-01T00:00:00Z to 2023-01-01T06:00:00Z\nShard_2: tenant=ducks-inc , 2023-01-01T00:00:00Z to 2023-01-01T06:00:00Z\n...\n</code></pre><p>Now when you query <code>| where tenant_name == \"ducks-inc\"</code>, the system only needs to scan shards tagged with . It can skip everything else no probabilistic guessing needed.</p><p>This approach works best for  fields with a small, fixed number of possible values like tenant names, regions, or event types. Partitioning by high-cardinality fields like user IDs or UUIDs would create too many tiny shards, making the search operation inefficient (we will probably cover shard merging in a future post).</p><h2>Beyond Membership: What Else Can We Prune? \n    </h2><p>Here’s a challenge: what about the following query which a Bloom filters can’t handle at all?</p><pre tabindex=\"0\"><code data-lang=\"kql\">@AuthLogs\n| where FailedAttempts &gt; 10\n</code></pre><p>Think about it for a moment. Bloom filters are designed for exact membership testing (“is X in the set?”), but this query asks “show me all of the logs with a value greater than 10.” How would you skip irrelevant shards?</p><p><em>Hint: Just like Bloom filters, you would need to store some metadata about the numeric values in a shard.</em></p><p>The answer: for each numeric field, store the  range:</p><pre tabindex=\"0\"><code>Shard_1: FailedAttempts: min=0, max=5\nShard_2: FailedAttempts: min=3, max=15\nShard_3: FailedAttempts: min=12, max=25\n</code></pre><p>Now  can immediately skip Shard_1 (max=5), while  can skip Shard_3 (min=12).</p><p>Here’s another puzzle, what about this query?</p><pre tabindex=\"0\"><code data-lang=\"kql\">@AuthLogs\n| where UserAgent contains \"Chrome/91\"\n</code></pre><p>How would you efficiently skip shards that definitely don’t contain that substring? Bloom filters work for exact matches, but substring searches are trickier…</p><p>Throughout our examples, we’ve made an important assumption that’s worth calling out: . Once written, they don’t change. This assumption breaks down when you need to update or delete data, which brings us to our next topic.</p><h3>Cuckoo Filters: When Elements Need to Leave the Nest \n    </h3><p>Bloom filters have one big limitation: they don’t forget. Once you add an element, you can’t remove it, because different elements might “share” the same bits. Clearing bits for one element could accidentally wipe out another leading to .\nOne workaround is to use a , which maintains a counter for each bit position rather than a single bit. When adding an element, you increment the counters; when removing, you decrement them. An element exists if all its positions have counts greater than zero. But this comes at a cost, as each position now requires multiple bits to store the counter.</p><p>That’s where Cuckoo filters come in as a more elegant alternative, named after the cuckoo bird’s charming habit of tossing other birds eggs out of the nest.\nUnlike Bloom filters, which use a bit array, Cuckoo filters use a fixed-size hash table to store fingerprints: small, fixed-size representations of the original items. Each fingerprint has two possible “homes” in the table, determined by hash functions. When both are full, the filter evicts an existing fingerprint to its alternate location, just like the cuckoo evicts its nest-mates, and repeats this process until it finds space.</p><p>Instead of a bit array, Cuckoo filters use a fixed-size hash table that stores short “fingerprints”, which are small hashes derived from the inserted values. These fingerprints are much shorter than the original items, which helps save space. Each fingerprint has two possible positions in the table, chosen using two different hash functions. If both positions are already occupied, the filter selects one of the existing fingerprints, evicts it (just like the cuckoo evicts its nest-mates) and moves it to its alternate location. If that spot is also full, the process continues by evicting again until an empty slot is found or the filter gives up after a fixed number of attempts.</p><p>Because each fingerprint is tied to a specific spot, deletion is possible by simply removing it the fingerprint if you find it in one of the expected slots.</p><ul><li>Deletion of elements (ideal for expiring old data)</li><li>Lower false positive rates compared to Bloom filters</li><li>Comparable or better space efficiency</li></ul><p>The trade-off? potentially slower insertions due to the evictions logic and slightly slower lookup.</p><p>Typically for security monitoring purposes you might need to answer questions like:</p><blockquote><p>“How many unique IP addresses attempted to authenticate to our VPN in the last 24 hours?”</p></blockquote><pre tabindex=\"0\"><code data-lang=\"kql\">@VPNLogs\n| where timestamp &gt; ago(24h)\n| summarize unique_ips = dcount(source_ip)\n</code></pre><blockquote><p>“How many distinct hosts communicated with domains on our watchlist this week?”</p></blockquote><pre tabindex=\"0\"><code data-lang=\"kql\">@DNSLogs\n| where timestamp &gt; ago(7d) and query.domain in (&lt;watchlist_domains&gt;)\n| summarize unique_hosts = dcount(source_host)\n</code></pre><blockquote><p>“How many different user accounts accessed our internal data-sensitive database this month?”</p></blockquote><pre tabindex=\"0\"><code data-lang=\"kql\">@DBLogs\n| where timestamp &gt; ago(30d) and db_name == \"sensitive_data_db\"\n| summarize unique_users = dcount(actor.username)\n</code></pre><p>These seem like simple questions, but at scale, they become challenging.\nThe naive approach to counting unique items is straightforward, collect items into a set and return the size:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The problem with this approach is that the memory requirements grow linearly with the number of unique elements. In a large scale data system, we can expect millions of unique IP addresses, hundreds of thousands of unique user accounts, and tens of thousands of unique hostnames. So you need to keep track of all of them, plus apart from the size of the raw data there is a significant overhead from the hash-set data structure itself.</p><p>The real problem isn’t just the memory for a single count. In practice, you’re running dozens of these queries simultaneously:</p><ul><li>Different time windows (hourly, daily, weekly, monthly)</li><li>Different log sources (VPN, auth, DNS, network traffic)</li><li>Different groupings (by region, department, risk level)</li></ul><p>What seemed like a simple counting problem quickly consumes gigabytes of memory.</p><p>Finally distributing exact counting across multiple machines requires coordination to avoid double-counting elements which can be tricky as well.</p><h2>Enter HyperLogLog++: Counting Without Remembering \n    </h2><p>HyperLogLog++ solves this using a different approach. Instead of “remembering” every element, it tries to estimate how many unique elements there are using the statistical properties of hash functions. The estimates are pretty accurate while using a tiny, fixed amount of memory.</p><p>The high-level idea is hashing each element and looking for rare patterns in the binary representation. The rarer the pattern you’ve observed, the more elements you’ve likely processed.</p><p>Think of it like estimating the population of a city by sampling random people and asking where they were born. If you ask 100 people and find that the most remote birthplace is someone from a tiny village 500 miles away, you can infer that the city probably has a pretty large population. The logic behind it is that the odds of randomly finding someone from such a remote place is low unless there are many people to sample from.\nAnother classic analogy is coin flips: if someone tells you they flipped 5 heads in a row, you might guess they’ve done around 32 flips total, since the probability of getting 5 consecutive heads is about \\(\\frac{1}{32}\\). The longer the streak of heads, the more flips they’ve likely made.</p><p>HyperLogLog works similarly but with binary patterns. Here’s the intuition:</p><ul><li>Hash everything consistently: Every element gets run through a hash function, giving us a random-looking binary string</li><li>Count leading zeros: Look at how many zeros appear at the start of each hash</li><li>Track the maximum: Keep track of the longest run of leading zeros you’ve ever seen</li><li>Estimate from extremes: The longer the maximum run of zeros, the more unique elements you’ve probably processed</li></ul><p>So similar to the coin flip analogy, if you’ve seen a hash starting ith 5 zeros  it safe to assume you’ve processed roughly \\(2^5 = 32\\) different elements since the probability of any single hash starting with 5 zero is about \\(\\frac{1}{32}\\). This of course only works if your hash function produces uniformly random bits so each bit position should be 0 or 1 with equal probability, independent of the input data or other bit positions just like coin flips.</p><p>You’re probably thinking now that relying on a single “maximum” doesn’t sound like a good idea, just like I thought when I first read about it. You might get lucky and see a very rare pattern early, leading to a massive overestimate, or unlucky and never see rare patterns, leading to underestimation. HyperLogLog++ addresses this problem by using multiple independent estimates and combining them to get a much more stable result.</p><h3>The HyperLogLog++ Algorithm \n    </h3><p>Instead of keeping one maximum, HyperLogLog++ maintains many buckets, each tracking the maximum leading zeros for a subset of elements. This provides multiple independent estimates that can be averaged for better accuracy.\nHere’s how it actually works:</p><ol><li> using a good hash function</li><li> Use the first  bits to choose a bucket ( total buckets), and count leading zeros in the . For example for the hash  and , we split it as  so the bucket index is 10 ( in binary) and we count 2 leading zeros in the remaining part.</li><li> If this is the longest run of zeros seen for this bucket, update it</li><li> Combine all bucket values using harmonic mean and bias correction</li></ol><p>The formula for the  of a set of \\(n\\) positive real numbers \\(x_1, x_2, \\dots, x_n\\) is:</p><p>$$\nH = \\frac{n}{\\sum_{i=1}^{n} \\frac{1}{x_i}}\n$$</p><p>Why use harmonic mean when estimating the count?\nEach bucket value represents the maximum leading zeros observed, which corresponds to an estimated count of \\({2^{buckets}}\\) elements. Say you have 4 buckets with values \\([2, 2, 2, 6]\\), representing estimated counts of \\([4, 4, 4, 64]\\) elements respectively.</p><ul><li>Using arithmetic mean: \\(\\frac{4 + 4 + 4 + 64}{4} = 19\\)</li><li>Using harmonic mean: \\(\\frac{4}{\\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{64}} \\approx 5.1\\)</li></ul><p>As you can see the harmonic mean is much less sensitive to that one outlier bucket that got lucky with a rare pattern, giving a more stable estimate.</p><p>The actual formula the algorithm use is:</p><p>$$\n\\frac{\\alpha \\cdot m^2}{\\sum 2^{-{buckets}}}\n$$</p><p>Based on the harmonic mean but adds:</p><ul><li>An extra factor of \\(m\\) (so \\(m^2\\) instead of m) - to scale from “average per bucket” to “total count”</li><li>The \\(\\alpha\\) constant - used to correct mathematical biases in the harmonic mean estimation and its value depends on the number of buckets.</li></ul><p>So for the 4 buckets from the example before with an \\(\\alpha = 0.568\\) will actually get \\(\\frac{0.568 \\times 4^2}{\\frac{1}{2^1} + \\frac{1}{2^1} + \\frac{1}{2^1} + \\frac{1}{2^8}} \\approx 11.9\\) total elements.</p><blockquote><p>Note: there’s no predefined alpha for 4 buckets as using HLL with such a small number is not supported in the original algorithm</p></blockquote><p>This raw estimate has systematic biases, especially when most buckets are still empty (value 0). HyperLogLog++ detects this and switches to a more accurate method for small datasets, plus uses pre-computed correction tables to fix predictable errors across different cardinality ranges.</p><div align=\"center\"><img src=\"https://blog.vega.io/posts/probabilistic_techniques/hll_1024_buckets_lucky_estimation.png\" alt=\"HyperLogLog bucket distribution showing a lucky estimation\"><p>\n    HyperLogLog with 1,024 buckets estimating 1,000 unique elements. Each bucket represents the maximum number of leading zeros + 1 seen. This \"lucky\" run achieved 0.2% error, showing how bucket values distribute across the hash space. <a href=\"https://djhworld.github.io/hyperloglog/counting/\" target=\"_blank\">Try playing with this online calculator</a></p></div><p>Here’s a simplified rust implementation:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><h3>Choosing the Right Precision \n    </h3><p>For most applications, \\(4,096\\) buckets (\\(2^{12}\\)) hit the sweet spot of good accuracy with minimal memory overhead. You can play with different configurations using this <a href=\"https://djhworld.github.io/hyperloglog/counting/\" target=\"_blank\">HyperLogLog calculator</a> which also has a nice visualization.</p><p>To see how significant the memory reduction can be, here’s an example: Say you’re tracking 1 million unique users from authentication logs each username is 10 characters long on average.</p><p>Using HLL++ with 4,096 buckets requires approximately 32KB of memory. According to <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40671.pdf\" target=\"_blank\"></a>, the standard error of the cardinality can be calculated using:\n$$\n\\text{SE} \\approx \\frac{1.04}{\\sqrt{m}} \\rightarrow \\frac{1.04}{\\sqrt{4096}} \\approx 0.01625\n$$\nAn error of \\(1.625\\%\\) which in our example is \\(\\pm 16,250\\), it means the estimated cardinality will most likely fall between 983,750 and 1,016,250.</p><p>Now let’s write a small Rust program to see how much memory we would need to store 1 million unique usernames each 10 characters long using a hash-set for exact count:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>Now let’s see how much memory that actually takes with :</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>The measurement shows 93.4 MB total memory usage. This includes overhead from String allocations, HashSet internal structure, and the format! macro. While the code could obviously be optimized, that’s a \\(\\frac{93.4 * 1024^2}{32 * 1024} = 2988.8\\)x memory reduction for a small accuracy loss – a trade-off worth taking for most applications.</p><h3>When HyperLogLog++ Stumbles \n    </h3><p>HyperLogLog++ has some important limitations worth knowing:</p><ul><li>: For datasets with fewer than ~100 unique elements, the probabilistic nature introduces more error than it’s worth. A simple hash set would be more accurate and use similar memory.</li><li>: In distributed systems, you often need to combine cardinality estimates from multiple sources. While you can merge HyperLogLog++ structures (by taking the maximum value for each bucket), the error accumulates with each merge operation.</li><li>: Unlike exact approaches, you can’t ask “have I seen element X before?”. You can only get total counts (making it unsuitable for deduplication tasks).</li><li>: HyperLogLog++ assumes your hash function produces truly random bits. If your data has patterns that survive hashing (like sequential IDs), accuracy can suffer. This is rare with good hash functions, but good to know.</li></ul><p>This algorithm is the basis for cardinality estimation for most search engines for example:</p><p><strong>Further Reading on HyperLogLog:</strong></p><p>We’ve explored how probabilistic data structures like Bloom filters and HyperLogLog++ can be used for shard pruning and cardinality estimation in large-scale log processing systems, trading small amounts of accuracy for massive gains in memory efficiency and query performance.</p><p>If you’re interested in learning more about probabilistic structures, here are some more useful ones: Count-Min Sketches estimate item frequencies, MinHash enables fast set similarity, and Quantile Sketches provide accurate percentile calculations. We may explore them in future posts.</p><p>Probabilistic structures are just one part of building a scalable log search system. We’ve already looked at query planning and optimization in distributed search in our blog post <a href=\"https://blog.vega.io/posts/distributed_search_optimizations/\" target=\"_blank\">“Hidden Complexities of Distributed SQL”</a>. Future posts will cover other critical challenges like high-throughput indexing for real-time ingestion, shard merging strategies to improve search efficiency by minimizing number of shards queried, tokenization and indexing design choices for different search capabilities, and distributed query coordination. All essential for systems that process terabytes of logs every day.</p>","contentLength":25093,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1l6bxg2/probably_faster_than_you_can_count_scalable_log/"},{"title":"Reports of Rocket's revival are greatly exaggerated","url":"https://www.reddit.com/r/rust/comments/1l64hnz/reports_of_rockets_revival_are_greatly_exaggerated/","date":1749360049,"author":"/u/AdmiralQuokka","guid":421,"unread":true,"content":"<p>Rocket has been dead for long stretches several times in the past. At this point, the pattern is 1-2 years of inactivity, then a little activity, maybe a release and promises of more active development, followed by another 1-2 years of inactivity.</p><p>The last time we went through this, an organisation was created to allow more contributors to take over instead of everything relying on the original creator. Well, that doesn't seem to have worked out, because : <a href=\"https://github.com/rwf2/Rocket/tree/v0.5.1\">https://github.com/rwf2/Rocket/tree/v0.5.1</a></p><p>Edit: Sorry for the really dumb mistake, I only looked at the commit history of the last release. There has been some activity in the meantime. Still, it's not very much and not even a patch release in over a year for a web framework is unacceptable in my view.</p><p>Let's not recommend Rocket to newbies asking about which web framework they should use.</p>","contentLength":850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","rust"]}