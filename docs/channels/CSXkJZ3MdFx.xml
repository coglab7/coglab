<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>EKS Automode + Karpenter</title><link>https://www.reddit.com/r/kubernetes/comments/1l6yxfr/eks_automode_karpenter/</link><author>/u/dont_name_me_x</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 9 Jun 2025 07:51:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Anyone using EKS automode with karpenter in facing an issue with terraform karpenter module. can i go with module or helm only. any suggestions    submitted by    /u/dont_name_me_x ]]></content:encoded></item><item><title>Side container.</title><link>https://www.reddit.com/r/kubernetes/comments/1l6yiz9/side_container/</link><author>/u/Available-Face-378</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 9 Jun 2025 07:23:44 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I am wondering in real life if anyone can write me some small assessment or some real example to explain why I need to use a side container. From my understanding for every container running there is a dormant side container. Can you share more or write me a real example so I try to implement it.]]></content:encoded></item><item><title>Found a course on microservices that may be scam</title><link>https://app.buildmicroservicesingo.com/</link><author>/u/prtty-eyess</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 9 Jun 2025 07:18:25 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Enter your email to receive a magic linkCheck your email for the magic link!]]></content:encoded></item><item><title>Rust is Officially in the Linux Kernel</title><link>https://open.substack.com/pub/weeklyrust/p/rust-is-officially-in-the-linux-kernel?r=327yzu&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false</link><author>/u/web3writer</author><category>dev</category><category>reddit</category><pubDate>Mon, 9 Jun 2025 06:57:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>KubeCon Japan</title><link>https://www.reddit.com/r/kubernetes/comments/1l6xzc5/kubecon_japan/</link><author>/u/saiaunghlyanhtet</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Mon, 9 Jun 2025 06:47:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Is there anyone joining KubeCon + CloudNative Con Japan next week? I'd like to connect for networking, and obviously this is my first time. My personal interests are mostly eBPF and Cilium‌, and I am actively contributing to Cilium. Sharing same interests would be great, but it doesn't matter that much.]]></content:encoded></item><item><title>The new features in JDK 25</title><link>https://www.infoworld.com/article/3846172/jdk-25-the-new-features-in-java-25.html</link><author>/u/Choobeen</author><category>dev</category><category>reddit</category><pubDate>Mon, 9 Jun 2025 06:42:04 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Java Development Kit (JDK) 25, a planned long-term support release of standard Java due in September, has reached the initial rampdown or bug-fixing phase with 18 features. The final feature, added June 5, is an enhancement to the JDK Flight Recorder (JFR) to capture CPU-time profiling information on Linux. JDK 25 comes on the heels of JDK 24, a six-month-support release that arrived March 18. As a long-term support (LTS) release, JDK 25 will get at least five years of Premier support from Oracle. JDK 25 is due to arrive as a production release on September 16, after a second rampdown phase beginning July 17 and two release candidates planned for August 17 and August 21. The most recent LTS release was JDK 21, which arrived in September 2023.Early access builds of JDK 25 can be downloaded from jdk.java.net. The features previously slated for JDK 25 include: a preview of PEM (Privacy-Enhanced Mail) encodings of cryptographic objects, the Shenandoah garbage collector, ahead-of-time command-line ergonomics, ahead-of-time method profiling, JDK Flight Recorder (JFR) cooperative sampling, JFR method timing and tracing, compact object headers, a third preview of primitive types in patterns, instanceof, and switch. Also, scoped values, a vector API, a key derivation function API, structured concurrency, flexible constructor bodies, module import declarations, compact source files and instance main methods, stable values, and removal of the 32-bit x86 port.]]></content:encoded></item><item><title>Forests offset warming more than thought: study</title><link>https://news.ucr.edu/articles/2025/05/29/does-planting-trees-really-help-cool-planet</link><author>m463</author><category>dev</category><category>hn</category><pubDate>Mon, 9 Jun 2025 04:48:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Replanting forests can help cool the planet even more than some scientists once believed, especially in the tropics. But even if every tree lost since the mid-19 century is replanted, the total effect won’t cancel out human-generated warming. Cutting emissions remains essential. In a new modeling study published in Communications Earth & Environment, researchers at the University of California, Riverside, showed that restoring forests to their preindustrial extent could lower global average temperatures by 0.34 degrees Celsius. That is roughly one-quarter of the warming the Earth has already experienced.The study is based on an increase in tree area of about 12 million square kilometers, which is 135% of the area of the United States, and similar to estimates of the global tree restoration potential of 1 trillion trees. It is believed the planet has lost nearly half of its trees (about 3 trillion) since the onset of industrialized society. “Reforestation is not a silver bullet,” said Bob Allen, a climate scientist at UC Riverside and the paper’s lead author. “It’s a powerful strategy, but it has to be paired with serious emissions reductions.”While previous studies have largely focused on trees’ ability to remove carbon from the atmosphere, this research includes another critical dimension. Trees also have an effect on the chemical makeup of the atmosphere in ways that amplify their cooling effect.Trees naturally release compounds known as biogenic volatile organic compounds, or BVOCs. These interact with other gases to form particles that reflect sunlight and encourage cloud formation, both of which help cool the atmosphere. Most climate models do not account for these chemical interactions.“When you include these chemical effects, the net cooling impact becomes more significant,” Allen said. “It’s a crucial part of the picture.”The benefits of reforestation, however, are not evenly distributed. The study found that tropical forests produce stronger cooling effects with fewer drawbacks. Trees in these regions are more efficient at absorbing carbon and produce greater amounts of BVOCs. They also have less of the surface darkening effect that can cause warming by trees in higher latitudes.Beyond global temperature, reforestation can also affect regional air quality. The researchers found a 2.5 percent reduction in atmospheric dust in the northern hemisphere under their restoration scenario. In the tropics, increased BVOC emissions were a mixed bag in terms of air quality. They were linked to worse air based on particulate matter associated with enhanced aerosol formation, but improved air quality based on ozone measurements.These localized effects, the researchers say, suggest that reforestation efforts do not need to be massive to be meaningful.“Smaller efforts can still have a real impact on regional climates,” said Antony Thomas, graduate student in UCR’s Department of Earth and Planetary Sciences and co-author of the study. “Restoration doesn’t have to happen everywhere at once to make a difference.”The researchers acknowledge that the scenario modeled in the study is unlikely to be realized. It assumes trees could be restored to all areas where they once grew, which would require reclaiming developments such as housing as well as farmland and pastures. That raises questions about food security and land-use priorities.“There are 8 billion people to feed,” Allen said. “We have to make careful decisions about where trees are planted. The best opportunities are in the tropics, but these are also the areas where deforestation continues today.”The researchers highlight Rwanda as an example of how conservation and economic development can align. There, tourism revenue tied to forest protection is reinvested in local communities, providing incentives to preserve land that might otherwise be cleared.The study began as a project in Allen’s graduate-level climate modeling course at UC Riverside. It eventually evolved into a collaborative research paper, drawing on Earth system modeling and land-use data to explore what large-scale reforestation could realistically achieve.Its conclusion is cautiously optimistic: forest restoration is a meaningful part of the climate solution, but not a substitute for cutting fossil fuel use.“Climate change is real,” Thomas said. “And every step toward restoration, no matter the scale, helps.”(Cover image of tropical forest: guenterguni/Getty)]]></content:encoded></item><item><title>Kagi Reaches 50k Users</title><link>https://kagi.com/stats?stat=members</link><author>tigroferoce</author><category>dev</category><category>hn</category><pubDate>Mon, 9 Jun 2025 04:38:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Kagi surprise is an enigmatic celebration of a milestone, designed exclusively for our esteemed members. The details are a well-guarded secret - for now. The current Kagi member population (50,074) is more populous than  world countries and inhabited territories.   Next up to match is  with 54,714!   (source) ]]></content:encoded></item><item><title>FSE meets the FBI</title><link>https://blog.freespeechextremist.com/blog/fse-vs-fbi.html</link><author>1337p337</author><category>dev</category><category>hn</category><pubDate>Mon, 9 Jun 2025 01:59:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I have for you a bizarre tale of scrapers, feds, data poisoning, Torswats, and everyone's favorite fedi instance.  It veers technical, because I suspect it will be of interest to other people running servers with UGC (i.e., every fedi instance), and also because the mechanics of how I figured out what I figured out might be useful.  It's also got information about how the FBI collects data, which is of interest to everyone, but especially US citizens.  I have a few pieces of the puzzle, maybe someone with interlocking pieces can say more; I'm happy to compare notes.
To summarize, the FBI pays some shady companies to scrape data, the data is scanned for keywords (yep, just like CARNIVORE).  Links and content are then fed into Facebook, organized by topic based on the keywords.  Some rudimentary analysis is performed (sentiment analysis at least, but as friendly as Microsoft is with the feds, and as LLMs have gotten popular, the influence of machines has probably expanded) and perused by agents, using some FBI internal interface.
The TL;DR above probably implies this, but this is the longest post on here to date, by a wide margin.  I expect that most people will skip around instead of reading straight through:  probably only people running instances are going to be interested in the technical parts, and when building out the chronology I erred on the side of providing too much information rather than simplifying.
A note about links that go to fedi sites:  some of them (like the main FSE site) are down, but if you put the links into your own instance's search box, you can generally find the post (if it federated; obviously if your instance is newer or if it's configured to scrub old posts, you might not have it) and view it on your own instance.  In either case, it's generally better to try searching for the post's URL on your home instance, whether that instance is currently live or not, because then you can interact with it locally.  (FSE is actually just a few commits from resuming federation, though being ready to use is a bit farther out.  Within the next few days, probably, it'll be possible to resume fetching objects from it; I have a couple of bugs to fix in the way Revolver stores users, though it's serving objects just fine.)
Pedophiles were showing up on FSE.  The rest of this section is background, the TL;DR is I had a problem getting pedophiles to stay away from FSE and I wanted to stop waking up in a cold sweat.  You can skip this bit without missing much besides my woes and what I did about them.
As far as I could reason, pedophiles were, besides being a problem themselves, the most likely reason for FSE to attract attention from law enforcement.  Nobody wants to host CP (or be in its vicinity) or get their gear seized because they fell asleep on the wheel and they let the pedos have free or they got CP-raided.  (If you are not familiar, this is a type of false-flag where a group of people floods a place with CP and then immediately alerts the FBI, usually done to get the site taken down.  Of course, you have to have it to upload it and usually if it results in any arrests, it results in the arrest of the attackers.)
I was not happy about their arrival because, like with most fedi instances, it represented the primary existential threat to FSE, but as long as they weren't bothering any kids or uploading anything illegal, FSE has zero viewpoint censorship.  Almost invariably, they would immediately do something illegal.
My initial suspicion was that they misunderstood the rules, the meaning of freedom of speech, something like that.  It turns out that a lot of them have a habit of just dumping illegal stuff  and coming back in a week to see which accounts have been banned and which places have admins that are asleep (an approach that basically leaves no doubt that they are knowingly parasitic), then telling their friends", so you have to stay on top of it and get rid of them early, or they bring more.  It should not be a surprise that if someone's gratification is predicated on getting what they want without regard for who it hurts, that person is happy to engage in parasitic behavior like this:  violating a server and panicking an adult is no big deal if you're willing to violate a child and potentially ruin the kid's life.
I'd like to also thank fediblock for never fact-checking anything ever, giving the false impression that things that FSE has never permitted were allowed.  FSE being fedi's equivalent of a dive bar, I understand people on "gated community" instances not wanting to deal with it (though it turns out that instance-blocking is ham-fisted and just blocking a handful of accounts solves the problem), but I would prefer if they did not lie about their reasons or about me personally.  That sort of thing doesn't help when pedos show up having heard that there are no rules.  The blocks don't help them, either:  I've sent messages to admins on instances that were (hopefully unknowingly) hosting CP but got no response because they blocked FSE.  That's  problem, though; hope the block was worth it.
It does mean that when the FBI seized the kolektiva.social database backups, nothing from FSE was in there.  As combative and block-happy as that instance was, very few other instances actually did make it into their database; for all anyone knows, the "accidental seizure" might have been just cover for a CI:  if the FBI wanted the database and the CI had it, serving an overly broad warrant lets them collect it without burning that informant.  I don't know anything about the people involved, but the FBI has used that tactic before.
So when someone came looking for or attempting to provide CP I started just posting their IP and email and UA and whatever I had or could dig up.  (If you've punched a waiter, you can't complain that he refuses to bring you food.  Likewise with anyone trying to get me to host CP and then whining about me leaving them at the mercy of the internet.  I'm happy to care about your expectations for reasonable terms of service until you intentionally try something that you know is not just against the rules anywhere, but that can get the site eliminated and get me arrested.  I will do my best to discourage you from proceeding.)  I wanted to convey, completely unambiguously, that this is a hostile place to people doing that kind of thing:  the thing that  worries pedos is transparency.
But it turns out that almost none of them were even paying attention:  they were just here to dump files or grab files and leave, or they expected to be banned 90% of the time and were looking for places where they didn't get banned.  So it didn't work, and I kept digging to figure out where they were coming from.
If you are running an instance, it's even odds that you're doing it because you're interested in computers in general and the best way to learn is solving the problems that crop up.  This is great!  I have some helpful information; I post about it a lot but I haven't put it in one place.  Here is one place.  It's hopefully helpful to people that don't have a lot of experience with the topic and possibly has some bits that are of interest even to people that have been at this a long time.
I've got a bigger piece about this in the larder but here's a survival guide:  it should give you enough that you can fill in the gaps by hitting the books, and enough of the technical background to understand the rest of the story.  It's a little dense in parts.
Unfortunately, a lot of the documentation for any given piece of software ignores the problems that crop up:  coders like to think their software is painless and they treat information to the contrary as a bug in their code (or sometimes as a bug in the real world).  Pleroma and similar software also have two audiences:  users and people with their hands in the guts, and people with their hands in the guts need the real info.  But workarounds, real troubleshooting information, things like this are a little embarrassing to include.  "This is how much it'll tolerate before it breaks" is critical information, but the first thought a coder usually has is "It shouldn't break" and try to come up with a solution rather than document the tolerances.  (Eventually, everything breaks.  You just don't know how much it takes or what happens when it breaks if you don't test it.)  Combine that with the the fact that most complaints come from people that expect the software to Just Work™ and you can expect that you won't get too many tips for learning how to deal with Weird, and if you run networked software that talks to the open internet, you will encounter Weird.  There are bots and scanners and worms, and as fedi grows, all servers become more interesting targets.  And if it's a high-traffic server with open registrations, you'll attract at least a little targeted attention.  (And of course, more if you call the server something like "freespeechextremist.com":  I may as well make the favicon a big, red bullseye.  But I like encountering Weird, so it's no trouble for me.)
But the Weird is only weird until you have an answer, so between that and the paucity of documentation of the Weird (because it is nebulous, because coders often don't know all the bugs and if they do, they often don't like to document them, and because high-traffic servers are rare on fedi), your best bet is to get good at diagnostic tooling and analyzing data.  Half of running a server of any sort is being able to tell if something undesirable is going on and the other half is figuring out the shape of it.  (Actually fixing it tends to be trivial.)  That is, you come up with questions and then you figure out how to answer them.  A lot of the questions are going to be "Why is this slow?" or "Why did this stop working?" but if you understand the stream of logs, sometimes the questions are going to be "What the hell is ?"
You want to be able to understand the logs directly, but you will also need to be able to understand them in aggregate and correlate them with other logs.  Luckily for you, it's all text streams and text files and Unix is  of tools for answering questions about text.  Here's the crash course!
This is dense but not difficult:  if you can set up Postgres and Pleroma and nginx, this material is all within your grasp.  Learn one of the things below and you will get some use out of it, and the use is exponential the more of them that you learn.
If you know any scripting languages, you can learn enough awk for it to be useful in 30 minutes:  every awk program is predicate1{action1} predicate2{action2} [...].  If you don't, you can probably pick up awk in a few hours.  (I'm serious, no exaggerating.  Do yourself a favor.)  If you combine this with  and  then this is as much as you need in order to do real-time analysis of your log files.  I can't speak highly enough of awk's usefulness:  it's like the SQL of plain text.
If you are any good with awk and some basic networking tools (dig, whois, traceroute, tcpdump, iftop), you know how to use datasets (NRO delegated stats, for example; whois on IPs will often give a geofeed URL, etc.) and services (whois again, Shodan is a good start, DDG lists several services that it integrates), it's possible to figure out just from your webserver logs who is who, when someone signed up (grep the logs for the  to the accounts endpoint), from where, whether or not it was a Tor exit or proxy, what language they have told their browser to say they speak, things like that.  There is no shortage of tools for network exploration, and the more you learn about how the Internet works, the better you'll be able to use them.  That should suffice:  do a few hours of reading and you have just put your competence into the top 10% of fedi admins, and learned some things that you can apply anywhere.
nginx, lighttpd, Apache httpd, and almost all of the other webservers that are popular to set in front of fedi for load-balancing or caching or filtering or rate-limiting, they all have a some directives that allow you to control the format of the logfiles, log arbitrary headers, timing information.  It's not always necessary, but keep it in your pocket for when you need it.  With FSE (and other servers I run, not just fedi stuff), I usually strip most of the quote marks added in the common log format, I add a lot of timing information (especially time required to get a response from the backend).  I also use tabs, effectively making the log files a big TSV:  the extra space makes the files a little easier to read visually, a little simpler to use with awk, but also opens up the tooling options:  R, sed, sort, nearly anything can read TSVs.  Read the output of grep into irb or some other REPL that's good at string-mangling; set  and pipe it into a  loop in bash.  Or if you have some favorite spreadsheet software, they can all read TSVs (but you'll probably want to filter it or split it into chunks unless you have the kind of spreadsheet software that doesn't choke on a 20GB file).  (Of course, because I deal with 20GB files, the tooling I use can handle pretty big files.)  In fact, although awk is like SQL for delimited text, you can even use regular SQL:  sqlite3 can operate on text pipelines.  And if you're good with SQL, check the man page for psql:  you can have it emit TSVs pretty easily!  A full SQL tutorial would take longer than awk, but SQL is very useful to pick up.
Really simple numerical analysis is indispensable when you are looking for aberrances:  the Weird sticks out.  If you're tailing the logs and piping that somewhere, you can keep a running average and calculate standard deviations and find the outliers.  68, 95, 99.7:  calculate the standard deviation of your window, and you can find things that stick out.  (In awk, this is really concise, since it keeps track of line—technically "record"—numbers automatically:   keeps the last thousand records in the array .)  Some endpoints start to get a disproportionate amount of traffic out of nowhere, some IPs show up out of nowhere and flood you.  Finding things that stick out is the point of data analysis.  (This is the sort of tool I was writing/using when I would post things like "90% of the POSTs are coming from the same address".)  On fedi, network-based DDoS tends to be more popular than targeting specific endpoints; if you see someone suddenly hammering TWKN, and it's probably a scraper.  (Probably!)
The last thing to learn is when you're being too paranoid:  sometimes it's not a DDoS, someone just linked to a post from a popular site like HN, sometimes an idiot wrote the scraper and doesn't understand rate-limiting.  (Always include a mechanism to limit the rate, even if you are making a trivial Markov bot.)  Sometimes, it's just someone trying to write a new client and the client has a bug:  fedi is exciting like that, there are hackers all over.
Over time, running a service, you bump into problems, and your service grows solutions to those problems, and you keep the solutions around so that you don't have the same problem twice.  The service acquires scar tissue, reminders of problems, marks that distinguish it from a new service.  (The scar tissue is one of the reasons that you encounter so many surprises when doing a big rewrite.)
Early in FSE's history, we had some malicious signups:  just normal spammers.  I view captchas as hostile to legitimate users and ineffective (and of course, Pleroma's captchas have been cracked since then).  Likewise with email verification, except that expecting a real email isn't just hostile design, it's also a privacy risk.  There's a tradeoff there:  I have to manually generate password resets if people forget them, FSE can't send out email notifications, things like that, but on the other hand, someone that wants email addresses isn't going to target FSE, and in the event FSE  compromised, the email addresses aren't leaked.  (If someone has an email address and a password, the first thing they'll do is try to use that combination on other services.  If they don't have a real email address, though, they can't.)  But there are a lot of tradeoffs you have to make and people that don't like the ones I made can make different servers or join different ones.
There's work stuff where, if you can use an off-the-shelf solution to a problem, that's usually what you want; it's generally an explicit policy to spend as little time as possible on things that aren't part of your core business.  That's reasonable but I don't think web applications have gotten  reliable in the decade and a half since this became conventional wisdom, so maybe it's worth considering the emergent properties of this kind of policy.  So when I'm on my own, a project where I have no boss and no reason to compromise, I generally roll my own solutions.  I like coding anyway so it's not a costly experiment, and these solutions generally end up faster, more reliable, and more flexible; aside from that, I'm not at a vendor's mercy for bugfixes.  (There are always bugs.  The difference is whether I've got to try to find them in someone else's 200kLOC codebase or in my own 4-line script.  Additionally, you can guess whether it's faster to bash a four-line script into the text editor or tweak a 400-line config file.)
So I needed a way to tamp down on the spam without making the site suck:  my solution was to tail the logs, send them through an awk script, and it would just email me when it saw someone do  and get a 200 back.  Eventually I expanded the script and it would do things like check  as a reasonable approximation of whether I was at my desk and if xlock wasn't running, it would pipe a message through espeak -s120 -v other/en-sc.  (I call him "scotbot".  As you can hear, I did have it correctly pluralize "user" but forgot to change "there are" to "there is" when only one occurred.)  At some point, we got a really big hit, and I had nginx rate-limit signups to one per minute.  Later on, I did the welcomebot, so new signups would be announced in public.
So I would keep an eye on new users arriving:  usually it was just merry shiptoasters, but once in a while I see a pedo show and this sets off the self-preservation instincts, so I dump the stuff they've done and if they are still there and I haven't had to hit the red button yet (that is, they haven't done  yet, compelling me to kill off their IP and account and delete all of the shit they uploaded), I can watch them move around, look at what they are searching for.
The pedos would land on a page, some post or something, usually a local mirror of a post from another instance, then they'd sign up, start mashing search terms into the box (which is usually how I noticed them:  some search terms were added to an awk script that would ping me), follow a handful of accounts, and usually just leave.  A peculiar thing stuck out:  a lot of them were coming from boardreader.com, based on the  header, so I tugged on that thread, and that thread turned out to be the weird one.
I'd never heard of them, so I looked around, and boardreader.com was a strange site indeed:  very barebones, didn't work over Tor, no contact information listed anywhere.  (Some time in the interim, they added a SocialGist banner at the bottom...that goes to a 404 now that SocialGist has moved.)  I bashed in some of the search queries that the pedos had used on FSE and was pretty horrified to find the posts they landed on, all of which originated on other servers, but all of which were also ascribed to FSE.
It turns out that BoardReader was a tool for searching forums.  The authors don't appear to understand what fedi is, so they had treated FSE as a forum, and all of the public posts that came to FSE from elsewhere as forum posts made on some forum called "freespeechextremist.com".
Apparently it was a small search index for forums and it got big enough to be bought by a Japanese company, the two founders had issues with the new owners, and the company eventually was sloughed off and acquired by SocialGist (which now redirects you to socialgist.ai).  SocialGist purports to sell "accessible social data", they list several data sources, and per their blog, their developers are in Serbia, which lines up with the IP they were using, so I've started thinking I've got the right people.
Most of the search results indicated that long ago, BoardReader identified itself in the User-Agent header and most of the targets viewed it as hostile:  it's present on a lot of lists of poorly behaved bots.  There are also complaints about it on a lot of forums, and there are threads where people are asking how to stop it; in those threads, some people show up from nowhere and suggest that the person running the board should be grateful for the traffic bump.  (If you owned a search engine for discussion boards, wouldn't you use it to search for mentions of your engine?  And if you were running a somewhat aggressive crawler that was annoying people, it's a matter of temperament to decide whether to ask what would bother people less versus showing up to argue with them.  I give it even odds that those posts were made by owners or employees at BoardReader itself.)
I went over and grepped the logs to see if they'd been to FSE:  nothing.  But they had to be getting data from FSE:  they had posts from other instances and links to FSE.  So I kept looking and found a large amount of scraping on /api/v1/timelines/public?local=false from a browser claiming to be Chrome, and coming through way faster than a human could scroll even if they were leaning on the Page Down key.
spider1.boardreader.com through spider43 all had A records, but traffic was coming through 45.15.176.187 (which was, at the time, owned by DediPath).  That was odd, right:  why would BoardReader go to the trouble of making A records for their spiders and then go through some other service?
So, I tell the server to drop traffic from the IPs that were scraping.  Problem solved!  Then immediately I start seeing a large number of attempts from different IPs.  Residential IPs in the US:  they're buying residential proxies.  It's one thing to lie in the User-Agent header, but it's a step past that to pay money to evade detection.  Someone that has money to burn wants FSE scraped, probably a business.  At this point I'm certain enough that it's BoardReader.  I dash off a quick email to info@boardreader.com asking for information on their crawler.  Since they are going to lengths to hide what they are doing, I don't expect much, but it doesn't hurt.
So I need an automated approach if they're automatically hopping proxies.   and  plus a really quick Ruby script to sit between nginx and Pleroma for that endpoint, and I can start dropping traffic from any IP that tries to hit FSE with that token.  If I keep it there, I'll exhaust their proxies before they fill their cache.
Eventually, the requests dry up and I see a request from an IP owned by a Serbian ISP that leads back to devtools.boardreader.com.  It acts like a normal browser:  it loads all the resources, grabs a Bearer token, executes JavaScript, and subsequent to that, the scraping resumes using that token.  They're trying to play back a browser session:  that's clever.  Watching the logs confirms it:  bots using that token start arriving, playing back the sequence of requests, and then hammering the hell out of the public timeline again.  To verify, I wander back through their site and see that they are indeed getting new posts from that batch of requests.
I start severely limiting TWKN by cranking the rate-limiting way up.  At this point, it starts throwing 429s even at legitimate users, so I finally talked about the problem in public some after mostly keeping it to myself or in DMs.  (And, of course, writing this has made me regret not having taken notes with timestamps.  I've had to piece everything together from scattered notes, timestamps on scripts and logs, DMs, etc.  The first draft of this post was missing some information and had the chronology wrong.)
BoardReader is sneaky and annoying and using fraudulent means to extract data from FSE over my objections, but now thay I can isolate their traffic, I can I try a lot of different approaches.  They aren't going to get any legitimate data again, but I can see how their crawler behaves.  I start just sending back 429s:  their scraper responds by sending  requests.  Apparently, if it doesn't get the response it wants, it just repeats the request immediately, no delay.  Rude, but they have some confidence in their ability to get around restrictions, so they don't have to be polite.  But this is worse:  they send the reqs back so fast that it actually saturates the pipe, it's basically a worse DoS than before.  Unsurprisingly, sending back 401s, 403s, 500s, same result.  So I start just sending 402 Payment Required, an idea I got from graf.  Unfortunately, this means no one gets anything from TWKN for a while.
...And that's when they finally get back to me.  I had sent an email to info@boardreader.com on the 5th telling them that I was looking for information on their crawler, and on the 13th, after the server has started adamantly refusing to I finally get an email from dave@socialgist.com asking what I want to know.  Noncommittal.  I reply to him a few hours later, at 20:45 UTC, explaining the problem and telling him if he wants to index that he'll need to only fetch local posts and use a UA that identifies BoardReader.  He tells me at 21:01 that he'll forward it to the engineering team and asks what domains I'd like them to quit crawling.  I give the entire IP range that I owned and complain about the pedophiles.  While we're corresponding, their developers are still scraping and actively debugging the scraper, so I mention that he could save them some time.  I can another Serbian IP address.  Over email, I offer to talk to their devs, I pass a few links to FediList (which was still on the  subdomain at the time), I try to explain how fedi works.
[2023-03-13T10:24:39+00:00] https://freespeechextremist.com/main/all [200] 109.92.154.188 https://devtools.boardreader.com/
[2023-03-13T10:53:48+00:00] https://freespeechextremist.com/main/all [200] 109.92.154.188 https://devtools.boardreader.com/
[2023-03-13T13:57:18+00:00] https://freespeechextremist.com/main/all [200] 109.92.154.188 https://devtools.boardreader.com/
So obviously I don't trust them.  Dave stops replying to emails, and they're not only ''still' scraping, but they are trying to get around the countermeasures:  SocialGist is lying.  They're actively putting work into continuing to do something they've promised to stop doing:  they can't get anything out of TWKN (the last real post they got from FSE was on the 8th), and they're doing their damnedest to try to rectify this, while telling me that they'll stop.  If they felt really good about it, they'd have no reason to lie, so either their motivations are not what what they say, or the person I'm dealing with is not the same person that has decided to put FSE on the list and then told the devs to make sure they can get posts from it.
The morning after Dave ghosted me, I got an email from an fbi.gov email address, the subject line "Emergency Disclosure Request", and this in the body:
This is Special Agent Peter Christenson, with the FBI. I am requesting subscriber information for the user "WitchKingOfAngmar." This user posted the attached threat. Please let me know if you can assist with this request.
It also includes FSE Screen Shot.PNG.  I've never seen someone outside fedi refer to freespeechextremist.com as FSE, so my first thought is it was a prank, but the headers and my mail server's logs and the SPF info for fbi.gov all indicated that this was a real email from the place it claimed to be from.
This was the attached screenshot, which, despite being labeled "FSE Screen Shot", is not a screenshot of FSE:
So after I was reasonably certain that it was a legitimate email, my first reaction was to rack my brain trying to remember who the hell "WitchKingOfAngmar" was.  He didn't sound familiar to me.  I checked and this was a user at sneed.social (which was dead for a long time, but appears to have come back recently).  The screenshot had some interesting bits in it.  For one thing, despite being named "FSE Screen Shot", FSE has never looked like that.  It also described FSE as a "forum".  In fact, the top said "Forum • Blackrock Executiv...".  Some text is highlighted, "kill blackrock" "larry fink", as if those were search terms.  There was also some rudimentary sentiment analysis.  The post itself was from 26 days before the email was sent, but the screenshot read "11 hours ago" and "13 hours".  (So much for "Emergency" from the subject line, but it also indicates that it takes two hours for a post to go through whatever that system is.)
Obviously, you don't expect to receive an email from the FBI, so it took me a minute to figure out what to make of it.  WitchKingOfAngmar's post was clearly a threat but it was also clearly absurd, an obvious joke, not a credible threat.  And obviously, he wrote it to troll the admins of the site.
I know a couple of pretty good lawyers in case anything crazy happens, but the goal is always to make sure that nothing crazy happens.  So, best-case scenario, everything from FSE is public:  if you can't see it, I don't have it.  Ideally, the FBI gets that and I don't have to do any convincing.  Worst-case scenario is they kick in the door and grab the server.  They'd need a warrant to do that, and they wouldn't ask politely if they had a warrant already...unless they were trying to get me to say no or they were trying to see how I respond.
Dealing with law enforcement is usually an uphill battle to convince a person afflicted with motivated reason of the obvious.  They are looking for something, their job performance is predicated on finding it, and when that's the case, it's hard to get them to look at something that isn't the thing they're looking for, even if that thing demonstrates very clearly that what they are looking for is not here.
On top of that, I'm paranoid.  I go ask the dead spacemen for their thoughts and one of them points out that posting about it might count as obstruction and the FBI had lately been somewhat zealous about obstruction charges.  (Good to have solid friends with level heads.)
The timing was a little obvious:  they got it from BoardReader.  I go and find the post on BoardReader, to make sure it's in their index.  It is:
The odds that the FBI and BoardReader would screw up the Unicode in exactly the same way are pretty low.  (The original Unicode codepoints, 1f9e2, 1f438, and 1f44d, got turned into question mark boxes indicating invalid Unicode.  BoardReader's codebase being a mess of PHP, no surprise.)  A common glitch is not absolute, but there is the other unlikely mistake, that a post from somewhere else is ascribed to FSE.  Guess now I know why Dave ghosted:  they're scraping for the FBI, he can't turn it off.  Legally, the FBI can't pay a private organization to do something that they can't do, but if the organization is doing something on their own and the FBI "doesn't" () know and doesn't ask, they're just buying access to a stream of data, not paying someone to violate the CFAA.
I had been saying for a while that the three-letter agencies don't really "get" fedi.  Decentralized networks take some explaining to regular people, but individuals can get them.  But there's a long way from an individual understanding it to an organization understanding it.  (If you haven't ever worked in a stultifying bureaucracy, think about the amount of time that passes between a rumor propagating through your extended family and any sort of concrete change in behavior propagating.  Something can be obvious to everyone and still not obvious to the organization as a whole.)  Apparently the German feds get it and the feds in the US are starting to get it.  But this more or less confirmed it:  SocialGist doesn't understand what fedi is, really, and the FBI saw "It came from this website" and they just rolled with it.
So I get my head together and reply, explaining that since the guy was on another server, I don't have the information he is looking for, that BoardReader wrote "Free Speech Extremist" on the post but that it didn't come from FSE.  And, miraculously, that works:  he asks who to ask, I tell him to check the origin server, and ask him if he'd rather I not discuss the exchange in public, with no response.
BoardReader is still hammering FSE and getting only 402s in response.
Still corresponding with Special Agent Christenson, but no reply until East Coast business hours start.  (My last email was 15:50 on the 14th in LA, 18:50 in Quantico.)  The last email is him saying thanks, I ask him one more time if it would bother the FBI if I said something, and nothing.  In the mean time, I've been alternating between disappearing from fedi and running around TWKN being twitchy and paranoid while the actual endpoint is spewing errors.  It's better to be transparent and I usually am but people are asking questions.  The FBI guy probably can't say "Sure, write whatever, here's a selfie!" but if they wanted me not to say anything, he could definitely tell me not to say anything, so I figure it's fine.  On the other hand, the whole thing still made no sense.  (Maybe it made no sense to him either and he was just doing what his boss said.)  So I give a limited explanation and a promise to deliver a full explanation, then wait for the flashbang to come through my window, because, although I was certain I wasn't doing anything wrong or illegal, I am really paranoid:
I realize that was a screenshot of a very long post in the middle of a very long article; you'll notice that it ends with an announcement that FSE is going into lockdown until further notice:  no viewing TWKN or public timelines without an account, and registrations closed.  I recommended everyone else do the same.  I hate doing that and I hate when other instances do it, but a lot of instances follow suit.
2023-03-16 (Thursday) and a while afterI'm on the edge of my seat, watching BoardReader continue to fail to get around the wall of 402s.  (As verifying the bearer token means a round-trip to the DB, I'm still mostly kicking them out by using nginx, along with a pile of awk scripts that .)  They're using residential proxies, they're using Tor, they're rotating the User-Agent strings every request  No word from them  the FBI for a week.
I don't say much beyond the public post, but I ping the admin of sneed.social and ask him if the FBI agent contacted him, I send him the link to my post.  (I didn't know him, but everyone said he was reasonable to deal with when it was something like this, and he was.)  He goes to check his email, says that he'll reach out, and remarks that the user in question was actively trying to get banned, due to some other issue; I didn't ask.
Detroit Riot City trolls the neo-Nazi admin of Pieville, Alex Linder.  (Neo-Nazis are notorious for having no sense of humor; they take themselves too seriously.  They also tend to have difficulty with subtlety.)  , right after Linder blocks them, someone registers a new account on DRC to post a threat to blow up some Jewish hospitals, and then someone reports this post to the FBI in under a minute, less time than it would have taken to read it.
This dissipates pretty quickly:  the fed checks it out because he's got to, but it's pretty obvious, isn't it?  Your guess is as good as mine with regards to why no one was arrested for sending the FBI a false report.
There are a couple of bits worth including here, so I'll quote them.  I've mentioned this before on the blog, but the tone on fedi is significantly less formal (one is less likely to tell the same joke at work on Monday morning that they told at the bar on Friday night), so please bear with me while I quote myself.  If you're unfamiliar with the slang, a "fedpost" is a post that includes threats of violence, and a "glowie" is a federal agent.  These terms are usually used humorously.
I can't find any other fedi instances on there, but this is a pretty annoying scraper to get rid of.
This hasn't changed; as far as I know, FSE is the only one they were scraping.
The glowies are (or want to convey that they are) specifically looking at threats against Blackrock executives.
It turns out that "want to convey that they are" was correct, but I didn't know that until much later.
Remember everyone that was freaking out about the various search engines on fedi, most recently as:Public? Remember that I keep saying that there are scrapers getting at fedi *without* identifying themselves? It turns out that I was right and this is because I AM A GODDAMN GENIUS and EVERYONE THAT HAS EVER TOLD ME THAT I AM WRONG IS A RETARDED COPROPHILIAC. There are scrapers getting data out of fedi without identifying themselves and at least one of them is selling data to the FBI.
This has still not sunk in for most of the people that are worried about, e.g., Archive Team, as:Public, FediList, etc.  (Especially the text in all-caps.)  I have linked to some other cases above.
I recommend that you be careful of fedposters on your instance.
I continue to recommend this.
I think I'm going to reopen the public timeline and registrations, but that's tentative. Since boardreader.com is still attempting to scrape TWKN, if I reopen TWKN to people that aren't logged in, it will be with the terrible hacks I was using before to get boardreader.com to stop scraping.
It is important to note here that  sentence in the following paragraph is  wrong.
It looked like the situation with the FBI was over and they had what they wanted.  They were just following up on some idiot making a random threat on the internet.  So the balance remaining was just mopping-up and getting BoardReader off my back.  That shouldn't be too difficult!
Despite promising to stop, BoardReader hasn't just kept scraping, but they are still trying to debug their scraper so that it can resume collecting posts:
[2023-03-22T14:57:57+00:00] 109.92.154.76 https://freespeechextremist.com/main/all [402] "https://devtools.boardreader.com/"
[2023-03-22T14:57:58+00:00] 109.92.154.76 https://freespeechextremist.com/main/all [402] "https://devtools.boardreader.com/"
[2023-03-22T14:58:03+00:00] 109.92.154.76 https://freespeechextremist.com/main/all [402] "https://devtools.boardreader.com/"
So I send SocialGist another email:
I sure would like to hear back from you confirming when your company plans to either comply with acceptable use or stop scraping my sites. I'd expect ten days would be enough time.
I've been keeping track of traffic referred to FSE by BoardReader.  Unsurprisingly, this post, written by a Markov bot that lives on a completely different server (and as always, attributed to FSE by BoardReader), is the most frequent URL that people land on if they come from BoardReader:
Anyone that is putting search terms into BoardReader and getting  post is someone that I would like to discourage from signing up.
2023-03-24 (Friday) and subsequent weeksClearly, sending them emails and making sure that they can't scrape were not working, and they're still trying to fix their scraper, they're still hammering the API endpoint for fetching the posts.  Pedophiles are still landing on FSE, coming from boardreader.com.  That was the issue from the beginning, and it's been weeks.  I want to open the timelines back up, re-open registrations.  We've been on lockdown too long!  Plus, the site is slow as hell because BoardReader is choking my server (even though they're getting no data, they're still sending multiple requests per second), and on top of that, I'm paying bandwidth overage charges.
Since I've talked to them and got them to agree and I've stopped sending them data, and they're still trying to get around the restrictions and still sending pedophiles, I've exhausted reasonable methods.
So I grab some samples of the timeline, and bash out a small CGI script:  it just does string substitutions, mashing together accounts that do not exist and generating posts that do not exist.  (Initially the IDs were just random 32-bit numbers.  Eventually half the number varied per post and half was derived from the timestamp of the request, so I could trace the posts through BoardReader more easily.)  I don't bother making the URLs match actual Pleroma URLs:  why would I?  They just have to be unique.  I also have to start up lighttpd to serve it:  FSE uses nginx, but since nginx doesn't support CGI scripts (a travesty), I've got to send the requests for that endpoint to lighttpd.  Because I was still all the way in awk mode, of course I just used awk.  For fun, I grabbed some lists of words to include in the posts:  some based on search terms people had used on BoardReader, and then rounded out with the CARNIVORE list.  (First search result; I don't know if it was the real list or not but it didn't really matter.)
The only problem was that the scraper loved it  much.  We were suddenly getting DoS'd by their scraper.  So I built in a little delay and then had a fun idea:  jam the BoardReader search terms in.  And I saw something  weird very suddenly:
This is an awk script that draws a histogram of the requests coming into FSE.  The basic idea is really simple:  the first version of it just printed a "." and then when the timestamp changed, it printed a newline, too.  This draws a histogram for you of the number of requests per second, in real-time if you can convince awk not to buffer excessively (e.g., ).  By this time, it had gotten somewhat more sophisticated:  the hyphens represent requests that received a 2xx status code, the blue hyphens representing POST requests and the others GETs.  When the request resulted in some other status code, the first digit was printed.  The end of the line contains summary data:  number of requests that second in brackets, followed by the number of 5xx errors, followed by the ratio of server errors to total requests (as a fraction and a percentage) and the average number of requests per second.  Like many scripts that were unreasonably useful and then gew bit by bit (usually under duress, while trying to fix a problem with the server), it is nearly unreadable but is surprisingly compact and reliable.  (Here is the version that I was running when I took the screenshot for the curious; you probably won't get any direct use out of it unless you're using the same logfile format as me, but if you can read it, it should be pretty straightforward:  it's messy, but not complex.)
The big field of green-on-blue 4s sticks out:  those are requests that resulted in a 402, in this case almost all originating from Facebook, and all of them requesting URLs matching the fake posts.  Facebook shouldn't have been crawling FSE's public timeline.
You might have noticed that the random IDs were present in the posts:  the script generating the random gibberish didn't keep any history, because I didn't want the problem of storing infinite random gibberish, but I could match posts on BoardReader to URLs in the webserver logs by just pasting these random IDs into the search form on BoardReader.  So I dropped IDs from the posts into the BoardReader search box, and that more or less confirmed it:  Facebook was fetching these posts shortly after BoardReader indexed them.  Apparently BoardReader was giving Facebook a feed of their data, but it wasn't just that:  there was a common thread in the gibberish, a pattern in the posts that Facebook was interested in.  You can probably guess the hypothesis, the test, and the result:  I opened up the CGI script and, where there had been a long, random list of words to cram into the posts, I replaced it with just one phrase:  "larry fink".
Almost as soon as I saved the file, Facebook started flooding my server.  I wanted my keystrokes to start echoing again so I un-did it, replacing the list with the previous version except without "larry fink", and the flow slowed to a trickle and then stopped.  Curiosity got the best of me so I re-did it, and after the wait for Boardreader to index it, the flood resumed.
So the pipeline was my terrible awk script generating JSON that represented gibberish posts, and that went out through lighttpd, then nginx, then it left my machine and went into BoardReader's crawlers, from there into their index (however that was built) and straight out to Facebook, and presumably from there to the FBI, and from there into whatever UI that was that they were using to search.  (Zuckerberg had just testified in Congress that Facebook was critical national infrastructure:  maybe he wasn't lying.)
How  you get BoardReader to stop?  I couldn't get them to respond to emails and filling their database with gibberish wasn't helping.
So I shoved some more delays in:  eventually I spaced out the writes until they were a trickle designed to finish exactly one second before the timeout happened.  That solved the bandwidth overages:  they were just using a trickle.  For fun, I tossed in a little more randomness:  once in a while, I'd omit some random characters from the end so that it wouldn't parse.  Maybe if the error rate spiked, they'd notice.  You can get partial data out of something like that if you're using an event-based parser or you've structured it to use coroutines or thunks or whatever; the type of thing that builds up a data structure piecemeal and leaves you with a valid (if incomplete) data structure (and this approach can let you work with JSON structures that are too big to fit in memory, of course), but the overwhelming majority of JSON parsers just take a string and give you a data structure or an error:  it's easier to call something like that.  It looks like BoardReader is using the common type, so they spend about a minute on a single request and end up with no useful data.  The situation stays exactly like that for a while.
It's annoying, but I don't have to worry about it too often.  I pop in and tweak the random timeline script's output or its behavior once in a while.  And then I have a pretty evil idea:  I just start putting Dave's phone number into the randomly generated posts.  The following morning I get a reply from Dave:
hey peter, sorry for the radio silence. i've filed a jira ticket this week and hopefully will have an answer for you shortly. if we need any more information i'll loop in one of our engineers.
Guess it worked!  We go back and forth very briefly and they stop scraping pretty quickly, though it takes about a week for them to get FSE out of their index.  It shouldn't take a week for that to happen; whether they were stalling or it actually took that long, I probably won't find out.
The story wrapped up just short of a year after it started, in a very unexpected way.
Torswats.  The guy responsible for creating hundreds of bomb scares, fake hostage situations.  This was essentially a griefing tactic, and he had a long enough run that he was able to build up a little business making anonymous calls to the police and the FBI.
It turns out that that's who the FBI was looking for.  That was "WitchKingOfAngmar", which apparently is a "Lord of the Rings" reference.  That was why they were so interested in threats against Larry Fink:  apparently, Torswats had a habit of tirades full of nonsensical threats against Larry Fink.  Apparently there's a lot of information about Torswats on KiwiFarms:  https://archive.ph/yqwuA.
And that's it:  most of the things that didn't make sense about the story fit together after that.  There are still some murky bits:  what is BoardReader at present?  Just a front to give a plausible excuse to SocialGist to scrape?  Around 2010, scraping social media to find ISIS recruitment got popular, and of course, PRISM was the logical conclusion of that:  is BoardReader even a legitimate site at this point, or just the forum-scraping division of SocialGist?  What was Facebook doing in the pipeline?  Are they providing the FBI with tools for this kind of thing or do they just act as a convenient repository for this kind of data?
The best advice is the advice that you are almost guaranteed not to take:  don't scrape fedi, it's evil.
If you want data from fedi, just make a fake instance and cram it onto a bunch of relays.  You're still a shady jagoff, but at least you don't break anyone else's server, it's easier than scraping, and the data gets delivered to you in real-time and dumped in your database rather than you having to make some Rube Goldberg system to extract it from unwilling participants.
I usually make a remark like "I wonder why they don't do this" but I can't be sure they're not:  how  you know if anyone actually is doing that?  Maybe the only scrapers we know about are the noisy ones doing conventional scraping and the other ones don't make enough noise to cause problems.  People only noticed newjack.city because it was full of followbots, but you don't need to use followbots any more.  There are several varieties of ActivityPub relay and ActivityPub software and some of them lend themselves to repurposing as a scraper.  As demonstrated by gangstalking.services (a signed-fetch workaround and proof of concept) as well as pls.zuck.dad and other instances, a lot of normal fedi software can be repurposed for this kind of thing.
So, if you're not a shady asshole and you're just trying to run a server, keep it in mind.  A company like SocialGist can make themselves hard to find:  I only knew about them because they screwed up, but once I knew where to look, it wasn't difficult to create a trail of breadcrumbs.  How many people or organizations are out there doing the same thing, but  SocialGist's mistakes?  How would you know?
]]></content:encoded></item><item><title>nano color syntax file that displays it&apos;s own named colors, as actual colors</title><link>https://git.envs.net/carbonwriter/nanocolors</link><author>/u/nad6234</author><category>dev</category><category>reddit</category><pubDate>Mon, 9 Jun 2025 00:21:23 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>rkyv is awesome</title><link>https://www.reddit.com/r/rust/comments/1l6qzqo/rkyv_is_awesome/</link><author>/u/ChadNauseam_</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 9 Jun 2025 00:13:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I recently started using the crate `rkyv` to speed up the webapp I'm working on. It's for language learning and it runs entirely locally, meaning a ton of data needs to be loaded into the browser (over 200k example sentences, for example). Previously I was serializing all this data to JSON, storing it in the binary with , then deserializing it with serde_json. But json is obviously not the most efficient-to-parse format, so I looked into alternatives and found rkyv. As soon as I switched to it, the deserialization time improved 6x, and I also believe I'm seeing some improvements in memory locality as well. At this point it's quick enough that i'm not even using the zero-copy deserialization features of rkyv, as it's just not necessary.(I likely would have seen similar speedups if I went with another binary format like bitcode, but I like that rkyv will allow me to switch to zero-copy deserialization later if I need to.)]]></content:encoded></item><item><title>Riding high in Germany on the world&apos;s oldest suspended railway</title><link>https://www.theguardian.com/travel/2025/jun/09/riding-high-in-germany-on-the-worlds-oldest-suspended-railway</link><author>pseudolus</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 23:38:58 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[t’s easy to be seduced by the romance of train travel. Think of sleeper trains, boat trains, vintage steam railways, elegant dining cars. But it’s rare that an urban transport system can capture the imagination quite as much as the Wuppertal Schwebebahn in Germany caught mine, and that of anyone else who’s clapped eyes on the world’s oldest suspended railway.In October it will be 125 years since Kaiser Wilhelm II took a test ride in the Schwebebahn, just a few months before the hanging railway officially opened for business in March 1901. It was an incredible feat of engineering then, and remains so today. Even with sleek modern carriages having long replaced the original ones, it looks like something imagined by Jules Verne, with carriages smoothly gliding under the overhead track. They have even preserved the first 1901 carriage, nicknamed Kaiserwagen, which can be hired for private occasions.A childlike feeling of glee filled me as I sat in the rear of the long carriage and watched the city reveal itself as I floated anything from 8 to 9 metres (26ft to 39ft) above it. At the railway’s westernmost end, Vohwinkel is the first of only four stations whose carriages run above the street, between iron arches. The rest of the railway, which in total runs for just over eight miles, follows the route of the river Wupper. As the hanging train curves and sways above the serpentine river, it turns this commuter service into something like a fairground ride for its 80,000 daily passengers. My hitherto unknown train geek had been unleashed and was utterly delighted.The Schwebebahn came about almost by accident. The Wupper valley, about 15 miles east of Düsseldorf, was a major textile production base when Germany was undergoing its own Industrial Revolution in the 19th century. As workers flooded to the growing cities of Barmen and Elberfeld – which merged in 1929 and were renamed Wuppertal in 1930 – the authorities realised a public transport system was needed. Other cities were going underground, but Wuppertal’s rocky soil and narrow, steep valley made any sort of U-Bahn impossible, forcing the Schwebebahn’s inventor, Eugen Langen, to look up instead.At Schwebodrom, the railway museum that opened in late 2023 near Werther Brücke station at the line’s eastern end, the rich history of the Schwebebahn is laid out in three galleries, revealing one fascinating detail after another. One gallery tells the story of Tuffi, a young circus elephant loaded into the Schwebebahn for a publicity stunt in 1950. Poor Tuffi was so spooked by jostling journalists that she bolted through a window and tumbled into the river. Luckily she was only lightly bruised and lived for another 49 years, her landing spot in the Wupper now marked by an elephant statue between Alter Markt and Adler Brücke stations. You can’t move in Wuppertal without seeing Tuffi on some souvenir or another – even on milk cartons.Among the museum’s films and displays, the highlight for me was the reproduction of an original carriage, where I sat glued to my VR headset and found myself in 1920s Wuppertal. After riding the rails in real life, I was able to go back in time to see what had changed. Much of Wuppertal had to be rebuilt after heavy allied bombing in the second world war, and the railway itself has been completely reconstructed – including its art nouveau stations – while keeping the original steampunk-style design in the iron girders.But there is a Wuppertal beyond the Schwebebahn, and this city of about 350,000 people was as full of pleasant surprises as its railway. Local guide Heike Fragemann took me to the tree-lined streets around Laurentiusplatz, a square dominated by the austere-looking 19th-century basilica of St Lawrence, dedicated to Wuppertal’s patron saint. Popular with many of the 23,000 students at the University of Wuppertal as well as people of all ages, the cosmopolitan streets hummed with cafes, delis, boutiques, bars and restaurants run by some of the many nationalities that have settled here over the decades – Italian, Turkish, Greek, Indian, Vietnamese and Spanish among them. In fact, the range of restaurants throughout the city was huge, and also included Lebanese, Chinese, Croatian and traditional German fare.Pointing out an example of Wuppertal’s distinctive style of architecture – slate cladding, green shutters and white window frames – Heike led me along the narrow streets behind Laurentiusplatz as we steadily walked uphill. Not only was Wuppertal Germany’s Manchester because of its industry, Heike told me, but it was also compared to San Francisco thanks to its steepness. “We are the city of steps,” she said as we came to yet another one. “We have 500 staircases, more than 12,000 steps within the city. This is the most famous one.” She pointed to a sign with the captivating name of Tippen-Tappen-Tönchen, in honour of those 19th-century workmen clopping in their wooden clogs towards the riverside factories – hence the tipping-tapping sound. One to add to my list of adorable street names.skip past newsletter promotionafter newsletter promotionIt was the wealthy 19th-century industrialists who shaped the city, not just with their comfortable hillside villas, but also with Wuppertal’s cultural institutions. The Von der Heydt Museum, named after an art-collecting banking family, houses its impressive collection of 19th- and early 20th-century art in what had been the neoclassical town hall. The entrance is flanked by two large sculptures by the Liverpool-born Turner prize-winner Tony Cragg, who made Wuppertal his home in 1977. The Historische Stadthalle concert hall, marking its 125th anniversary this year, had Richard Strauss as one of its first conductors and Sir Simon Rattle rated its acoustics among the best in the world. Public gardens fill many of the gaps in the city, including the vast hilly Botanical Garden.As I sat in the warm, bookish surroundings of Café Engel in Laurentiusplatz, I was reminded of Friedrich Engels, the son of a wealthy Wuppertal textile manufacturer, who turned his back on his bourgeois background to co-author The Communist Manifesto with Karl Marx after seeing the appalling working conditions in mid-19th-century Manchester. Engels died in London six years before the Schwebebahn opened, and it was many years earlier that the city’s industrialists had already implemented social reforms for working-class residents that were ahead of their time. The Schwebebahn, too, looks like something from the future, but its story is purely of Wuppertal’s unique past. Here, in Germany’s old industrial heartland, the high life is yours from €3.60 a ticket.]]></content:encoded></item><item><title>Experimenting with Linux cgroups to tweak memory limits for processes</title><link>https://www.reddit.com/r/linux/comments/1l6q23q/experimenting_with_linux_cgroups_to_tweak_memory/</link><author>/u/pirate_husky</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 23:28:23 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hey, I recently decided to get back to studying systems regularly and so I am conducting small experiments for learning purposes.I recently explored how cgroups can restrict process memory usage. Here's what I did:Created a cgroup with a 1MB memory limit.Ran a simple program that tried to allocate ~5MB.Observed the process getting killed due to exceeding the memory limit (OOM kill).Checked cgroup memory events to confirm the behavior.You can find the detailed steps here.Are there better ways to experiment with cgroups or other interesting use cases you'd recommend I should try? I wish to hear your thoughts and suggestions.]]></content:encoded></item><item><title>Self-hosted x86 back end is now default in debug mode</title><link>https://ziglang.org/devlog/2025/#2025-06-08</link><author>brson</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 22:24:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Now, when you target x86_64, by default, Zig will use its own x86 backend rather than using LLVM to lower a bitcode file to an object file.The default is not changed on Windows yet, because more COFF linker work needs to be done first.The x86 backend is now passing 1987 behavior tests, versus 1980 passed by the LLVM backend. In reality there are 2084 behavior tests, but the extra ones there are generally redundant with LLVM’s own test suite for its own x86 backend, so we only run those when testing with self-hosted x86. Anyway, my point is that Zig’s x86 backend is now  than its LLVM backend in terms of implementing the Zig language.Why compete with LLVM on code generation? There are a handful of reasons, but mainly, because we can dramatically outperform LLVM at compilation speed.Benchmark 1 (6 runs): zig build-exe hello.zig -fllvm
  measurement          mean ± σ            min … max           outliers         delta
  wall_time           918ms ± 32.8ms     892ms …  984ms          0 ( 0%)        0%
  peak_rss            214MB ±  629KB     213MB …  215MB          0 ( 0%)        0%
  cpu_cycles         4.53G  ± 12.7M     4.52G  … 4.55G           0 ( 0%)        0%
  instructions       8.50G  ± 3.27M     8.50G  … 8.51G           0 ( 0%)        0%
  cache_references    356M  ± 1.52M      355M  …  359M           0 ( 0%)        0%
  cache_misses       75.6M  ±  290K     75.3M  … 76.1M           0 ( 0%)        0%
  branch_misses      42.5M  ± 49.2K     42.4M  … 42.5M           0 ( 0%)        0%
Benchmark 2 (19 runs): zig build-exe hello.zig
  measurement          mean ± σ            min … max           outliers         delta
  wall_time           275ms ± 4.94ms     268ms …  283ms          0 ( 0%)        ⚡- 70.1% ±  1.7%
  peak_rss            137MB ±  677KB     135MB …  138MB          0 ( 0%)        ⚡- 36.2% ±  0.3%
  cpu_cycles         1.57G  ± 9.60M     1.56G  … 1.59G           0 ( 0%)        ⚡- 65.2% ±  0.2%
  instructions       3.21G  ±  126K     3.21G  … 3.21G           1 ( 5%)        ⚡- 62.2% ±  0.0%
  cache_references    112M  ±  758K      110M  …  113M           0 ( 0%)        ⚡- 68.7% ±  0.3%
  cache_misses       10.5M  ±  102K     10.4M  … 10.8M           1 ( 5%)        ⚡- 86.1% ±  0.2%
  branch_misses      9.22M  ± 52.0K     9.14M  … 9.31M           0 ( 0%)        ⚡- 78.3% ±  0.1%
For a larger project like the Zig compiler itself, it takes the time down from 75 seconds to 20 seconds.We’re only just getting started. We’ve already started work fully parallelizing code generation. We’re also just a few linker enhancements and bug fixes away from making incremental compilation stable and robust in combination with this backend. There is still low hanging fruit for improving the generated x86 code quality. And we’re looking at aarch64 next - work that is expected to be accelerated thanks to our new Legalize pass.The CI has finished building the respective commit, so you can try this out yourself by fetching the latest master branch build from the download page.Finally, here’s a gentle reminder that Zig Software Foundation is a 501(c)(3) non-profit that funds its development with donations from generous people like you. If you like what we’re doing, please help keep us financially sustainable!I’ve released a few days ago a new video on YouTube where I show how to get started with the Zig build system for those who have not grokked it yet.In the video I show how to create a package that exposes a Zig module and then how to import that module in another Zig project. After June I will add more videos to the series in order to cover more of the build system.Author: Alex Rønne PetersenPull requests #23835 and #23913 have now been merged. This means that, using  or , you can now build binaries targeting FreeBSD 14.0.0+ and NetBSD 10.1+ from any machine, just as you’ve been able to for Linux, macOS, and Windows for a long time now.This builds on the strategy we were already using for glibc and will soon be using for other targets as well. For any given FreeBSD/NetBSD release, we build libc and related libraries for every supported target, and then extract public symbol information from the resulting ELF files. We then combine all that information into a very compact  file that gets shipped with Zig. Finally, when the user asks to link libc while cross-compiling, we load the  file and build a stub library for each constituent libc library (, , etc), making sure that it accurately reflects the symbols provided by libc for the target architecture and OS version, and has the expected soname. This is all quite similar to how the llvm-ifs tool works.We currently import crt0 code from the latest known FreeBSD/NetBSD release and manually apply any patches needed to make it work with any OS version that we support cross-compilation to. This is necessary because the OS sometimes changes the crt0 ABI. We’d like to eventually reimplement the crt0 code in Zig.We also ship FreeBSD/NetBSD system and libc headers with the Zig compiler. Unlike the stub libraries we produce, however, we always import headers from the latest version of the OS. This is because it would be far too space-inefficient to ship separate headers for every OS version, and we realistically don’t have the time to audit the headers on every import and add appropriate version guards to all new declarations. The good news, though, is that we do accept patches to add version guards when necessary; we’ve already had many contributions of this sort in our imported glibc headers.Please take this for a spin and report any bugs you find!We would like to also add support for OpenBSD libc and Dragonfly BSD libc, but because these BSDs cannot be conveniently cross-compiled from Linux, we need motivated users of them to chip in. Besides those, we are also looking into SerenityOS, Android, and Fuchsia libc support.The official Zig website now builds using standalone Zine. A lot of code got rewritten so if you see regressions on the website, please open an issue. Regressions only please, thanks!Normally a Zine update would not be worthy of a devlog entry, but the recent update to it was pretty big as Zine went from being a funky Zig build script to a standalone executable. If you were interested in Zine before but never got the time to try it out, this milestone is a great moment to give it a shot. Run  to get a sample website that also implements a devlog for you out of the box.P.S. I’ve also added dates to each entry on the page, people were asking for this for a while :^)The 0.14.0 release is coming shortly. We didn’t get the release notes done yet, and I’m calling it a day.Tomorrow morning I’ll make the tag, kick off the CI, and then work to finish the release notes while it builds.I know there were a lot of things that sadly didn’t make the cut. Let’s try to get them into 0.14.1 or 0.15.0. Meanwhile, there are a ton of major and minor enhancements that have already landed, and will debut tomorrow.Lately, I’ve been extensively working with C interop, and one thing that’s been sorely missing is clear error messages from UBSan. When compiling C with , Zig provides better defaults, including implicitly enabling . This has been great for catching subtle bugs and makes working with C more bearable. However, due to the lack of a UBSan runtime, all undefined behavior was previously caught with a  instruction.For example, consider this example C program:(, ) {
    
}

() {
    (, )(, )
}
Running this with  used to result in an unhelpful error:$ zig run test.c -lc
fish: Job 1, 'zig run empty.c -lc' terminated by signal SIGILL (Illegal instruction)
Not exactly informative! To understand what went wrong, you’d have to run the executable in a debugger. Even then, tracking down the root cause could be daunting. Many newcomers ran into this  error without realizing that UBSan was enabled by default, leading to confusion. This issue was common enough to warrant a dedicated Wiki page.With the new UBSan runtime merged, the experience has completely changed. Now instead of an obscure , you get a much more helpful error message:$ zig run test.c -lc
thread 208135 panic: signed integer overflow: 2147483647 + 2147483647 cannot be represented in type 'int'
/home/david/Code/zig/build/test.c:4:14: 0x1013e41 in foo (test.c)
    return x + y;
             ^
/home/david/Code/zig/build/test.c:8:18: 0x1013e63 in main (test.c)
    int result = foo(0x7fffffff, 0x7fffffff);
                 ^
../sysdeps/nptl/libc_start_call_main.h:58:16: 0x7fca4c42e1c9 in __libc_start_call_main (../sysdeps/x86/libc-start.c)
../csu/libc-start.c:360:3: 0x7fca4c42e28a in __libc_start_main_impl (../sysdeps/x86/libc-start.c)
???:?:?: 0x1013de4 in ??? (???)
???:?:?: 0x0 in ??? (???)
fish: Job 1, 'zig run test.c -lc' terminated by signal SIGABRT (Abort)
Now, not only do we see  went wrong (signed integer overflow), but we also see  it happened – two critical pieces of information that were previously missing.While the new runtime vastly improves debugging, there are still two features that LLVM’s UBSan runtime provides which ours doesn’t support yet:In C++, UBSan can detect when an object’s vptr indicates the wrong dynamic type or when its lifetime hasn’t started. Supporting this would require replicating the Itanium C++ ABI, which isn’t worth the extreme complexity.Currently, the runtime doesn’t show the exact locations of attributes like  and . This should be relatively straightforward to add, and contributions are welcome!If you’ve ever been frustrated by cryptic  errors while trying out Zig, this update should make debugging undefined behavior a lot easier!Alright, I know I’m supposed to be focused on issue triage and merging PRs for the upcoming release this month, but in my defense, I do some of my best work while procrastinating.Jokes aside, this week we had CI failures due to Zig’s debug allocator creating too many memory mappings. This was interfering with Jacob’s work on the x86 backend, so I spent the time to rework the debug allocator.Since this was a chance to eliminate the dependency on a compile-time known page size, I based my work on contributor archbirdplus’s patch to add runtime-known page size support to the Zig standard library. With this change landed, it means Zig finally works on Asahi Linux. My fault for originally making page size compile-time known. Sorry about that!Along with detecting page size at runtime, the new implementation no longer memsets each page to 0xaa bytes then back to 0x00 bytes, no longer searches when freeing, and no longer depends on a treap data structure. Instead, the allocation metadata is stored inline, on the page, using a pre-cached lookup table that is computed at compile-time:It’s pretty nice because you can tweak some global constants and then get optimal slot sizes. That assert at the end means if the constraints could not be satisfied you get a compile error. Meanwhile in C land, equivalent code has to resort to handcrafted lookup tables. Just look at the top of malloc.c from musl:[]  {
	, , , , , , , ,
	, , , ,
	, , , ,
	, , , ,
	, , , ,
	, , , ,
	, , , ,
	, , , ,
	, , , ,
	, , , ,
	, , , ,
}Not nearly as nice to experiment with different size classes. The water’s warm, Rich, come on in! 😛Anyway, as a result of reworking this allocator, not only does it work with runtime-known page size, and avoid creating too many memory mappings, it also performs significantly better than before. The motivating test case for these changes was this degenerate ast-check task, with a debug compiler:Benchmark 1 (3 runs): master/bin/zig ast-check ../lib/compiler_rt/udivmodti4_test.zig
  measurement          mean ± σ            min … max           outliers         delta
  wall_time          22.8s  ±  184ms    22.6s  … 22.9s           0 ( 0%)        0%
  peak_rss           58.6MB ± 77.5KB    58.5MB … 58.6MB          0 ( 0%)        0%
  cpu_cycles         38.1G  ± 84.7M     38.0G  … 38.2G           0 ( 0%)        0%
  instructions       27.7G  ± 16.6K     27.7G  … 27.7G           0 ( 0%)        0%
  cache_references   1.08G  ± 4.40M     1.07G  … 1.08G           0 ( 0%)        0%
  cache_misses       7.54M  ± 1.39M     6.51M  … 9.12M           0 ( 0%)        0%
  branch_misses       165M  ±  454K      165M  …  166M           0 ( 0%)        0%
Benchmark 2 (3 runs): branch/bin/zig ast-check ../lib/compiler_rt/udivmodti4_test.zig
  measurement          mean ± σ            min … max           outliers         delta
  wall_time          20.5s  ± 95.8ms    20.4s  … 20.6s           0 ( 0%)        ⚡- 10.1% ±  1.5%
  peak_rss           54.9MB ±  303KB    54.6MB … 55.1MB          0 ( 0%)        ⚡-  6.2% ±  0.9%
  cpu_cycles         34.8G  ± 85.2M     34.7G  … 34.9G           0 ( 0%)        ⚡-  8.6% ±  0.5%
  instructions       25.2G  ± 2.21M     25.2G  … 25.2G           0 ( 0%)        ⚡-  8.8% ±  0.0%
  cache_references   1.02G  ±  195M      902M  … 1.24G           0 ( 0%)          -  5.8% ± 29.0%
  cache_misses       4.57M  ±  934K     3.93M  … 5.64M           0 ( 0%)        ⚡- 39.4% ± 35.6%
  branch_misses       142M  ±  183K      142M  …  142M           0 ( 0%)        ⚡- 14.1% ±  0.5%
I didn’t stop there, however. Even though I had release tasks to get back to, this left me  to make a fast allocator - one that was designed for multi-threaded applications built in ReleaseFast mode.It’s a tricky problem. A fast allocator needs to avoid contention by storing thread-local state, however, it does not directly learn when a thread exits, so one thread must periodically attempt to reclaim another thread’s resources. There is also the producer-consumer pattern - one thread only allocates while one thread only frees. A naive implementation would never reclaim this memory.Inspiration struck, and 200 lines of code later I had a working implementation… after Jacob helped me find a couple logic bugs.I created Where in the World Did Carmen’s Memory Go? and used it to test a couple specific usage patterns. Idea here is to over time collect a robust test suite, do fuzzing, benchmarking, etc., to make it easier to try out new Allocator ideas in Zig.After getting good scores on those contrived tests, I turned to the real world use cases of the Zig compiler itself. Since it can be built with and without libc, it’s a great way to test the performance delta between the two.Here’s that same degenerate case above, but with a release build of the compiler - glibc zig vs no libc zig:Benchmark 1 (32 runs): glibc/bin/zig ast-check ../lib/compiler_rt/udivmodti4_test.zig
  measurement          mean ± σ            min … max           outliers         delta
  wall_time           156ms ± 6.58ms     151ms …  173ms          4 (13%)        0%
  peak_rss           45.0MB ± 20.9KB    45.0MB … 45.1MB          1 ( 3%)        0%
  cpu_cycles          766M  ± 10.2M      754M  …  796M           0 ( 0%)        0%
  instructions       3.19G  ± 12.7      3.19G  … 3.19G           0 ( 0%)        0%
  cache_references   4.12M  ±  498K     3.88M  … 6.13M           3 ( 9%)        0%
  cache_misses        128K  ± 2.42K      125K  …  134K           0 ( 0%)        0%
  branch_misses      1.14M  ±  215K      925K  … 1.43M           0 ( 0%)        0%
Benchmark 2 (34 runs): SmpAllocator/bin/zig ast-check ../lib/compiler_rt/udivmodti4_test.zig
  measurement          mean ± σ            min … max           outliers         delta
  wall_time           149ms ± 1.87ms     146ms …  156ms          1 ( 3%)        ⚡-  4.9% ±  1.5%
  peak_rss           39.6MB ±  141KB    38.8MB … 39.6MB          2 ( 6%)        ⚡- 12.1% ±  0.1%
  cpu_cycles          750M  ± 3.77M      744M  …  756M           0 ( 0%)        ⚡-  2.1% ±  0.5%
  instructions       3.05G  ± 11.5      3.05G  … 3.05G           0 ( 0%)        ⚡-  4.5% ±  0.0%
  cache_references   2.94M  ± 99.2K     2.88M  … 3.36M           4 (12%)        ⚡- 28.7% ±  4.2%
  cache_misses       48.2K  ± 1.07K     45.6K  … 52.1K           2 ( 6%)        ⚡- 62.4% ±  0.7%
  branch_misses       890K  ± 28.8K      862K  … 1.02M           2 ( 6%)        ⚡- 21.8% ±  6.5%
And finally here’s the entire compiler building itself:Benchmark 1 (3 runs): glibc/bin/zig build -Dno-lib -p trash
  measurement          mean ± σ            min … max           outliers         delta
  wall_time          12.2s  ± 99.4ms    12.1s  … 12.3s           0 ( 0%)        0%
  peak_rss            975MB ± 21.7MB     951MB …  993MB          0 ( 0%)        0%
  cpu_cycles         88.7G  ± 68.3M     88.7G  … 88.8G           0 ( 0%)        0%
  instructions        188G  ± 1.40M      188G  …  188G           0 ( 0%)        0%
  cache_references   5.88G  ± 33.2M     5.84G  … 5.90G           0 ( 0%)        0%
  cache_misses        383M  ± 2.26M      381M  …  385M           0 ( 0%)        0%
  branch_misses       368M  ± 1.77M      366M  …  369M           0 ( 0%)        0%
Benchmark 2 (3 runs): SmpAllocator/fast/bin/zig build -Dno-lib -p trash
  measurement          mean ± σ            min … max           outliers         delta
  wall_time          12.2s  ± 49.0ms    12.2s  … 12.3s           0 ( 0%)          +  0.0% ±  1.5%
  peak_rss            953MB ± 3.47MB     950MB …  957MB          0 ( 0%)          -  2.2% ±  3.6%
  cpu_cycles         88.4G  ±  165M     88.2G  … 88.6G           0 ( 0%)          -  0.4% ±  0.3%
  instructions        181G  ± 6.31M      181G  …  181G           0 ( 0%)        ⚡-  3.9% ±  0.0%
  cache_references   5.48G  ± 17.5M     5.46G  … 5.50G           0 ( 0%)        ⚡-  6.9% ±  1.0%
  cache_misses        386M  ± 1.85M      384M  …  388M           0 ( 0%)          +  0.6% ±  1.2%
  branch_misses       377M  ±  899K      377M  …  378M           0 ( 0%)        💩+  2.6% ±  0.9%
I feel that this is a key moment in the Zig project’s trajectory. This last piece of the puzzle marks the point at which the language and standard library has become  to use than C and libc.While other languages build on top of libc, Zig instead has conquered it!Author: Alex Rønne PetersenOne of the major things Jacob has been working on is good debugging support for Zig. This includes an LLDB fork with enhancements for the Zig language, and is primarily intended for use with Zig’s self-hosted backends. With the self-hosted x86_64 backend becoming much more usable in the upcoming 0.14.0 release, I decided to type up a wiki page with instructions for building and using the fork.If you’re already trying out Zig’s self-hosted backend in your workflow, please take the LLDB fork for a spin and see how it works for you.]]></content:encoded></item><item><title>The Looming Problem of Slow &amp; Brittle Proofs in SMT Verification (and a Step Toward Solving It)</title><link>https://kirancodes.me/posts/log-proof-localisation.html</link><author>/u/Gopiandcoshow</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 21:57:41 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
Sadly, the story didn't quite end there, and when we moved to actually
check the verification times of these rewritten programs, we found
that the programs were now failing to verify.

As it turns out, UNSAT cores are actually incomplete: but not in an
unsound way!

The results from the SMT solver did indicate all the logically
relevant axioms that were needed for the proof, but it turns out that
this list doesn't capture all the facts that are needed for a proof to
go through — as it turns out, there's an entire class of additional
axioms that I discovered that are missed: .
 – axioms in an SMT query that are logically irrelevant
 to the goal being proven but in practice are required for the proof
 to succeed.

How can this be possible? Well, it once again all comes back to our
old friend, triggers and quantifier instantiations.

Let's go back to our program from before, but let's consider a
different set of triggers for these axioms:
: , :  :: l,r = l \/ r;

:  :: l > 0 = l;

Here, we've set the trigger for the first axiom to be
 and , and the trigger for the second axiom to be
.

Now the problem here is that if we're trying to prove the verification
condition from before:

\begin{gather*}
BG \wedge (\text{NonEmpty}(x) \wedge \text{NonEmpty}(y)) \Rightarrow \text{NonEmpty}(\text{Append}(x,y))
\end{gather*}


Then the only logically relevant axiom, and the axiom that will show
up in the UNSAT core, is axiom 1 as before. But if we try to verify
our program with only this axiom, then verification would fail, as the
SMT solver would never have the term  or  in its
context. If we include both axioms, then axiom 2 acts as a : it gets instantiated during the proof search, and introduces a
term of  into the
context, which can then enable the SMT solver to instantiate the
logically relevant one.

Long story short, if we want our proofs to go through, then it is not
only necessary to include the axioms in the UNSAT core in our
localised programs, but we must also capture lurking axioms, but how
can we do this?

This brings us to the final key idea in this work, which is to exploit
SMT traces! SMT solvers, such as Z3, can be instructed to produce a
log of all the quantifier instantiations they make during the proof
search – tools such as ETH-Zurich's Axiom Profiler can use this
information to produce a graph of all the instantiations made during
the proof search:

Here this graph represents the instantiations that were made in order
to instantiate axiom 1 with x and y. The shaded boxes represent
instantiations of axioms, the square boxes are terms in the SMT
solver's context, and arrows denote dependencies between the two. From
the graph, we can see that in order for the logically relevant axiom
to be instantiated, it depended on terms produced by the lurking axiom.

Putting it all together, in the final tool, alongside the axioms from
the UNSAT core, we extract the instantiation graph as well, and
perform a breadth-first search to also include the necessary lurking
axioms as well, and thereby were able to automatically rewrite Boogie
programs to reduce their verification times.
]]></content:encoded></item><item><title>Your way of adding attributes to structs savely</title><link>https://www.reddit.com/r/golang/comments/1l6nimj/your_way_of_adding_attributes_to_structs_savely/</link><author>/u/ArtisticRevenue379</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 8 Jun 2025 21:32:55 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I often find myself in a situation where I add an attribute to a struct:type PublicUserData struct { ID string `json:"id"` Email string `json:"email"` } type PublicUserData struct { ID string `json:"id"` Email string `json:"email"` IsRegistered bool `json:"isRegistered"` } However, this can lead to cases where I construct the struct without the new attribute:PublicUserData{ ID: reqUser.ID, Email: reqUser.Email, } This leads to unexpected behaviour.How do you handle this? Do you have parsing functions or constructors with private types? Or am I just stupid for not checking the whole codebase and see if I have to add the attribute manually?]]></content:encoded></item><item><title>Building supercomputers for autocrats probably isn&apos;t good for democracy</title><link>https://helentoner.substack.com/p/supercomputers-for-autocrats</link><author>rbanffy</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 21:11:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why Android can&apos;t use CDC Ethernet (2023)</title><link>https://jordemort.dev/blog/why-android-cant-use-cdc-ethernet/</link><author>goodburb</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 20:49:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[If you just want the answer to the question posed in the title, click the TLDR below and then move on with your day. Otherwise, buckle in, we’re going debugging; this post is mostly about my thought process and techniques I used to arrive at the answer rather than the answer itself.Android contains support for USB ethernet adapters. There’s even menus for them!This means that if you very carefully select a USB Ethernet adapter that you know has a chipset compatible with your Android device, you can plug it in and these settings will spring to life. How do you know what chipsets are compatible with your phone?I’m not entirely kidding. If the company that you bought your phone from sells a USB ethernet adapter as an accessory to it, you have a pretty good chance of that one working. Otherwise, it’s hit-or-miss; phone manufacturers rarely, if ever, publish lists of supported Ethernet adapters. The best you’re going to get is finding a forum post from someone that has the same phone as you saying that they bought a particular adapter that worked, and hoping you can find the same thing to buy.As you may know, if you dig deep beneath Android’s Googly carapace, you’ll find a Linux kernel. To build the Linux kernel, you must first configure it. This configuration determines what features and hardware the resulting kernel will support. Thus, the list of Ethernet adapters supported by your phone will more-or-less correspond to those selected in the kernel configuration for your phone, although it’s possible (but unlikely) that your phone’s manufacturer doesn’t ship all of the drivers that they build, or that they build additional third-party drivers separately.So, in order to figure out what Ethernet adapters your phone supports, you’re going to want to find your phone’s kernel configuration. How do we do that?First, enable USB debugging and install ADBIf you’d like to follow along with this blog post, you’re going to need enable USB debugging and to install ADB (Android Debug Bridge) — this is a command-line tool that is used by developers to interact with Android devices. In this post, we will be using it to run shell commands on a phone.There’s good documentation elsewhere on how to do these things so I’m not going to waste time by rewriting it poorly. Instead, have some links:Congratulations, you can now run commands on your phone. Type  and press enter when you’re ready to exit the ADB shell.Next, we need to switch things up so that ADB connects to the phone over the network, instead of via USB. We need to do this because we’re going to try plugging some network adapters into the phone’s USB port, so we can’t also use the port for debugging.With your phone connected to your computer via USB:Connect your phone to the same network as your computer via wifiFigure out your phone’s IP address - you can do this by digging around the Settings app, or you can try With the phone still connected via USB, run Disconnect the USB cable from the phoneReconnect to the phone by running adb connect YOUR_PHONE_IP:5555 (replacing YOUR_PHONE_IP with the IP address from the phone)Try  to make sure it still worksOnce you have ADB working over the network, you can proceed with trying to figure out what version of the kernel your Android device is running.If you have a newer phone…If your phone shipped with Android 11 or later, you have something called a GKI kernel - in this case, Google builds the kernel and the phone manufacturer puts all of their model-specific secret sauce into kernel modules. In this case, you can find the configuration that Google is using by navigating to the appropriate branch of the kernel repository, and looking at the file arch/$ARCH/configs/gki_defconfig, where  is the processor architecture of your phone. For example, if your phone has a 64-bit ARM processor (and it almost certainly does) then you will find this configuration at arch/arm64/configs/gki_defconfig.How do I find out for sure what kernel version and processor architecture my phone has?Now that we have the ability to run shell commands on the phone, we can turn to good old  to discover the kernel version and architecture that’s currently running.Run  on the phone, either by running  and then running , or all in one go by running .You should get output something like this:You’ll the kernel version in the third field and the architecture in the second-to-last; you’ll have to make an educated guess about which branch or tag in Google’s kernel repository corresponds to the one running on your phone.What if I have an older phone?If you have an older phone, then you’re in the same boat as me; I have an iPhone as a daily driver, but I keep a Samsung Galaxy s20 around as an Android testbed. Unfortunately, the s20 shipped with Android 10, which is the version just before all of this standardized kernel stuff from Google became required. Even though the s20 has since been upgraded to Android 13, Google doesn’t require phone manufacturers to update the kernel along with the Android version, and so Samsung didn’t; it still runs a kernel based on Linux 4.19.In this case, you need to get the kernel configuration from your phone manufacturer, so you’d better hope they’re actually doing regular source releases. Samsung does do this; you can find sources for their phones at opensource.samsung.com.Once you have the sources for your device, you’re going to have to dig around a bit to figure out what kernel config. The sources I obtained for my phone from Samsung included a ; inside of this archive was a Linux kernel source tree, along with a few additions. One of those additions was a shell script called , which goes a little something like this:If you squint at this long enough, you’ll spot a reference to something that looks like a kernel config: vendor/x1q_usa_singlex_defconfig. There isn’t a subdirectory called  in the root of the archive, so I used  to figure out exactly where the file lives:Aha, there it is, deeply nested in a subdirectory.Finding the kernel config sounds hard, is there an easier way?There might be, if you’re lucky! Give this a shot:If you’re lucky, and your phone manufacturer has enabled the relevant kernel option, then a compressed copy of the configuration that your kernel was compiled with is available at . If this is the case, you’ll have a large amount of output streaming to your terminal. You probably want to redirect it somewhere so you can peruse it at your leisure:If you’re unlucky, you’ll see something like this:In this case, there is no easy way out; you’ll have to refer to the sources your phone’s kernel was built from.What does a kernel configuration look like?Your kernel configuration should look very similar to this, but not identical, unless you have the same phone that I do.OK, I have the kernel configuration for my phone, what now?For the purpose of determining which USB Ethernet adapters the kernel supports, most of the configuration variables that we are interested will start with , so just  the kernel configuration for that string:Look for a  that looks like it relates to the chipset of the adapter you want to use. The best news is if it is set to ; that means the driver is built-in to your kernel and that your phone’s kernel definitely supports that chipset. If it’s set to , that’s still  good news; that means that the driver was compiled as a module when your kernel was built, and that the module is likely loadable on your phone unless your phone’s manufacturer specifically left it out. If you see , then that is the worst news; the driver was neither built-in to your kernel, nor was it compiled as a module, so it’s likely not available for you to use.If you’re having trouble figuring out which configuration items correspond to which chipsets, have a look at  in your kernel tree. This file will contain extended descriptions of each configuration item.Unfortunately, to figure out which chipset a particular adapter uses, you’re mostly back to hearsay; few manufacturers of USB Ethernet adapters explicitly advertise which chipset they use.So what’s this about CDC Ethernet and why should I care?CDC stands for Communications Device Class. This is a set of interrelated standards that manufacturers of USB devices can follow; among them are a trio of standards called EEM (Ethernet Emulation Model), ECM (Ethernet Control Model), and NCM (Network Control Model) that can be used to build USB Ethernet adapters. Most of the difference between these three standards is a matter of complexity; EEM is the simplest to implement and is easy to support on underpowered devices, but may not result in the best performance. ECM is more complex to implement for both the USB host and the device, but promises better performance than EEM; NCM is a successor to ECM that promises even higher speeds. Many devices implement more than one of these protocols, and leave it up to the host operating system to communicate with the device using the one that it prefers.The point of these standards is that, assuming manufacturers follow them, operating systems can provide a single common driver that works with a variety of drivers. You generally don’t need special drivers for USB keyboards or mice because of the USB HID standard; the USB CDC standard attempts to accomplish the same for USB networking devices.One particularly fun thing is that Linux implements both the host and the device side of the CDC Ethernet standards. That means that if you have hardware with a USB OTG port, which is common on the Raspberry Pi and other small ARM devices, you can tell the kernel to use that port to pretend to be an Ethernet adapter. This creates a USB network interface on the host that is directly connected to an interface on the guest; this lets you build cool things like embedded routers, firewalls, and VPN gateways that look like just another Ethernet adapter to the host.Linux, as well as Windows and macOS (but not iOS) include drivers for CDC Ethernet devices. Unfortunately, none of this works on Android devices, despite Android being based on Linux. Why is Android like this?Based on the kernel configuration, Android  to support CDCLet’s have another look at our kernel config, and grep for USB_NET_CDC:Here we can see that Samsung has built support for all 3 CDC Ethernet standards into their kernel ( corresponds to ECM). Google’s GKI kernels are somewhat less generous and appear to leave out ECM and NCM, but still include support for EEM as a module.I’ve got a device with an OTG port that I’ve configured as an Ethernet gadget. It works when I plug it into my Mac. It works when I plug it into my Ubuntu desktop. It even works when I plug it into my Windows game machine (actually the same computer as the Ubuntu desktop, booted off of a different drive ). It doesn’t work at all when I plug it into my Galaxy s20. The Ethernet settings are still greyed out:Let’s grab a shell on the phone and dig in a bit.The Linux kernel exposes information about itself in a pseudo-filesystem called sysfs - this looks like a directory tree full of files, but reading the files actually gets you information about the current state of the kernel.Among other things, sysfs contains a directory named , which contains one entry for every network interface that the kernel is aware of. Let’s connect our Ethernet gadget to the phone and see if anything shows up there:Could  be the gadget? Let’s use  to check it out:That certainly looks like our gadget. Too bad the interface is down. Unfortunately, the Ethernet settings on the phone are still greyed out:Let’s unplug the gadget and make sure  goes away when we do:It looks like we’re using EEM mode. In addition to the  module, Linux also includes a thing called configfs that can be used to create custom gadgets. Let’s try one that only supports ECM and see if that works:It’s still detected, but it’s still down. Will NCM fare any better?So why doesn’t CDC work on Android?At this point, we’ve more-or-less established that everything is fine on the kernel level. I’m pretty sure that if I wanted to, I could root this phone, manually configure the interface with , and it would pass traffic just fine. That means the problem must be somewhere in the stack of software above the kernel.If this was a regular Linux system, this is the point where I’d start poking at systemd-networkd, or NetworkManager, or ifupdown, depending on the particulars. This is not a regular Linux system, though; it’s an Android device, and none of that stuff exists here. What do I know about how Android configures network interfaces? I know nothing about how Android configures network interfaces. How do we figure this out?Well, Android is at least sort of open source; many of the good bits are closed behind the veil of something called “Google Play Services” but maybe there’s enough in the sources that are released to figure this out.To play along with this bit, you’ll need to download the source to Android. This is a whole process on its own, so I’ll leave you to Google’s documentation for this, except to note that you’ll need a special tool called . This seems to be meant to make it easier to download sources from multiple Git repositories at once; sometimes it feels like I’m the only person that actually likes Git submodules. There are a lot of sources to download, so start this process and then go knock off a few shrines in Zelda while it wraps up.I figure that searching for the string  is probably a good starting point. Because there is so much source to go through, I’m going to skip vanilla  this time and enlist the aid of ripgrep. There’s a lot of configuration files and other clutter in the Android sources, as well as most of a Linux distro, but I know that any code that we’re going to care about here is likely written in Java, so I’m going to restrict  to searching in Java files:At this point, there’s not much else to do but look at the files where we’ve got hits and try to figure out what part of the code we can blame for our problem. Fortunately for you, I’ve saved you the trouble. After reading a bunch of Android code, I’m certain that our culprit is . This appears to be a service that listens on a Netlink socket and receives notifications from the kernel about new network interfaces. The EthernetTracker contains a method that determines if an Ethernet interface is “valid”; if it is valid, the EthernetTracker reports to the rest of the system that an interface is available, and the Settings app allows the interface to be configured. If an interface is not valid, then the EthernetTracker simply ignores it.How does the EthernetTracker determine if an interface is valid?Where does this regex come from?It comes from a method called getInterfaceRegexFromResource. Where does that method get it from?There’s actually a nice comment at the top of the file that explains this:Let’s go back to ripgrep to see if we can skip to finding out what config_ethernet_iface_regex is:…and there it is. The default value of config_ethernet_iface_regex is ; in regex parlance, that means the literal string , followed by a digit.The kernel on the phone calls our CDC Ethernet gadget . This doesn’t start with the string , so EthernetTracker ignores it. Unfortunately, this setting is not user-configurable, although you can hack it by rooting the phone.It really is that silly; an entire USB device class brought low by a bum regex.I can’t tell if this is intentional or not; it feels like an oversight by Google, since even the newest GKI kernels apparently go out of their way to include support for EEM adapters, but because the interface name doesn’t match the regex, the kernel’s support for EEM adapters is unusable. This puts you in a rather perverse situation when shopping for USB Ethernet adapters to use with Android; instead of looking for devices that implement the CDC standards, you need to explicitly  the standards-based devices and look for something that is supported with a vendor/chipset-specific driver.I hope you enjoyed going on this journey with me, or even better that I saved you from duplicating my efforts. Perhaps if I am feeling feisty, I will try to figure out how to submit a patch to Android to change that regex to  in the next few weeks. If a real Android dev or someone at Google reads this and beats me to the punch, I owe you the beverage of your choice.]]></content:encoded></item><item><title>Omnimax</title><link>https://computer.rip/2025-06-08-Omnimax.html</link><author>aberoham</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 20:41:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In a previous life, I worked for a location-based entertainment company, part
of a huge team of people developing a location for Las Vegas, Nevada. It was
COVID, a rough time for location-based anything, and things were delayed more
than usual. Coworkers paid a lot of attention to another upcoming Las Vegas
attraction, one with a vastly larger budget but still struggling to make
schedule: the MSG (Madison Square Garden) Sphere.I will set aside jokes about it being a square sphere, but they were perhaps
one of the reasons that it underwent a pre-launch rebranding to merely the
Sphere. If you are not familiar, the Sphere is a theater and venue in Las
Vegas. While it's know mostly for the video display on the  that's
just marketing for the : a digital dome theater, with seating at a
roughly 45 degree stadium layout facing a near hemisphere of video displays.It is a "near" hemisphere because the lower section is truncated to allow a
flat floor, which serves as a stage for events but is also a practical
architectural decision to avoid completely unsalable front rows. It might seem
a little bit deceptive that an attraction called the Sphere does not quite pull
off even a hemisphere of "payload," but the same compromise has been reached by
most dome theaters. While the use of digital display technology is flashy,
especially on the exterior, the Sphere is not quite the innovation that it
presents itself as. It is just a continuation of a long tradition of dome
theaters. Only time will tell, but the financial difficulties of the Sphere
suggest that follows the tradition faithfully: towards commercial failure.You could make an argument that the dome theater is hundreds of years old, but
I will omit it. Things really started developing, at least in our modern
tradition of domes, with the 1923 introduction of the Zeiss planetarium
projector. Zeiss projectors and their siblings used a complex optical and
mechanical design to project accurate representations of the night sky. Many
auxiliary projectors, incorporated into the chassis and giving these projectors
famously eccentric shapes, rendered planets and other celestial bodies. Rather
than digital light modulators, the images from these projectors were formed by
purely optical means: perforated metal plates, glass plates with etched
metalized layers, and fiber optics. The large, precisely manufactured image
elements and specialized optics created breathtaking images.While these projectors had considerable entertainment value, especially in the
mid-century when they represented some of the most sophisticated projection
technology yet developed, their greatest potential was obviously in education.
Planetarium projectors were fantastically expensive (being hand-built in
Germany with incredible component counts) [1], they were widely installed in
science museums around the world. Most of us probably remember a dogbone-shaped
Zeiss, or one of their later competitors like Spitz or Minolta, from our
youths. Unfortunately, these marvels of artistic engineering were mostly
retired as digital projection of near comparable quality became similarly
priced in the 2000s.But we aren't talking about projectors, we're talking about theaters.
Planetarium projectors were highly specialized to rendering the night sky, and
everything about them was intrinsically spherical. For both a reasonable
viewing experience, and for the projector to produce a geometrically correct
image, the screen had to be a spherical section. Thus the planetarium itself:
in its most traditional form, rings of heavily reclined seats below a
hemispherical dome. The dome was rarely a full hemisphere, but was usually
truncated at the horizon. This was mostly a practical decision but integrated
well into the planetarium experience, given that sky viewing is usually poor
near the horizon anyway. Many planetaria painted a city skyline or forest
silhouette around the lower edge to make the transition from screen to wall
more natural. Later, theatrical lighting often replaced the silhouette,
reproducing twilight or the haze of city lights.Unsurprisingly, the application-specific design of these theaters also limits
their potential. Despite many attempts, the collective science museum industry
has struggled to find entertainment programming for planetaria much beyond Pink
Floyd laser shows [2]. There just aren't that many things that you look 
at. Over time, planetarium shows moved in more narrative directions.  Film
projection promised new flexibility---many planetaria with optical star
projectors were also equipped with film projectors, which gave show producers
exciting new options. Documentary video of space launches and animations of
physical principles became natural parts of most science museum programs, but
were a bit awkward on the traditional dome. You might project four copies of
the image just above the horizon in the four cardinal directions, for example.
It was very much a compromise.With time, the theater adapted to the projection once again: the domes began to
tilt. By shifting the dome in one direction, and orienting the seating towards
that direction, you could create a sort of compromise point between the
traditional dome and traditional movie theater. The lower central area of the
screen was a reasonable place to show conventional film, while the full size of
the dome allowed the starfield to almost fill the audience's vision. The
experience of the tilted dome is compared to "floating in space," as opposed to
looking up at the sky.In true Cold War fashion, it was a pair of weapons engineers (one nuclear
weapons, the other missiles) who designed the first tilted planetarium. In
1973, the planetarium of what is now called the Fleet Science Center in San
Diego, California opened to the public. Its dome was tilted 25 degrees to the
horizon, with the seating installed on a similar plane and facing in one
direction. It featured a novel type of planetarium projector developed by Spitz
and called the Space Transit Simulator. The STS was not the first, but still an
early mechanical projector to be controlled by a computer---a computer that
also had simultaneous control of other projectors and lighting in the theater,
what we now call a show control system.Even better, the STS's innovative optical design allowed it to warp or bend the
starfield to simulate its appearance from locations other than earth. This was
the "transit" feature: with a joystick connected to the control computer, the
planetarium presenter could "fly" the theater through space in real time. The
STS was installed in a well in the center of the seating area, and its compact
chassis kept it low in the seating area, preserving the spherical geometry (with
the projector at the center of the sphere) without blocking the view of audience
members sitting behind it and facing forward.And yet my main reason for discussing the Fleet planetarium is not the the
planetarium projector at all. It is a second projector, an "auxiliary" one,
installed in a second well behind the STS. The designers of the planetarium
intended to show film as part of their presentations, but they were not content
with a small image at the center viewpoint. The planetarium commissioned a few
of the industry's leading film projection experts to design a film projection
system that could fill the entire dome, just as the planetarium projector did.They knew that such a large dome would require an exceptionally sharp image.
Planetarium projectors, with their large lithographed slides, offered excellent
spatial resolution. They made stars appear as point sources, the same as in the
night sky. 35mm film, spread across such a large screen, would be obviously
blurred in comparison. They would need a very large film format.Fortuitously, almost simultaneously the Multiscreen Corporation was developing
a "sideways" 70mm format. This 15-perf format used 70mm film but fed it through
the projector sideways, making each frame much larger than typical 70mm film.
In its debut, at a temporary installation in the 1970 Expo Osaka, it was dubbed
IMAX. IMAX made an obvious basis for a high-resolution projection system, and
so the then-named IMAX Corporation was added to the planetarium project. The
Fleet's film projector ultimately consisted of an IMAX film transport with a
custom-built compact, liquid-cooled lamphouse and spherical fisheye lens
system.The large size of the projector, the complex IMAX framing system and cooling
equipment, made it difficult to conceal in the theater's projector well.
Threading film into IMAX projectors is quite complex, with several checks the
projectionist must make during a pre-show inspection. The projectionist needed
room to handle the large film, and to route it to and from the enormous reels.
The projector's position in the middle of the seating area left no room for any
of this. We can speculate that it was, perhaps, one of the designer's missile
experience that lead to the solution: the projector was serviced in a large
projection room beneath the theater's seating. Once it was prepared for each
show, it rose on near-vertical rails until just the top emerged in the theater.
Rollers guided the film as it ran from a platter, up the shaft to the
projector, and back down to another platter. Cables and hoses hung below the
projector, following it up and down like the traveling cable of an elevator.To advertise this system, probably the greatest advance in film projection
since the IMAX format itself, the planetarium coined the term Omnimax.Omnimax was not an easy or economical format. Ideally, footage had to be taken
in the same format, using a 70mm camera with a spherical lens system. These
cameras were exceptionally large and heavy, and the huge film format limited
cinematographers to short takes. The practical problems with Omnimax filming
were big enough that the first Omnimax films faked it, projecting to the larger
spherical format from much smaller conventional negatives. This was the case
for "Voyage to the Outer Planets" and "Garden Isle," the premier films at
the Fleet planetarium. The history of both is somewhat obscure, the latter
especially."Voyage to the Outer Planets" was executive-produced by Preston Fleet, a
founder of the Fleet center (which was ultimately named for his father, a WWII
aviator). We have Fleet's sense of showmanship to thank for the invention of
Omnimax: He was an accomplished business executive, particularly in the
photography industry, and an aviation enthusiast who had his hands in more than
one museum. Most tellingly, though, he had an eccentric hobby. He was a theater
organist. I can't help but think that his passion for the theater organ, an
instrument almost defined by the combination of many gizmos under
electromechanical control, inspired "Voyage." The film, often called a
"multimedia experience," used multiple projectors throughout the planetarium to
depict a far-future journey of exploration. The Omnimax film depicted travel
through space, with slide projectors filling in artist's renderings of the many
wonders of space.The ten-minute Omnimax film was produced by Graphic Films Corporation, a brand
that would become closely associated with Omnimax in the following decades.
Graphic was founded in the midst of the Second World War by Lester Novros, a
former Disney animator who found a niche creating training films for the
military. Novros's fascination with motion and expertise in presenting
complicated 3D scenes drew him to aerospace, and after the war he found much of
his business in the newly formed Air Force and NASA. He was also an enthusiast
of niche film formats, and Omnimax was not his first dome.For the 1964 New York World's Fair, Novros and Graphic Films had produced "To
the Moon and Beyond," a speculative science film with thematic similarities to
"Voyage" and more than just a little mechanical similarity. It was presented in
Cinerama 360, a semi-spherical, dome-theater 70mm format presented in a special
theater called the Moon Dome. "To the Moon and Beyond" was influential in many
ways, leading to Graphic Films' involvement in "2001: A Space Odyssey" and its
enduring expertise in domes.The Fleet planetarium would not remain the only Omnimax for long. In 1975, the
city of Spokane, Washington struggled to find a new application for the
pavilion built for Expo '74 [3]. A top contender: an Omnimax theater, in some
ways a replacement for the temporary IMAX theater that had been constructed for
the actual Expo. Alas, this project was not to be, but others came along: in
1978, the Detroit Science Center opened the second Omnimax theater ("the
machine itself looks like and is the size of a front loader," the  wrote). The Science Museum of Minnesota, in St. Paul, followed shortly
after.Omnimax hit prime time the next year, with the 1979 announcement of an Omnimax
theater at Caesars Palace in Las Vegas, Nevada. Unlike the previous
installations, this 380-seat theater was purely commercial. It opened with the
1976 IMAX film "To Fly!," which had been optically modified to fit the Omnimax
format. This choice of first film is illuminating. "To Fly!" is a 27 minute
documentary on the history of aviation in the United States, originally
produced for the IMAX theater at the National Air and Space Museum [4]. It doesn't
exactly seem like casino fare.The IMAX format, the flat-screen one, was born of world's fairs. It premiered
at an Expo, reappeared a couple of years later at another one, and for the
first years of the format most of the IMAX theaters built were associated with
either a major festival or an educational institution. This noncommercial
history is a bit hard to square with the modern IMAX brand, closely associated
with major theater chains and the Marvel Cinematic Universe.Well, IMAX took off, and in many ways it sold out. Over the decades since the
1970 Expo, IMAX has met widespread success with commercial films and theater
owners. Simultaneously, the definition or criteria for IMAX theaters have
relaxed, with smaller screens made permissible until, ultimately, the
transition to digital projection eliminated the 70mm film and more or less
reduce IMAX to just another ticket surcharge brand. It competes directly with
Cinemark xD, for example. To the theater enthusiast, this is a pretty sad turn
of events, a Westinghouse-esque zombification of a brand that once heralded the
field's most impressive technical achievements.The same never happened to Omnimax. The Caesar's Omnimax theater was an odd
exception; the vast majority of Omnimax theaters were built by science museums
and the vast majority of Omnimax films were science documentaries. Quite a few
of those films had been specifically commissioned by science museums, often on
the occasion of their Omnimax theater opening. The Omnimax community was fairly
tight, and so the same names recur.The Graphic Films Corporation, which had been around since the beginning,
remained so closely tied to the IMAX brand that they practically shared
identities. Most Omnimax theaters, and some IMAX theaters, used to open with a
vanity card often known as "the wormhole." It might be hard to describe beyond
"if you know you know," it certainly made an impression on everyone I know that
grew up near a theater that used it. There are some
videos, although unfortunately
none of them are very good.I have spent more hours of my life than I am proud to admit trying to untangle
the history of this clip. Over time, it has appeared in many theaters with many
different logos at the end, and several variations of the audio track. This is
in part informed speculation, but here is what I believe to be true: the
"wormhole" was originally created by Graphic Films for the Fleet planetarium
specifically, and ran before "Voyage to the Outer Planets" and its
double-feature companion "Garden Isle," both of which Graphic Films had worked
on. This original version ended with the name Graphic Films, accompanied by an
odd sketchy drawing that was also used as an early logo of the IMAX
Corporation.  Later, the same animation was re-edited to end with an IMAX logo.This version ran in both Omnimax and conventional IMAX theaters, probably as a
result of the extensive "cross-pollination" of films between the two formats.
Many Omnimax films through the life of the format had actually been filmed for
IMAX, with conventional lenses, and then optically modified to fit the Omnimax
dome after the fact. You could usually tell: the reprojection process created
an unusual warp in the image, and more tellingly, these pseudo-Omnimax films
almost always centered the action at the middle of the IMAX frame, which was
too high to be quite comfortable in an Omnimax theater (where the "frame
center" was well above the "front center" point of the theater). Graphic Films
had been involved in a lot of these as well, perhaps explaining the animation
reuse, but it's just as likely that they had sold it outright to the IMAX
corporation which used it as they pleased.For some reason, this version also received new audio that is mostly the same
but slightly different. I don't have a definitive explanation, but I think
there may have been an audio format change between the very early Omnimax
theaters and later IMAX/Omnimax systems, which might have required remastering.Later, as Omnimax domes proliferated at science museums, the IMAX Corporation
(which very actively promoted Omnimax to education) gave many of these theaters
custom versions of the vanity card that ended with the science museum's own
logo. I have personally seen two of these, so I feel pretty confident that they
exist and weren't all that rare (basically 2 out of 2 Omnimax theaters I've
visited used one), but I cannot find any preserved copies.Another recurring name in the world of IMAX and Omnimax is MacGillivray Freeman
Films. MacGillivray and Freeman were a pair of teenage friends from Laguna
Beach who dropped out of school in the '60s to make skateboard and surf films.
This is, of course, a rather cliché start for documentary filmmakers but we
must allow that it was the '60s and they were pretty much the ones creating the
cliché. Their early films are hard to find in anything better than VHS rip
quality, but worth watching: Wikipedia notes their significance in pioneering
"action cameras," mounting 16mm cinema cameras to skateboards and surfboards,
but I would say that their cinematography was innovative in more ways than just
one. The 1970 "Catch the Joy," about sandrails, has some incredible shots that
I struggle to explain. There's at least one where they definitely cut the shot
just a couple of frames before a drifting sandrail flung their camera all the
way down the dune.For some reason, I would speculate due to their reputation for exciting
cinematography, the National Air and Space Museum chose MacGillivray and
Freeman for "To Fly!".  While not the first science museum IMAX documentary by
any means (that was, presumably, "Voyage to the Outer Planets" given the
different subject matter of the various Expo films), "To Fly!" might be called
the first modern one. It set the pattern that decades of science museum films
followed: a film initially written by science educators, punched up by
producers, and filmed with the very best technology of the time. Fearing that
the film's history content would be dry, they pivoted more towards
entertainment, adding jokes and action sequences. "To Fly!" was a hit, running
in just about every science museum with an IMAX theater, including Omnimax.Sadly, Jim Freeman died in a helicopter crash shortly after production.
Nonetheless, MacGillivray Freeman Films went on. Over the following decades,
few IMAX science documentaries were made that didn't involve them somehow.
Besides the films they produced, the company consulted on action sequences
in most of the format's popular features.I had hoped to present here a thorough history of the films that were actually
produced in the Omnimax format. Unfortunately, this has proven very difficult:
the fact that most of them were distributed only to science museums means that
they are very spottily remembered, and besides, so many of the films that ran
in Omnimax theaters were converted from IMAX presentations that it's hard to
tell the two apart. I'm disappointed that this part of cinema history isn't
better recorded, and I'll continue to put time into the effort. Science museum
documentaries don't get a lot of attention, but many of the have involved
formidable technical efforts.Consider, for example, the cameras: befitting the large film, IMAX cameras
themselves are very large. When filming "To Fly!", MacGillivray and Freeman
complained that the technically very basic 80 pound cameras required a lot of
maintenance, were complex to operate, and wouldn't fit into the "action cam"
mounting positions they were used to. The cameras were so expensive, and so
rare, that they had to be far more conservative than their usual approach out
of fear of damaging a camera they would not be able to replace. It turns out
that they had it easy. Later IMAX science documentaries would be filmed in
space ("The Dream is Alive" among others) and deep underwater ("Deep Sea 3D"
among others). These IMAX cameras, modified for simpler operation and housed
for such difficult environments, weighed over 1,000 pounds. Astronauts had to
be trained to operate the cameras; mission specialists on Hubble service
missions had wrangling a 70-pound handheld IMAX camera around the cabin and
developing its film in a darkroom bag among their duties. There was a lot of
film to handle: as a rule of thumb, one mile of IMAX film is good for eight
and a half minutes.I grew up in Portland, Oregon, and so we will make things a bit more
approachable by focusing on one example: The Omnimax theater of the Oregon
Museum of Science and Industry, which opened as part of the museum's new
waterfront location in 1992. This 330-seat boasted a 10,000 sq ft dome and 15
kW of sound. The premier feature was "Ring of Fire," a volcano documentary
originally commissioned by the Fleet, the Fort Worth Museum of Science and
Industry, and the Science Museum of Minnesota. By the 1990s, the later era of
Omnimax, the dome format was all but abandoned as a commercial concept. There
were, an announcement article notes, around 90 total IMAX theaters (including
Omnimax) and 80 Omnimax films (including those converted from IMAX) in '92.
Considering the heavy bias towards science museums among these theaters, it
was very common for the films to be funded by consortia of those museums.Considering the high cost of filming in IMAX, a lot of the documentaries had a
sort of "mashup" feel. They would combine footage taken in different times and
places, often originally for other projects, into a new narrative. "Ring of
Fire" was no exception, consisting of a series of sections that were sometimes
more loosely connected to the theme. The 1982 Loma Prieta earthquake was a
focus, and the eruption of Mt. St. Helens, and lava flows in Hawaii. Perhaps
one of the reasons it's hard to catalog IMAX films is this mashup quality, many
of the titles carried at science museums were something along the lines of
"another ocean one." I don't mean this as a criticism, many of the IMAX
documentaries were excellent, but they were necessarily composed from
painstakingly gathered fragments and had to cover wide topics.Given that I have an announcement feature piece in front of me, let's also use
the example of OMSI to discuss the technical aspects. OMSI's projector cost
about $2 million and weighted about two tons. To avoid dust damaging the
expensive prints, the "projection room" under the seating was a
positive-pressure cleanroom. This was especially important since the paucity of
Omnimax content meant that many films ran regularly for years. The 15 kW
water-cooled lamp required replacement at 800 to 1,000 hours, but
unfortunately, the price is not noted.By the 1990s, Omnimax had become a rare enough system that the projection
technology was a major part of the appeal. OMSI's installation, like most later
Omnimax theaters, had the audience queue below the seating, separated from the
projection room by a glass wall. The high cost of these theaters meant that
they operated on high turnovers, so patrons would wait in line to enter
immediately after the previous showing had exited. While they waited, they
could watch the projectionist prepare the next show while a museum docent
explained the equipment.I have written before about multi-channel audio
formats, and
Omnimax gives us some more to consider. The conventional audio format for much
of Omnimax's life was six-channel: left rear, left screen, center screen, right
screen, right rear, and top. Each channel had an independent bass cabinet (in
one theater, a "caravan-sized" enclosure with eight JBL 2245H 46cm woofers),
and a crossover network fed the lowest end of all six channels to a "sub-bass"
array at screen bottom. The original Fleet installation also had sub-bass
speakers located beneath the audience seating, although that doesn't seem to
have become common.IMAX titles of the '70s and '80s delivered audio on eight-track magnetic tape,
with the additional tracks used for synchronization to the film. By the '90s,
IMAX had switched to distributing digital audio on three CDs (one for each two
channels). OMSI's theater was equipped for both, and the announcement amusingly
notes the availability of cassette decks. A semi-custom audio processor made
for IMAX, the Sonics TAC-86, managed synchronization with film playback and
applied equalization curves individually calibrated to the theater.IMAX domes used perforated aluminum screens (also the norm in later
planetaria), so the speakers were placed behind the screen in the scaffold-like
superstructure that supported it. When I was young, OMSI used to start
presentations with a demo program that explained the large size of IMAX film
before illuminating work lights behind the screen to make the speakers visible.
Much of this was the work of the surprisingly sophisticated show control system
employed by Omnimax theaters, a descendent of the PDP-15 originally installed
in the Fleet.Despite Omnimax's almost complete consignment to science museums, there were
some efforts it bringing commercial films. Titles like Disney's "Fantasia" and
"Star Wars: Episode III" were distributed to Omnimax theaters via optical
reprojection, sometimes even from 35mm originals. Unfortunately, the quality of
these adaptations was rarely satisfactory, and the short runtimes (and
marketing and exclusivity deals) typical of major commercial releases did not
always work well with science museum schedules. Still, the cost of converting
an existing film to dome format is pretty low, so the practice continues today.
"Star Wars: The Force Awakens," for example, ran on at least one science museum
dome. This trickle of blockbusters was not enough to make commercial Omnimax
theaters viable.Caesars Palace closed, and then demolished, their Omnimax theater in 2000. The
turn of the 21st century was very much the beginning of the end for the dome
theater. IMAX was moving away from their film system and towards digital
projection, but digital projection systems suitable for large domes were still
a nascent technology and extremely expensive. The end of aggressive support
from IMAX meant that filming costs became impractical for documentaries, so
while some significant IMAX science museum films were made in the 2000s, the
volume definitely began to lull and the overall industry moved away from IMAX
in general and Omnimax especially.It's surprising how unforeseen this was, at least to some. A ten-screen
commercial theater in Duluth opened an Omnimax theater in 1996! Perhaps due to
the sunk cost, it ran until 2010, not a bad closing date for an Omnimax
theater. Science museums, with their relatively tight budgets and less
competitive nature, did tend to hold over existing Omnimax installations well
past their prime. Unfortunately, many didn't: OMSI, for example, closed its
Omnimax theater in 2013 for replacement with a conventional digital theater
that has a large screen but is not IMAX branded.Fortunately, some operators hung onto their increasingly costly Omnimax domes
long enough for modernization to become practical. The IMAX Corporation
abandoned the Omnimax name as more of the theaters closed, but continued to
support "IMAX Dome" with the introduction of a digital laser projector with
spherical optics. There are only ten examples of this system. Others, including
Omnimax's flagship at the Fleet Science Center, have been replaced by custom
dome projection systems built by competitors like Sony.Few Omnimax projectors remain. The Fleet, to their credit, installed the modern
laser projectors in front of the projector well so that the original film
projector could remain in place. It's still functional and used for reprisals
of Omnimax-era documentaries. IMAX projectors in general are a dying breed, a
number of them have been preserved but their complex, specialized design and
the end of vendor support means that it may become infeasible to keep them
operating.We are, of course, well into the digital era. While far from inexpensive,
digital projection systems are now able to match the quality of Omnimax
projection.  The newest dome theaters, like the Sphere, dispense with
projection entirely. Instead, they use LED display panels capable of far
brighter and more vivid images than projection, and with none of the complexity
of water-cooled arc lamps.Still, something has been lost. There was once a parallel theater industry, a
world with none of the glamor of Hollywood but for whom James Cameron hauled a
camera to the depths of the ocean and Leonardo DiCaprio narrated repairs to the
Hubble. In a good few dozen science museums, two-ton behemoths rose from
beneath the seats, the zenith of film projection technology. After decades of
documentaries, I think people forgot how remarkable these theaters were.Science museums stopped promoting them as aggressively, and much of the
showmanship faded away. Sometime in the 2000s, OMSI stopped running the
pre-show demonstration, instead starting the film directly. They stopped
explaining the projectionist's work in preparing the show, and as they shifted
their schedule towards direct repetition of one feature, there was less for the
projectionist to do anyway. It became just another museum theater, so it's no
wonder that they replaced it with just another museum theater: a generic
big-screen setup with the exceptionally dull name of "Empirical Theater."From time to time, there have been whispers of a resurgence of 70mm film.
Oppenheimer, for example, was distributed to a small number of theaters in this
giant of film formats: 53 reels, 11 miles, 600 pounds of film. Even
conventional IMAX is too costly for the modern theater industry, though.
Omnimax has fallen completely by the wayside, with the few remaining dome
operators doomed to recycling the same films with a sprinkling of newer
reformatted features. It is hard to imagine a collective of science museums
sending another film camera to space.Omnimax poses a preservation challenge in more ways than one. Besides the lack
of documentation on Omnimax theaters and films, there are precious few
photographs of Omnimax theaters and even fewer videos of their presentations.
Of course, the historian suffers where Madison Square Garden hopes to succeed:
the dome theater is perhaps the ultimate in location-based entertainment.
Photos and videos, represented on a flat screen, cannot reproduce the
experience of the Omnimax theater. The 180 horizontal degrees of screen, the
sound that was always a little too loud, in no small part to mask the sound of
the projector that made its own racket in the middle of the seating. You had to
be there.IMAGES: Omnimax projection room at OMSI, Flickr user truk. Omnimax dome with
work lights on at MSI Chicago, Wikimedia Commons user GualdimG. Omnimax
projector at St. Louis Science Center, Flickr user pasa47.[1] I don't have extensive information on pricing, but I know that in the 1960s
an "economy" Spitz came in over $30,000 (~10x that much today).[2] Pink Floyd's landmark album  debuted in a release
event held at the London Planetarium. This connection between Pink Floyd and
planetaria, apparently much disliked by the band itself, has persisted to the
present day. Several generations of Pink Floyd laser shows have been licensed
by science museums around the world, and must represent by far the largest
success of fixed-installation laser projection.[3] Are you starting to detect a theme with these Expos? the World's Fairs,
including in their various forms as Expos, were long one of the main markets
for niche film formats. Any given weird projection format you run into, there's
a decent chance that it was originally developed for some short film for an
Expo. Keep in mind that it's the nature of niche projection formats that they
cannot easily be shown in conventional theaters, so they end up coupled to
these crowd events where a custom venue can be built.[4] The Smithsonian Institution started looking for an exciting new theater in
1970. As an example of the various niche film formats at the time, the
Smithsonian considered a dome (presumably Omnimax), Cinerama (a three-projector
ultrawide system), and Circle-Vision 360 (known mostly for the few surviving
Expo films at Disney World's EPCOT) before settling on IMAX. The Smithsonian
theater, first planned for the Smithsonian Museum of Natural History before
being integrated into the new National Air and Space Museum, was tremendously
influential on the broader world of science museum films. That is perhaps an
understatement, it is sometimes credited with popularizing IMAX in general, and
the newspaper coverage the new theater received throughout North America lends
credence to the idea. It is interesting, then, to imagine how different our
world would be if they had chosen Circle-Vision. "Captain America: Brave New
World" in Cinemark 360.]]></content:encoded></item><item><title>The wire that transforms much of Manhattan into one big, symbolic home (2017)</title><link>https://www.atlasobscura.com/articles/eruv-manhattan-invisible-wire-jewish-symbolic-religious-home</link><author>rmason</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 20:22:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Rabbi Moshe Tauber leaves his home in Rockland County, New York, at about 3:30 a.m. He arrives in Manhattan an hour later and drives the 20-mile length of a nearly invisible series of wires that surrounds most of the borough. He starts at 126th Street in Harlem and drives down, hugging the Hudson River most of the way, to Battery Park and back up along the East River, marking in a small notebook where he notices breaks in the line. Known as an eruv, the wire is a symbolic boundary that allows observant Jews to carry out a range of ordinary activities otherwise forbidden on the Shabbat.Any necessary repairs must be finished before sundown on Friday, when Shabbat begins. The day of rest then lasts until the following day when there’s no more red in the western sky. Throughout that time, observant Jews are prohibited from performing many basic activities, and the observance of this law has been updated over time to reflect current technologies, such as cars, electricity, and keys. “Carrying from one domain to another,” or moving objects between public and private areas, for example, is forbidden. Eruvin (the plural of eruv) transcend this restrictive rule by serving as a symbolic border that links together many private spaces in the community, which in turn permits people to ferry around keys, children, and canes, or push wheelchairs and strollers.But a single break in any part of the line voids that symbolic space. According to the 100 pages devoted to eruvin in the ancient Talmud, the boundary is only effective when the entire line is intact. And there are plenty of ways these breaks can happen. Sometimes it’s the elements, but more often construction is responsible. The wires, attached to telephone and light poles, can be severed or simply pushed down (the eruv must remain at the top of the pole) to make room for maintenance on other lines. And this is where Tauber comes in. “If they’re lousy they’ll just cut the lines and let it go,” he says. He’s been doing this carefully orchestrated monitoring since 2000. The repairs are “a secret operation,” chairman of the Manhattan Eruv Committee Rabbi Adam Mintz told the in 2015. That’s by design.Tauber checks the lines so early in the morning in the interests of efficiency—driving around the island at any other time would be virtually impossible due to traffic. It was Mintz who suggested I go out with Tauber at “the ungodly hour,” but I opted to meet with him at about 8:30 on a Friday morning instead, at 110th Street and Lexington Avenue, where someone had removed the cap from the top of a light pole, leaving the eruv a few inches from the top. I watched as two cable workers made the repairs by snipping the wire and passing it through a hole at the top of the pole.In Manhattan, the required repairs are almost always a thoroughly low-tech endeavor. Aside from the cherry picker used to get to the top of the poles, the only other necessary tools are a spool of wire and wirecutters. After 110th, I rode with Tauber down the Henry Hudson Parkway. He parked on a service road and ran off to tie a broken length of the wire back together. We then met back up with cable workers, on 58th Street and 11th Avenue, where the eruv wire was down for two whole blocks. One worker spooled out the line from the raised cherry-picker basket, while the other drove slowly down to 56th Street.The eruv has only been down for the Shabbat once during Tauber’s tenure, when a 2010 snowstorm shut down most of the city on a Friday. Maintenance crews were unable to get to areas that needed to be fixed in time. For a while, the status of the eruv was reported by a Twitter account (it’s been inactive since last October) that essentially repeated itself on a weekly basis: “The Eruv is Up this Shabbat, October 13–14.” The account also notes November 1, 2012, when Hurricane Sandy caused damage to the eruv in 22 places. But everything was repaired in time for the holy day.Eruvin have been around for 2,000 years, though Manhattan’s line has been in place, in some form or another, for just over a century. The term “eruv” is derived from the Hebrew word for “mixture,” and in Manhattan it’s a fitting title: The line encircling much of the island is a patchwork formed by 20 years of breaks and repairs. It’s only since the late ’90s that there has been a structured system for its maintenance. An early version surrounded the whole island, but no one seemed to know its precise boundaries, and everyone just sort of assumed someone else was in charge of maintaining it. When a group of rabbis in the ’80s took a boat around Manhattan to create a map, they realized that most of the wire was gone. Now it’s more tightly regulated, and subsidized by the community it helps to create.Zachary Levine, Director of Exhibitions and Collections at the National Building Museum, says an eruv “creates a visual language that defines space.” The series of practically invisible wires becomes a necessity that “benefits the most vulnerable people of the community.” He sees it not only as a way for communities to come together, but also as a way for the more affluent to give back. The eruv is funded entirely by the Jewish community, with a considerable portion of that support coming from wealthy philanthropists.For six days of the week, or to passersby outside the community, the eruv is just a simple, more or less invisible, set of strands across physical space. But during Shabbat, the holy day, it takes on an important meaning for those who rely on the symbolic border to expand the domain of their homes while staying true to their belief system. As Levine puts it, the eruv “doesn’t matter, unless it matters to you.”]]></content:encoded></item><item><title>simply_colored is the simplest crate for printing colored text!</title><link>https://github.com/nik-rev/simply-colored</link><author>/u/nikitarevenco</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 8 Jun 2025 19:54:49 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Pointcloud rendering system for the terminal</title><link>https://www.reddit.com/r/rust/comments/1l6kris/pointcloud_rendering_system_for_the_terminal/</link><author>/u/whoShotMyCow</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 8 Jun 2025 19:34:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[a few examples from the pointcloud rendering system that I'm building. clockwise from top left: a cube, some scattered points, spiral viewed from below x-y plane, and spiral viewed from the side. code's not up yet since I'm still cleaning up the point adding interface (currently it only reads from files) but I'll publish it soon. the core rendering mechanism is inspired from terminal3d; and I built it because I wanted something so I could display 3d plots on my website without having to add images (2d plots were solved by gnuplot's dumb rendering mode). Do you see yourself using this? If so, what are some features you think would be great? any comments/suggestions/ideas are welcome, tia!]]></content:encoded></item><item><title>Considering replacing GoMobile with Rust uniffi for shared core mobile/desktop/core/wasm</title><link>https://www.reddit.com/r/rust/comments/1l6jp8e/considering_replacing_gomobile_with_rust_uniffi/</link><author>/u/cinemast</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 8 Jun 2025 18:49:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[We’re working on zeitkapsl.eu an end-to-end encrypted alternative to Google photos, offering native apps for Android, iOS, Desktop and the web, with a shared core implemented in Go, using GoMobile for FFI to iOS and Android. While GoMobile works “okay,” we’ve hit several frustrating limitations that make us looking for alternatives.Some of our main pain points with GoMobile: across the FFI boundary — no slices, arrays, or complex objects, so we rely heavily on protobuf for data passing. Still, we often need to massage types manually.Cross-compilation with  (libwebp, SQLite) is complicated and brittle. Zig came to the rescue here, but it is still a mess. binaries are huge and slow to compile; our web client currently has no shared core logic. We looked at , which is cool but would basically also be a rewrite.Debugging across FFI barriers is basically impossible.No native async/coroutine support on Kotlin or Swift sides, so we rely on callbacks and threading workarounds.We are currently considering to build a spike prototype in Rust to evaluate the following:SQLite CRUD with our schema (media, collections, labels, etc.)FFI support for Android, iOS, desktop — cancellable calls, async if feasibleImage processing: HEIC decode, WebP encode, Lanczos3 resizingProtobuf encoding/decodingONNX Runtime for AI inferenceLocal webserver to serve mediaMP4 parsing and HLS muxingAES-GCM encryption, SHA3, PBKDF2, HKDF, secure key genConfigurable worker pool for processing media in parallelWe’d love to hear from Rust experts:uniffi-rs seems a promising alternative to gomobile, any insights that you can share? Especially with deployment in Android, iOS and WASM environmentsAny recommended crates for above mentioned aspects. We’re also considering alternatives like Kotlin Multiplatform or Zig, but currently Rust looks most promising.I have looked at Bitwarden SDK, they operate in a similar context, except for the media processing. Has someone been working on a project with similar requirements? ]]></content:encoded></item><item><title>I wrote a programming language in Rust for procedural art</title><link>https://www.reddit.com/r/rust/comments/1l6ja4l/i_wrote_a_programming_language_in_rust_for/</link><author>/u/masterofgiraffe</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 8 Jun 2025 18:32:07 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I wanted to share that I’ve been working on a functional programming language aimed at generating procedural art. Although it’s still in the early stages, the language has a defined syntax and a comprehensive standard library. I’ve also been documenting the project on GitBook.I’m looking for users to help explore its potential use cases. There may be many creative applications I haven’t considered, and I’d appreciate identifying any gaps in its capabilities.The language is implemented in Rust and runs an interpreter that compiles code into a collection of shapes, which are then rendered as PNG images. All code is distilled down to a single root function.root = hsl (rand * 360) 0.4 0.2 FILL : grid grid_size = 10 grid = t (-width / 2.0) (-height / 2.0) (ss (float width / grid_size) (collect rows)) rows = for i in 0..grid_size collect (cols i) cols i = for j in 0..grid_size hsl (rand * 360) 0.5 0.6 ( t (i + 0.5) (j + 0.5) (r (rand * 360) (ss 0.375 SQUARE))) If you’re interested in creative coding, I encourage you to take a look!]]></content:encoded></item><item><title>Integrating Rust with other applications</title><link>https://www.reddit.com/r/rust/comments/1l6i7hk/integrating_rust_with_other_applications/</link><author>/u/OnionDelicious3007</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 8 Jun 2025 17:48:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Lately, I’ve been thinking about how to connect a Rust service with a Laravel backend. I wanted to take advantage of Rust’s performance without overcomplicating the communication between the two. That’s when I came across Unix sockets — a simple and extremely fast way for two processes to talk to each other on the same machine.Unlike solutions based on HTTP or TCP, Unix socket communication is handled directly by the operating system’s kernel. This removes the overhead of networking protocols and layers, making it lightweight and efficient. It’s a raw byte-to-byte communication channel, which gives you complete freedom to define your own protocol.In my setup, I built a small server in Rust that listens for connections using UnixListener. Laravel connects to the socket and sends data using the MessagePack format, which is compact and efficient. On the Rust side, I first read the length of the incoming message, then read the actual content. Using rmp_serde, I deserialize the bytes into Rust structs, process the request, and send a response following the same pattern: length first, then the actual data.I implemented a REST-style API to manage a list of people (name and age). To persist the data, I used bincode, a lightweight binary serialization format. It keeps things fast and compact — perfect for small services where a full database might be overkill.I was genuinely impressed by the performance of this approach. Unix socket communication is significantly faster than HTTP — often 2 to 5 times faster than local TCP. You can check out the full project here on GitHub: https://github.com/matheus-git/unix-socket-restThis experiment got me thinking: what other ways are there to integrate Rust with other platforms? Are there higher-level options that don’t require handling raw bytes or custom protocols? I’d love to hear your thoughts!]]></content:encoded></item><item><title>Is this a thing with `goreleaser` or it&apos;s a windows `exe`thing ?</title><link>https://github.com/prime-run/togo/issues/27</link><author>/u/DisplayLegitimate374</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 8 Jun 2025 17:26:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[So this project of mine is as simple as it gets! And someone reported this and seems to be legit! The binary is a simple TUI todo manager. I'm really confused with this! ]]></content:encoded></item><item><title>Administering immunotherapy in the morning seems to matter. Why?</title><link>https://www.owlposting.com/p/the-time-of-day-that-immunotherapy</link><author>abhishaike</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 16:18:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[a now disproven idea. via a viral Twitter threadASCO25cancer stayed under control for longerImportant context: the current standard of care for immunotherapy is not designed with timing in mind.which does dip during the nightTLDR: early-in-the-day immunotherapy administration consistently leads to massive improvements in survival time,What’s going on? Where is this coming from? the amount of them present in cells rises and falls over the course of the dayWhat’s the point of the cycle? One way to understand them is through an evolutionary lens, a way for the body to prepare for dependable environment cues. For example, at the start of our circadian rhythm, we wake up. We crawl out of our safe cocoon — a private bed in modernity, or a predator-sheltered hole in ancient history — and start to engage in very risky behavior, immunologically speaking. Eating leftover food that may be contaminated, being scrapped by bacteria-covered rocks, holding dead animals to roast for dinner, and so on. But, as night comes, we retreat back to our private beds or holes, feasting on freshly cooked food, few interactions with unknown creatures, and little chance for injury as we wind down. How could evolution optimize this process?Well…if you didn’t have any priors on when new antigens would come through the door, you wouldn’t care when T cells decided to exit/enter the lymphatic system. When they exit, they are moving to new tissue. When they enter, they are actively looking for dendritic cells to bind to. Perfectly fine to do this randomly in the null case of uniform antigen exposure. the authors demonstrate that this entire process entirely depends on clock genesBut this is just one immune-circadian tweak that evolution has made. Are there others?That existsThat exists as well.Technically, this was also a result from the prior paper, so this too exists.Also exists!Which means the effectiveness of that green light depends entirely on what the immune system is already doing at that moment.Thus, we can propose a decent argument as to why immunotherapies seem to work best during the start of a circadian rhythm. The immune system, by evolutionary coincidence, is simply most prepared to begin their assault during that time. That would only make sense if immune checkpoint blockades had an extremely short half life that fit into this primed immune system period, but they don’t.Well, you’ve got me there! I am unsure what the answer could be. And as far as I can tell, so is everyone else, nobody has a clear, consistent answer to the question. But let’s take a stab at it. gets a little bit more ‘exhausted’. In the limit, it will simply kill itself. PembrolizumabOf course all the ones we talked about earlier: A greater number of T-cells are in the lymphatic system, so more opportunity to prevent exhaustion.Dendritic cells are more aggressively collecting cancer antigens, so more opportunity for T cells to be activated.The lymphatic system is more permissible to dendritic cell entry, allowing more interactions between dendritic cells and T-cells.Perhaps the second take is genuinely true and answers the story entirely. Lots of immunologically useful things are going on in the morning, each contributing a little bit. As is often the case in biology, there is no singular causal factor for why early-morning immunotherapy seems to help so much, just many small things. But let’s veer off into speculation. Maybe we are missing something?Perhaps we’re being overoptimistic on this idea of ‘steady state circulating antibodies’ being useful for T-cell activationpaperit appears that challenging the immune system with an antibody at a specific time of day not only changes the quantity but also the quality of the response so that the immune system, once stimulated at the “wrong” time, may not be able to respond anymore to the same level and quality as an immune system challenged at the “right” time—just 12 h apart. Of course, many questions follow from this. What is the temporal “window of imprintability” for T cells? Does that imply that early-activated T-cell clones dominate the final pool of T-cells? And what would mechanistically cause all of this? I don’t have the answer to any of these, and I suspect nobody does. But again, maybe this is the wrong idea entirely, and there is no singular causal factor for these impressive time-of-day results. Maybe it is, once again, a bunch of small things — increased T-cell activation, but also stronger dendritic cell function and increased lymphatic vessel permissibility and many others — adding up to a strong signal.  ‘early morning immunotherapy is useful’ phenomenon are also important for infectious disease vaccines, is still ongoing for melanomathere are calls for more to be runHYGIA trialpotentially harmful territorythisthisWell, yes! We should be on guard for everything, especially since our only major piece of evidence is a as-of-yet incomplete trial. But I’m personally erring on the side of the connection between the immune system and the circadian rhythm being much stronger than it is for other physiological functions, just given how large the lymphocyte concentrations in the bloodstream can shift from night to day. I’m also betting a little on the first wave of T-cell activation being particularly important, for reasons that are still not understood. Very open to being completely wrong though!cut-off times can vary by 4-5 hours]]></content:encoded></item><item><title>Timeouts and cancellation for humans</title><link>https://vorpus.org/blog/timeouts-and-cancellation-for-humans/</link><author>/u/pkkm</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 16:09:46 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[ code might be perfect and never fail, but unfortunately the
outside world is less reliable. Sometimes, other people's programs
crash or freeze. Networks go down; printers catch on fire. Your code needs to be
prepared for this: every time you read from the network, attempt to
acquire an inter-process lock, or send an HTTP request, there are at
least three possibilities you need to think about:It might hang forever, never succeeding or failing: days pass,
leaves fall, winter comes, yet still our request waits, yearning for
a response that will never come.The first two are straightforward enough. To handle that last case,
though, you need timeouts. Pretty much every place your program
interacts with another program or person or system, it needs a
timeout, and if you don't have one, that's a latent bug.Let's be honest: if you're like most developers, your code probably
has  of bugs caused by missing timeouts. Mine certainly does.
And it's weird – since this need is so ubiqituous, and so fundamental
to doing I/O correctly, you'd think that every programming environment
would provide easy and robust ways to apply timeouts to arbitrary
operations. But... they don't. In fact, most timeout APIs are so
tedious and error-prone that it's just not practical for developers to
reliably get this right. So don't feel bad – it's not your fault your
code has all those timeout bugs, it's the fault of those I/O
libraries!But now I'm, uh, writing an I/O library. And not just any I/O library, but
one whose whole selling point is that it's obsessed with being easy to
use. So I wanted to make sure that in my library – Trio – you can
easily and reliably apply timeouts to arbitrary I/O operations. But
designing a user-friendly timeout API is a surprisingly tricky task,
so in this blog post I'm going to do a deep dive into the landscape of
possible designs – and in particular the many precursors that inspired
me – and then explain what I came up with, and why I think it's a real
improvement on the old state-of-the-art. And finally, I'll discuss how
Trio's ideas could be applied more broadly, and in particular, I'll
demonstrate a prototype implementation for good old synchronous
Python.So – what's so hard about timeout handling?The simplest and most obvious way to handle timeouts is to go through
each potentially-blocking function in your API, and give it a
 argument. In the Python standard library you'll see this
in APIs like :lock = threading.Lock()

# Wait at most 10 seconds for the lock to become available
lock.acquire(timeout=10)
If you use the  module for networking, it works the same
way, except that the timeout is set on the socket object instead of
passed to every call:sock = socket.socket()

# Set the timeout once
sock.settimeout(10)
# Wait at most 10 seconds to establish a connection to the remote host
sock.connect(...)
# Wait at most 10 seconds for data to arrive from the remote host
sock.recv(...)
This is a little more convenient than having to remember to pass in
explicit timeouts every time (and we'll discuss the convenience issue
more below) but it's important to understand that this is a purely
cosmetic change. The semantics are the same as we saw with
: each method call gets its own separate 10 second
timeout.So what's wrong with this? It seems straightforward enough. And if we
always wrote code directly against these low level APIs, then it would
probably be sufficient. But – programming is about abstraction. Say we
want to fetch a file from S3. We might do that with
boto3, using S3.Client.get_object.
What does  do? It makes a series of HTTP
requests to the S3 servers, by calling into the requests library for each one. And then each
call to  internally makes a series of calls to the
 module to do the actual network communication .From the user's point of view, these are three different APIs that
fetch data from a remote service:Sure, they're at different levels of abstraction, but the whole idea
of abstracting away such details is that the user doesn't have to
care. So if our plan is to use  arguments everywhere, then
we should expect these each to take a  argument:Now here's the problem: if this is how we're doing things, then
actually implementing these functions is a pain in the butt. Why?
Well, let's take a simplified example. When processing HTTP response,
there comes a point when we've seen the  header, and
now we need to read that many bytes to fetch the actual response body.
So somewhere inside  there's a loop like:Now we'll modify this loop to add timeout support. We want to be able
to say "I'm willing to wait at most 10 seconds to read the response
body". But we can't just pass the timeout argument through to
, because imagine the first call to  takes 6 seconds –
now for our overall operation to complete in 10 seconds, our second
 call has to be given a timeout of 4 seconds. With the
 approach, every time we pass between levels of
abstraction we need to write some annoying gunk to recalculate
timeouts:(And even this is actually simplified because we're pretending that
 takes a  argument – if you wanted to this for
real you'd have to call  before every socket method, and
then probably use some / thing to set it back or
else risk confusing some other part of your program.)In practice, nobody does this – all the higher-level Python libraries
I know of that take  arguments, just pass them through
unchanged to the lower layers. And this breaks abstraction. For
example, here are two popular Python APIs you might use today, and
they look like they take similar  arguments:But in fact these two  arguments mean totally different
things. The first one means "try to acquire the lock, but give up
after 10 seconds". The second one means "try to fetch the given URL,
but give up if at any point any individual low-level socket operation
takes more than 10 seconds". Probably the whole reason you're using
 is that you don't want to think about low-level sockets,
but sorry, you have to anyway. In fact it is currently  to guarantee that  will return in 
finite time: if a malicious or misbehaving server sends at least 1
byte every 10 seconds, then our  call above will keep
resetting its timeout over and over and never return.I don't mean to pick on  here – this problem is everywhere
in Python APIs. I'm using  as the example because Kenneth
Reitz is famous for his obsession with making its API as obvious and
intuitive as possible, and this is one of the rare places where he's
failed. I think this is the only part of the requests API that gets a
big box in the documentation warning you that it's counterintuitive.
So like... if even Kenneth Reitz can't get this right, I think we can
conclude that "just slap a  argument on it" does not lead
to APIs fit for human consumption.If  arguments don't work, what can we do instead? Well,
here's one option that some people advocate. Notice how in our
 example above, we converted the incoming relative
timeout ("10 seconds from the moment I called this function") into an
absolute deadline ("when the clock reads 12:01:34.851"), and then
converted back before each socket call. This code would get simpler if
we wrote the whole API in terms of  arguments, instead of
 arguments. This makes things simple for library
implementors, because you can just pass the deadline down your
abstraction stack:But this approach also has a downside: it succeeds in moving the
annoying bit out of the library internals, and and instead puts it on
the person using the API. At the outermost level where timeout policy
is being set, your library's users probably want to say something like
"give up after 10 seconds", and if all you take is a 
argument then they have to do the conversion by hand every time. Or
you could have every function take both  and 
arguments, but then you need some boilerplate in every function to
normalize them, raise an error if both are specified, and so forth.
Deadlines are an improvement over raw timeouts, but it feels like
there's still some missing abstraction here.Here's the missing abstraction: instead of supporting two different
arguments:we can encapsulate the timeout expiration information into an object
with a convenience constructor:That looks nice and natural for users, but since it uses an absolute
deadline internally, it's easy for library implementors too.And once we've gone this far, we might as well make things a bit more
abstract. After all, a timeout isn't the only reason you might want to
give up on some blocking operation; "give up after 10 seconds have
passed" is a special case of "give up after <some arbitrary condition
becomes true>". If you were using  to implement a web
browser, you'd want to be able to say "start fetching this URL, but
give up when the 'stop' button gets pressed". And libraries mostly
treat this  object as totally opaque in any case – they
just pass it through to lower-level calls, and trust that eventually
some low-level primitives will interpret it appropriately. So instead
of thinking of this object as encapsulating a deadline, we can start
thinking of it as encapsulating an arbitrary "should we give up now"
check. And in honor of its more abstract nature, instead of calling it
a  let's call this new thing a :So promoting the cancellation condition to a first-class object makes
our timeout API easier to use, and  makes it
dramatically more powerful: now we can handle not just timeouts, but
also arbitrary cancellations, which is a very common requirement when
writing concurrent code. (For example, it lets us express things like:
"run these two redundant requests in parallel, and as soon as one of
them finishes then cancel the other one".) This is a  idea. As
far as I know, it originally comes from Joe Duffy's cancellation
tokens
work in C#, and Go context objects are essentially the same idea.
Those folks are pretty smart! In fact, cancel tokens also solve some
other problems that show up in traditional cancellation systems.In our little tour of timeout and cancellation APIs, we started with
timeouts. If you start with cancellation instead, then there's another
common pattern you'll see in lots of systems: a method that lets you
cancel a single thread (or task, or whatever your framework uses as a
thread-equivalent), by waking it up and throwing in some kind of
exception. Examples include asyncio's Task.cancel,
Curio's Task.cancel,
pthread cancellation, Java's Thread.interrupt,
C#'s Thread.Interrupt,
and so forth. In their honor, I'll call this the "thread interrupt"
approach to cancellation.In the thread-interrupt approach, cancellation is a point-in-time
 that's directed at a : one call → one
exception in one thread/task. There are two issues here.The problem with scale is fairly obvious: if you have a single
function you'd like to call normally  you might need to cancel
it, then you have to spawn a new thread/task/whatever just for that:http_thread = spawn_new_thread(requests.get, "https://...")
# Arrange that http_thread.interrupt() will be called if someone
# clicks the stop button
stop_button.on_click = http_thread.interrupt
try:
    http_response = http_thread.wait_for_result()
except Interrupted:
    ...
Here the thread isn't being used for concurrency; it's just an awkward
way of letting you delimit the scope of the cancellation.Or, what if you have a big complicated piece of work that you want to
cancel – for example, something that internally spawns multiple worker
threads? In our example above, if  spawned some
additional backgrounds threads, they might be left hanging when we
cancel the first thread. Handling this correctly would require some
complex and delicate bookkeeping.Cancel tokens solve this problem: the work they cancel is "whatever
the token was passed into", which could be a single function, or a
complex multi-tiered set of thread pools, or anything in between.The other problem with the thread-interrupt approach is more subtle:
it treats cancellation as an . Cancel tokens, on the other
hand, model cancellation as a : they start out in the
uncancelled state, and eventually transition into the cancelled state.This is subtle, but it makes cancel tokens less error-prone. One way
to think of this is the edge-triggered/level-triggered distinction: thread-interrupt APIs provide
edge-triggered notification of cancellations, as compared to
level-triggered for cancel tokens. Edge-triggered APIs are notoriously
tricky to use. You can see an example of this in Python's
threading.Event:
even though it's called "event", it actually has an internal boolean
state; cancelling a cancel token is like setting an Event.That's all pretty abstract. Let's make it more concrete. Consider the
common pattern of using a / to make sure that a
connection is shut down properly. Here's a rather artificial example
of a function that makes a Websocket connection, sends a message, and
then makes sure to close it, regardless of whether 
raises an exception: Now suppose we start this function running, but at some point the
other side drops off the network and our  call hangs
forever. Eventually, we get tired of waiting, and cancel it.With a thread-interrupt style edge-triggered API, this causes the
 call to immediately raise an exception, and then our
connection cleanup code automatically runs. So far so good. But here's
an interesting fact about the websocket protocol: it has a "close"
message you're
supposed to send before closing the connection. In general this is a
good thing; it allows for cleaner shutdowns. So when we call
, it'll try to send this message. But... in this case,
the reason we're trying to close the connection is because we've given
up on the other side accepting any new messages. So now 
also hangs forever.If we used a cancel token, this doesn't happen:Once the cancel token is triggered, then  future operations on
that token are cancelled, so the call to  doesn't get
stuck. It's a less error-prone paradigm.It's kind of interesting how so many older APIs could get this wrong.
If you follow the path we did in this blog post, and start by thinking
about applying a timeout to a complex operation composed out of
multiple blocking calls, then it's obvious that if the first call uses
up the whole timeout budget, then any future calls should fail
immediately. Timeouts are naturally level-triggered. And then when we
generalize from timeouts to arbitrary cancellations, the insight
carries over. But if you only think about timeouts for primitive
operations then this never arises; or if you start with a generic
cancellation API and then use it to implement timeouts (like e.g.
Twisted and asyncio do), then the advantages of level-triggered
cancellation are easy to miss.So cancel tokens have really great semantics, and are certainly better
than raw timeouts or deadlines, but they still have a usability
problem: to write a function that supports cancellation, you have to
accept this boilerplate argument and then make sure to pass it on to
every subroutine you call. And remember, a correct and robust program
has to support cancellation in every function that ever does I/O,
anywhere in your stack. If you ever get lazy and leave it out, or
just forget to pass it through to any particular subroutine call, then
you have a latent bug.Humans suck at this kind of boilerplate. I mean, not you, I'm sure
you're a very diligent programmer who makes sure to implement correct
cancellation support in every function and also flosses every day.
But... perhaps some of your co-workers are not so diligent? Or maybe
you depend on some library that someone else wrote – how much do you
trust your third-party vendors to get this right? As the size of your
stack grows then the chance that everyone everywhere always gets this
right approaches zero.Can I back that up with any real examples? Well, consider this: in
both C# and Go, the most prominent languages that use this approach
and have been advocating it for a number of years, the underlying
networking primitives still do not have cancel token support.
These are like... THE fundamental operations that might hang for
reasons outside your control and that you need to be prepared to time
out or cancel, but... I guess they just haven't gotten around to
implementing it yet? Instead their socket layers support an older
mechanism for setting timeouts
or deadlines on
their socket objects, and if you want to use cancel tokens you have to
figure out how to bridge between the two different systems yourself.The Go standard library does provide one example of how to do this:
their function for establishing a network connection (basically the
equivalent of Python's ) does accept a cancel token.
Implementing this requires 40 lines of source code,
a background task, and the first try had a race condition that took a
year to be discovered in production. So... in Go if you
want to use cancel tokens (or s, in Go parlance), then I
guess that's what you need to implement every time you use any socket
operation? Good luck?I don't mean to make fun. This stuff is hard. But C# and Go are huge
projects maintained by teams of highly-skilled full-time developers
and backed by Fortune 50 companies. If they can't get it right, who
can? Not me. I'm one human trying to reinvent I/O in Python. I can't
afford to make things that complicated.Remember way back at the beginning of this post, we noted that Python
socket methods don't take individual timeout arguments, but instead
let you set the timeout once on the socket so it's implicitly passed
to every method you call? And in the section just above, we noticed
that C# and Go do pretty much the same thing? I think they're on to
something. Maybe we should accept that when you have some data that
has to be passed through to every function you call, that's something
the computer should handle, rather than making flaky humans do the
work – but in a general way that supports complex abstractions, not
just sockets.Here's how you impose a 10 second timeout on an HTTP request in Trio:But since this post is about the underlying design, we'll focus on the
primitive version. (Credit: the idea of using  blocks for
timeouts is something I first saw in Dave Beazley's Curio, though I
changed a bunch. I'll hide the details in a footnote: .)You should think of  as creating a cancel
token, but it doesn't actually expose any  object
publically. Instead, the cancel token is pushed onto an invisible
internal stack, and automatically applied to any blocking operations
called inside the  block. So  doesn't have to do
anything to pass this through – when it eventually sends and receives
data over the network, those primitive calls will automatically have
the deadline applied.When an operation is cancelled, it raises a  exception,
which is used to unwind the stack back out to the appropriate  block. Cancel scopes can be nested; 
exceptions know which scope triggered them, and will keep propagating
until they reach the corresponding  block. (As a consequence,
you should always let the Trio runtime take care of raising and
catching  exceptions, so that it can properly keep track
of these relationships.)Supporting nesting is important because some operations may want to
use timeouts internally as an implementation detail. For example, when
you ask Trio to make a TCP connection to a hostname that has multiple
IP addresses associated with it, it uses a "happy eyeballs" algorithm
to run multiple connections attempts in parallel with a staggered
start.
This requires an internal timeout
to decide when it's time to initiate the next connection attempt. But
users shouldn't have to care about that! If you want to say "try to
connect to , but give up after 10 seconds", then
that's just:And everything works; thanks to the cancel scope nesting rules, it
turns out  handles this correctly with no
additional code.Writing code that's correct in the face of cancellation can be tricky.
If a  exception were to suddenly materialize in a place
the user wasn't prepared for it – perhaps when their code was half-way
through manipulating some delicate data structure – it could corrupt
internal state and cause hard-to-track-down bugs. On the other hand, a
timeout and cancellation system doesn't do much good if you don't
notice cancellations relatively promptly. So an important challenge
for any system is to first pick a "goldilocks rule" that checks often
enough, but not too often, and then somehow communicate this rule to
users so that they can make sure their code is prepared.In Trio's case, this is pretty straightforward. We already, for other
reasons, use Python's async/await syntax to annotate blocking
functions. The main thing does is let you look at the text of any
function and immediately see which points might block waiting for
something to happen. Example:Here we can see that the call to  blocks, because it has
the special  keyword. You can't call  – or any
other of Trio's built-in blocking primitives – without using this
keyword, because they're marked as async functions. And then Python
enforces that if you want to use the  keyword, then you have
to mark the calling function as async as well, which means that all
 of  will also use the 
keyword. This makes sense, since if  calls a
blocking function, that makes it a blocking function too. In many
other systems, whether a function might block is something you can
only determine by examining all of its potential callees, and all
their callees, etc.; async/await takes this global runtime property
and makes it visible at a glance in the source code.Trio's cancel scopes then piggy-back on this system: we declare that
whenever you see an , that's a place where you might have to
handle a  exception – either because it's a call to one
of Trio's primitives which directly check for cancellation, or because
it's a call to a function that indirectly calls one of those
primitives, and thus might see a  exception come bubbling
out. This has several nice properties. It's extremely easy to explain
to users. It covers all the functions where you absolutely need
timeout/cancellation support to avoid infinite hangs – only functions
that block can get stuck blocking forever. It means that any function
that does I/O on a regular basis also automatically checks for
cancellation on a regular basis, so most of the time you don't need to
worry about this (though for the occasional long-running pure
computation, you may want to add some explicit cancellation checks by
calling  – which you have to do anyway to let
the scheduler work!). Blocking functions tend to have a large variety
of failure modes,
so in many cases any cleanup required to handle 
exceptions will be shared with that needed to handle, for example, a
misbehaving network peer. And Trio's cooperative multi-tasking system
also uses the  points to mark places where the scheduler
might switch to another task, so you already have to be careful about
leaving data structures in inconsistent states across an .
Cancellation and async/await go together like peanut butter and
chocolate.While checking for cancellation at all blocking primitive calls makes
a great default, there are some very rare cases where you want to
disable this and take explicit control over cancellation. They're so
rare that I don't have a simple example to use here (though there are
a few arcane examples in the Trio source that you can grep for if
you're really curious). To provide this escape hatch, you can set a
cancel scope to "shield" its contents from outside cancellations. It
looks like this:To support composition, shielding is sensitive to the cancel scope
stack: it only blocks outer cancel scopes from applying, and has no
effect on inner scopes. In our example above, our shield doesn't have
any affect on any cancel scopes that might be used  – those still behave normally. Which is good, because
whatever  does internally is its own private
implementation detail. And in fact, use a
cancel scope internally!
One reason that  is an attribute on cancel scopes instead of
having a special "shield scope" is that it makes it convenient to
implement this kind of nesting, because we can re-use cancel scope's
existing stack structure. The other reason is that anywhere you're
disabling external timeouts, you need to think about what you're going
to do instead to make sure things can't hang forever, and having a
cancel scope right there makes it easy to apply a new timeout that's
under the local code's control:Now if you're a Trio user please forget you read this section; if you
think you need to use shielding then you almost certainly should
rethink what you're trying to do. But if you're an I/O runtime
implementer looking to add cancel scope support, then this is an
important feature.Finally, there's one more feature of Trio that should be mentioned
here. So far in this essay, I haven't discussed concurrency much at
all; timeouts and cancellation are largely independent, and everything
above applies even to straightforward single-threaded synchronous
code. But we did make some assumptions that might seem trivial: that
if you call a function inside a  block, then (a) the execution
will actually happen inside the  block, and (b) any exceptions
it throws will propagate back to the  block so it can catch
them. Unfortunately, many threading and concurrency libraries violate
this, specifically in the case where some work is spawned or
scheduled:If we were only looking at the  block alone, this would seem
perfectly innocent. But when we look at how  is
implemented, we realize that it's likely that we'll exit the 
block before the background task finishes, so there's some ambiguity:
should the timeout apply to the background task or not? And then if it
does apply, then how should we handle the  exception? For
most system, unhandled exceptions in background threads/tasks are
simply discarded.However, these problems don't arise in Trio, because of its unique
approach to concurrency. Trio's nursery system
means that child tasks are always integrated into the call stack,
which effectively becomes a call tree. Concretely, the way this is
enforced is that Trio has no global 
primitive; instead, if you want to spawn a child task, you have to
first open a "nursery" block (for the child to live in, get it?), and then the
lifetime of that child is tied to the  block that created the
nursery:This system has many advantages, but the relevant one here is that it
preserves the key assumptions that cancel scopes rely on. Any given
nursery is either inside or outside the cancel scope – we can tell by
checking whether the  block encloses the
 block. And then it's straightforward to
say that if a nursery is inside a cancel scope, then that scope should
apply to all children in that nursery. This means that if we apply a
timeout to a function, it can't "escape" by spawning a child task –
the timeout applies to the child task too. (The exception is if you
pass an outside nursery into the function, then it can spawn tasks
into that nursery, which can escape the timeout. But then this is
obvious to the caller, because they have to provide the nursery – the
point is to make it clear what's going on, not to make it impossible
to spawn background tasks.)Returning to our initial example: I've been doing some initial work on
porting  to run on Trio (you can help!), and so far it
looks like the Trio version will not only handle timeouts better than
the traditional synchronous version, but that it will be able to do
this using  – all the places where you'd want to
check for cancellation are the ones where Trio does so automatically,
and all the places where you need special care to handle the resulting
exceptions are places where  is prepared to handle
arbitrary exceptions for other reasons.There are no free lunches; cancellation handling can still be a source
of bugs, and requires care when writing code. But Trio's cancel scopes
are dramatically easier to use – and therefore more reliable – than
any other system I've found. Hopefully we can make timeout bugs the
exception rather than the rule.So... that's great if you're using Trio. Is this something that only
works in Trio's context, or is it more general? What kind of
adaptations would need to be made to use this in other environments?If you want to implement cancel scopes, then you'll need:Some kind of implicit context-local storage to track the cancel
scope stack. If you're using threads, then thread-local storage
works; if you're using something more exotic, then you'll need to
figure out the equivalent in your system. (So for example, in Go
you'd need goroutine-local storage, which famously doesn't exist.)
This can be a bit tricky; for example in Python, we need something
like PEP 568 to iron
out some bad interactions between cancel scopes and generators.A way to delimit the boundaries of a cancel scope. Python's 
blocks work great; other options would include dedicated syntax, or
restricting cancel scopes to individual function calls like
 (though this could force
awkward factorings, and you'd need to figure out some way to expose
the cancel scope object).A strategy for unwinding the stack back to the appropriate cancel
scope after a timeout/cancellation occurs. Exceptions work great, so
long as you have a way to catch them at cancel scope boundaries –
this is another reason that Python's  blocks work so well
for this. But if your language uses, say, error code returns instead
of exceptions, then I'm sure you could build some stack unwinding
convention out of those.A story for how cancel scopes integrate with your concurrency API
(if any). Of course the ideal is something like Trio's nursery
system (which also has many other advantages, but that's a whole
'nother blog post). But even without that, you could for example
deem that any new tasks spawned inside a cancel scope inherit that
cancel scope, regardless of when they finish. (Unless they opt out
using something like the shielding feature.)Some rule to determine which operations are cancellable and
communicate that to the user. As noted above, async/await works
perfectly for this, but if you aren't using async/await then other
conventions are certainly possible. Languages with rich static type
systems might be able to exploit them somehow. Worst case you could
just be careful to document it on each function.Cancel scope integration for all of the blocking I/O primitives you
care about. This is reasonably straightforward if you're building a
system from scratch. Async systems have an advantage here because
integrating everything into an event loop already forces you to
reimplement all your I/O primitives in some uniform way, which gives
you an excellent opportunity to add uniform cancellation handling at
the same time.Our original motivating examples involved , an ordinary
synchronous library. And pretty much everything above applies equally
to synchronous or concurrent code. So I think it's interesting to
explore the idea of using these in classic synchronous Python. Maybe
we can fix  so it doesn't have to apologize for its
 argument!There are a few limitations we'll have to accept:It won't be ubiquitous – libraries will have to make sure that they
only use "scope-enabled" blocking operations. Perhaps in the long
run we could imagine this becoming part of the standard library and
integrated into all the standard primitives, but even then there
will still be third-party extension libraries that do their own I/O
without going through the standard library. On the other hand, a
library like  can be careful to only use scope-enabled
libraries, and then document that it itself is scope-enabled. (This
is perhaps the biggest advantage an async library like Trio has when
it comes to timeouts and cancellation: being async doesn't make a
difference per se, but an async library is forced to reimplement all
the basic I/O primitives to integrate them into its I/O loop; and if
you're reimplementing everything , it's easy to make
cancellation support consistent.)There's no marker like  to show which operations are
cancellable. This means that users will have to take somewhat more
care and check the documentation for individual functions – but
that's still less work then what it currently takes to make timeouts
work right.Python's underlying synchronous primitives generally only support
cancellation due to timeouts, not arbitrary events, so we probably
can't provide a  operation. But this
limitation doesn't seem too onerous, because if you have a
single-threaded synchronous program and the single thread is stuck
in some blocking operation, then who's going to call 
anyway?Summing up: it can't be quite as nice as what Trio provides, but it'd
still be pretty darn useful, and certainly nicer than what we have
now.One of the original motivations for this blog post was talking to
Yury about whether we could retrofit any
of Trio's improvements back into asyncio. Looking at asyncio through
the lens of the above analysis, a few things jump out at us:There's some impedence mismatch between the cancel scope model of
implicit stateful arbitrarily-scale cancel tokens, and asyncio's
current task-oriented, edge-triggered cancellation (and then the
s layer has a slightly different cancellation model
again), so we'd need some story for how to meld those together. Or
maybe it would be possible to migrate s to a stateful
cancellation model?Without nurseries, there's no reliable way to propagate cancellation
across tasks, and there are a lot of different operations that are
sort of like spawning a task but at a different level of abstraction
(e.g. ). You could have a rule that any new tasks
always inherit their spawner's cancel scopes, but I'm not sure
whether this would be a good idea or not – it needs some thought.Without a generic mechanism for propagating exceptions back up the
stack, there's no way to reliably route  exceptions
back to the original scope; generally asyncio simply prints and
discards unhandled exceptions from s. Maybe that's fine?Unfortunately asyncio's in a bit of a tricky position, because it's
built on an architecture derived from the previous decade of
experience with async I/O in Python... and then after that
architecture was locked in, it added new syntax to Python that
invalidated all that experience. But hopefully it's still possible to
adapt some of these lessons – at least with some compromises.If you're working in another language, I'd love to hear how the cancel
scope idea adapts – if at all. For example, it'll definitely need some
adjustment for languages that don't use exceptions, or that are
missing the kind of user-extensible syntax that Python's 
blocks provide.]]></content:encoded></item><item><title>wii-linux part 2: xorg + i3wm works</title><link>https://www.reddit.com/r/linux/comments/1l6fqn6/wiilinux_part_2_xorg_i3wm_works/</link><author>/u/trustytrojan0</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 16:05:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[since i can't crosspost with videos this is a link post to r/archwanted to share part 2 with you guys   submitted by    /u/trustytrojan0 ]]></content:encoded></item><item><title>Show HN: Let’s Bend – Open-Source Harmonica Bending Trainer</title><link>https://letsbend.de/</link><author>egdels</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 16:00:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ Let's Bend - Learn to play the harmonica and bend like a pro  For beginners,  notes on a harmonica is a big hurdle. It requires a lot of time through regular practice. By bending, even the semitones can be sounded on the harmonica through special drawing and blowing techniques, which cannot be reached by regular playing of the channels. Beginners ask themselves, "Is that bending now or is the semitone exactly hit?"  The lean and performant  app makes it easy and fun for anyone to master the art of bending. By visualising the notes you play, you'll be bending like a pro in no time.  A short video about the functionality is published here: The original idea in developing the  app was to create an application that is available on all common operating systems and whose application window scales well. It is not uncommon to have other applications open while practising or to want to use the programme on different devices. Last but not least, all keys and the most common special tunings of harmonicas should be supported. As a result, there are now two versions of the app. A desktop version for the operating systems macOS, Debian and Windows, and an Android version for mobile devices.For detailed instructions, check out our User Guide. The source code of the applications is published here. If you would like to support the developer of  by making a voluntary donation, there is a  function in the desktop version. But even here,  remains free of charge.  But enough talk now. ]]></content:encoded></item><item><title>Is linux a red flag for employers?</title><link>https://www.reddit.com/r/linux/comments/1l6e4rn/is_linux_a_red_flag_for_employers/</link><author>/u/Bassman117</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 14:57:49 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hello y’all, I got a question that’s been stuck in my head after an interview I had. I mentioned the fact that I use Linux on my main machine during an interview for a tier 2 help desk position. Their environment was full windows devices and mentioned that I run a windows vm through qemu with a gpu passed through. Through the rest of the interview they kept questioning how comfortable I am with windows.My background is 5 years of edu based environments and 1 year while working at an msp as tier 1 help desk. All jobs were fully windows based with some Mac’s. Has anyone else experience anything similar? ]]></content:encoded></item><item><title>7 years of development: discipline in software engineering</title><link>https://www.fossable.org/projects/sandpolis/7-years-of-development/</link><author>/u/fossable</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 14:25:25 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[June 7th 2025
marks 7 long years of development on
Sandpolis, an attempt to build the
ultimate remote administration/management tool for sysadmins like you and me.For 7 years I thought about and/or worked on this project almost daily, and yet
it's still nowhere near being finished, not even MVP-level. Am I the worst
software developer ever to touch a keyboard?Quite a few things happened in the last 7 years: college, a job, a wife, a
house, a child, a full rewrite from Java to Rust...... is what I would have said, but I now realize those are shallow excuses.What's slowing down this project is a critical quality that's becoming as scarce
as  in software engineering: .Discipline in the software engineering disciplineIf your project is fueled by necessity, curiosity, or excitement alone, it's
unlikely to reach 100%.That's primarily what motivated these last 7 years of development on Sandpolis.
I'd have a satisfying streak of progress in some interesting area until I got
stuck. And since solving hard problems is hard, I'd jump to somewhere else and
repeat.Unlike other crafts, in software engineering, it really is possible to build the
roof before you finish building the walls. And, as if this was Minecraft, it's
also possible to build that roof so it doesn't even eventually line up with the
walls.Nevertheless, at one point in 2019, I had an application that technically
worked. The server side was entirely Java (~50K lines) with a cute little iOS
frontend app in Swift (~10K lines). Let's admire it for a second:Back then, I thought Java was quite alright. Modern features made the language
decently comfortable. I don't remember exactly what prompted it, but at some
point, I decided to rewrite everything in Rust - probably a case of cosmic rays
in my brain, much like in my code.So, while my curiosity took me down an exciting new path with the most hyped
language of the decade, I no longer had a working application (a grievous
mistake). Rewrites are extremely costly and rarely the right answer. By that
point, I was experienced enough to know better, but I let the decision make
itself instead of taking a strategic path.Since this was just a side project, I usually worked on the fun stuff over the
hard stuff. After a while, what you're left with is a project full of holes.
Parts of it are certainly nicely done, but if it's not possible to unite those
pieces into a working whole, you can't get off the launchpad. is what unites a project into a working whole. It's what allows you
to solve the hard problems. It's what keeps you on a path going forward when
temptations arise.Surely there are some people out there with a side project that they enjoy
coding just for the sake of coding, but I always aspired to make my programs
actually do something useful. Maybe even something that no one else has ever
done before.Now that AI development tools are here to stay, discipline in software
engineering is more important than ever. It's just a classic problem reframed:Why remember directions when the GPS is always right?Why learn long division when everyone has a calculator within arms reach?Why practice handwriting when we type almost everything?The general reason in all of those cases, and likewise in software engineering,
is doing those things leads to  (sometimes of things you wouldn't
expect). Which is something that AIs don't, and maybe can't, have. is what causes you to do the things that lead to long-term
understanding, even when a shortcut is right in front of you.I'm not suggesting you uninstall copilot and stop prompting ChatGPT for code,
but I am saying it's extremely easy to mistakenly take it too far.How to practice discipline when codingIn general, do the highest priority thing until it's 100% done, even when it's
not the most enjoyable part.Don't let problems on the critical path leave your L1 cacheFor example, the data model in Sandpolis is critical for everything else to
work. When I got stuck on this hard problem early on, I diverted my attention to
other aspects that weren't as important like how user passwords are handled to
avoid hash shucking attacks.Sometimes when I'm stuck on a problem, I can work on something else in the
meantime and a solution to my original problem suddenly presents itself, as if
my unconscious brain was thinking about it the whole time. Once a problem leaves
your L1 cache because you haven't thought about it in a while, you're no longer
making progress on it. In fact, it's the opposite of progress because you slowly
start to lose context which will take effort to regain.That's why it's best to mostly stay on the critical path than to hop around.
It's OK to do some side quests here and there, but don't let them subvert the
main questline.Minimize the length of time the software is brokenTo make real improvements in software, you usually have to break things first.
The longer it takes to get your application compiling or running again, the more
costly the improvement becomes. Once a change starts taking weeks, the full
context becomes hard to keep in your L1 cache and the probability of
 increases.I'm a repeat offender when it comes to this. I've made many fundamental
improvements very low in the stack that affects basically everything, but I've
failed to propagate that change completely for a long time, usually due to some
difficult edge case that I didn't think about initially.As a corollary, don't get tempted to rewrite so easily. Rewriting is the
ultimate form of breaking your software. My favorite discussion on why rewriting
is bad is Spolsky's
Things You Should Never Do. Part I.Usually, both in software and in life, it's better to pay now rather than pay
later. It's going to cost much more time and energy to change a function
prototype after it has hundreds of call sites than when it has none or just a
few. In other words, do it right or do it again.It takes experience to predict what's going to be worth paying your time into in
the future and what will be a waste when it becomes quickly irrelevant. Of
course, it's still important to have a healthy amount of YAGNI (you ain't gonna
need it) to stay working on the things that actually matter.As an aside, you can combine "pay later" with the witty "later is never" quip to
derive: "pay never", which sounds like a sweet deal until you realize that it
turns your software into concrete over time - impossible to change without a
jackhammer.For the rest of the year, I'm going to focus on perfecting the data model in
Sandpolis. When I accomplish that, I'll have momentum to move onto other (more
interesting) features.Look forward to a very different 8th year anniversary post!]]></content:encoded></item><item><title>Why aren&apos;t people talking about AppArmor and SELinux in the age of AI?</title><link>https://www.reddit.com/r/linux/comments/1l6ddqu/why_arent_people_talking_about_apparmor_and/</link><author>/u/Bartmr</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 14:24:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Currently, AI bots and software, like Cursor and MCPs like Github, can read all of your home directory (including cookies and access tokens in your browser) to give you code suggestions or act on integrations like email and documents. Not only that, these AI tools rely heavily on dozens of new libraries that haven't been properly vetted and whose contributors are picked on the spot. Cursor does not even hide the fact that its tools may start wondering around. These MCP servers are also more prone to remote code execution, since they are impossible to have 100% hard limits. Why aren't people talking more about how AppArmor or SELinux can isolate these AI applications, like mobile phones do today? ]]></content:encoded></item><item><title>Jordan Petridis: An update on the X11 GNOME Session Removal</title><link>https://blogs.gnome.org/alatiera/2025/06/08/the-x11-session-removal/</link><author>/u/marcthe12</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 14:10:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[A year and a half ago, shortly after the GNOME 45 release, I opened a pair of Pull Requests to deprecate and remove the X11 Session.A lot has happened since. The GNOME 48 release addressed all the remaining blocking issues, mainly accessibility regressions, but it was too late in the development cycle to drop the session as well.We went ahead and disabled the X11 session by default and from now on it needs to be explicitly enabled when building the affected modules. (gnome-session, GDM, mutter/gnome-shell). This does not affect XWayland, it’s only about the X11/Xorg session and related functionality. GDM’s ability to launch other X11 sessions will be also preserved.Usually we release a single Alpha snapshot, but this time we have released earlier snapshots (49.alpha.0), 3 weeks ahead of the normal schedule, to gather as much feedback and testing as possible. (There will be another snapshot along the complete GNOME 49 Alpha release).If you are a distributor, please try to not change the default or at least let us (or me directly) know why you’d need to still ship the X11 session.As I mentioned in the tracking issue ticket, there 3 possible scenarios.The most likely scenario is that all the X11 session code stays disabled by default for 49 with a planned removal for GNOME 50.The ideal scenario is that everything is perfect, there are no more issues and bugs, we can go ahead and drop all the code before GNOME 49.beta.And the very unlikely scenario is that we discover some deal-breaking issue, revert the changes and postpone the whole thing.Having gathered feedback from our distribution partners, it now depends entirely on how well the early testing will go and what bugs will be uncovered.You can test GNOME OS Nightly with all the changes today. We found a couple minor issues but everything is fixed in the alpha.0 snapshot. Given how smooth things are going so far I believe there is a high likely-hood there won’t be any further issues and we might be able to proceed with the Ideal scenario.TLDR: The X11 session for GNOME 49 will be disabled by default and it’s scheduled for removal, either during this development cycle or more likely during the next one (GNOME 50). There are release snapshots of 49.alpha.0 for some modules already available. Go and try them out!Happy Pride month and Free Palestine ✊]]></content:encoded></item><item><title>Probably Faster Than You Can Count: Scalable Log Search with Probabilistic Techniques · Vega Security Blog</title><link>https://blog.vega.io/posts/probabilistic_techniques/</link><author>/u/Duckuks</author><category>dev</category><category>reddit</category><pubDate>Sun, 8 Jun 2025 13:07:13 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Imagine you want to build a system that needs to search through petabytes of log data, with new logs streaming in at multiple terabytes per day. Using traditional data structures and exact algorithms it’s hard to keep up with the pressure of such scale. Database indices grow unwieldy, memory requirements explode, and query times stretch from milliseconds to minutes or even hours. When working at this scale, the pursuit of 100% precision can become your worst enemy.Following up on our exploration of log search engines in “Search Logs Faster than Sonic”, it’s time to introduce a class of solutions that isn’t very common in the standard software engineer’s toolbox but shines best at extreme scale: probabilistic data structures and approximation algorithms.These tools aren’t just a part of theoretical computer science. They’re working behind the scenes in systems you likely use every day. Redis, ElasticSearch, ClickHouse rely on them to optimize lookups and provide estimations in queries that would otherwise crash the servers or take forever to complete.The basic idea is simple, there is a trade-off between accuracy and performance. Sometimes a small compromise on accuracy can results in massive performance gains while still producing a sufficient result. Instead of keeping track of everything exactly (which gets expensive fast), these structures / algorithms maintain a good-enough approximation that requires far less memory and processing time. It’s like estimating the number of rubber ducks I have in my collection instead of counting each one – you might be off by a few, but you’ll get a good-enough answer fast, without searching for the ones my cats have “sharded” across the apartment.Let’s explore how these techniques can help process massive amounts of logs without breaking your infrastructure budget.The Challenge of Data Sharding 
    When working with massive datasets, high-scale systems often split data into smaller, more manageable horizontal partition of data called shards.When you want to query this data, you need to know which shards contain relevant information. Otherwise, you’re forced to read from all of them leading to many expensive io operations whether the shards should be read from disk or over network (e.g. from s3).The simplest pruning approach is time-based filtering. Each shard tracks its minimum and maximum timestamps:Shard_1: 2023-01-01T00:00:00Z to 2023-01-01T06:00:00Z
Shard_2: 2023-01-01T03:00:00Z to 2023-01-01T09:00:00Z
Shard_3: 2023-01-01T06:00:00Z to 2023-01-01T12:00:00Z
...
When a query comes in requesting data for a specific timeframe:@Table
| where timestamp > '2023-01-01T07:00:00Z'
We can immediately eliminate  from consideration.
This concept is widely used, for example elasticsearch organizes data into time-based indices and shards within those indices, ClickHouse partitions tables by date ranges and S3-based data lakes organize files into prefixes and time-based partitions.But what about other filter conditions? Consider this simple query:@Table
| where source.ip = "192.168.1.1" AND timestamp > '2023-01-01T07:00:00Z'
Time-based pruning helps with the timestamp condition, but we still need to check all remaining shards for the specific IP.A naive approach might be to maintain an exact index of all values for each field using a hashmap. The shard can be skipped if the filtered value isn’t present:Shard_2 contains:
  source.ip: {"192.168.1.1", "10.0.0.1", ... 10,000s more IPs}
The problem is for high-cardinality fields like user IDs, request paths or if you’re really unlucky some uuid as storing and checking complete value lists consumes enormous amounts of memory and processing time.A Bloom filter solves this by providing a memory efficient way to answer a simple question: “Could this value exist in this shard?” It can tell you with certainty when something is NOT in the dataset (no false-negative), while occasionally producing false positives.You can think of Bloom filters like trying to guess what your coworker is heating up in the office microwave just by the smell, so you know if it’s worth asking for a bite.
Smells carry less information than the full dish, but if you recognize the scent of leftover fried chicken, you can usually make a decent guess.
The problem is that scents can overlap so you might think it’s fried chicken, but it’s actually reheated chicken nuggets 😕 (that’s a false positive).
But if none of the familiar smells are present, you know for sure it’s not what you’re hoping for (no false negatives).Here’s how a Bloom filter works:Start with a bit array of  bits, all initially set to 0Choose  different hash functions (scents) that each map an input to one of the  array positionsTo add an element, run it through all  hash functions to get  array positions, then set all those positions to 1To check if an element exists, run it through all  hash functions to get  positions
If ALL positions contain a 1, the element is PROBABLY in the set (it could be a false positive due to hash collisions)Otherwise, the element is DEFINITELY not in the set.What I like about Bloom filters is that both adding and searching are done in a time-complexity which doesn’t depend on the data, it depends solely on the number of chosen hash function  which of-course affect the false positive rate.
So you can control the trade-off between memory usage and false positive rate!
The probability of a false positive is approximately:$$
p ≈ (1 - e^(\frac{-kn}{m}))^k
$$ is the size of the bit array is the number of elements in the set is the number of hash functionsSo for our use case, for each shard and each “relevant” field (we’ll touch on when to avoid Bloom filters later on) in the table’s schema, we can maintain a separate Bloom filter that tracks all values for that field in that shard.
This lets us quickly eliminate shards that definitely don’t contain our target values.So let’s say you estimate a particular field will have  in a shard of data and you’re willing to retrieve shards without relevant data (false positives) at a rate of \(1\%\).
You would need approximately:$$
m = -\frac{n \cdot \ln(p)}{(\ln 2)^2}
= -\frac{1000 \cdot \ln(0.01)}{(\ln 2)^2}
\approx 9585 \text{ Bits} \approx 1198 \text{ Bytes} \approx 1.17 \text{ KB}
$$And you would need approximately:
$$
k = \frac{m}{n} \cdot \ln 2
= \frac{9585}{1000} \cdot \ln 2
\approx 6.64 = 7 \text{ hash functions}
$$The point is that this is dramatically more space-efficient than storing the complete set of elements.
Here’s a simple implementation:As I mentioned you can find them everywhere, for example:Elasticsearch is based on Apache Lucene search which uses Bloom filters in engine for efficient term lookups.Cassandra uses Bloom filters to avoid checking every SSTable data file for the partition being requested.ClickHouse uses Bloom filters them to skip indexes.Loki uses Bloom filters to accelerate queries by skipping irrelevant logs as well.When Bloom Filters Fall Short 
    Bloom filters shine when you’re looking for something specific and rare, the classic “needle in a haystack” scenario. But they quickly lose their edge when that needle becomes a recurring pattern.A classic example is multi-tenancy. When handling logs from many tenants, it’s common to have a  field. In that case most queries if not all will filter on a specific :@AuthLogs
| where tenant_name = 'ducks-corp'
...
As mentioned earlier, shards are often partitioned by time ranges so that we could skip irrelevant data when filtering by timestamp. The problem is that logs from many tenants are usually mixed together across time so their logs are likely to show up in almost every shard. That means a Bloom filter on  will be pretty useless as it will return “maybe” for almost every shard and we’ll still need to scan all of them.The  example is a pretty extreme case, let’s take a proper example, say you’re hunting for activity related to a single user “ducker”@AuthLogs
| where actor.username == "ducker" and timestamp > ago(7d)
You’re in a large organization:1TB worth of data is ingested per day.Authentication logs make up about 5% of the total → 50 GB/day.Each log entry averages around 1 KB → roughly 50 million  entries per day.Each shard contains about 1 million entries → 50 shards per day.Now assuming our suspect  appears in just  of the logs, that’s 10,000 logs total per day.
Note that  may be a power IT user that is shared across many people or a user that is being used by some automation.
If the data is  then each shard has 200 matching entries. Under a , the chance of a shard having zero matches is:
$$
P(\text{no match}) = (1 - 0.0002)^{1,000,000} \approx 1.35^{-87}
$$
In both cases, Bloom filters mark every shard as a “maybe”, offering no pruning.
It’s important to note that although having large shards have their benefits, the larger the shard the more likely that even low-frequency value will appear at least once. So basically it will be much harder for Bloom filters to prune any shard…So now we understand that Bloom filters are optimized for infrequent matches. When the match rate is high, the bit array becomes saturated.A More General Rule of Thumb
Bloom filters become ineffective when:The value you’re searching for is not rare so it appears frequently across many shards.Each shard is  that even rare terms still appear often.The field being filtered has , e.g. categorical field like  or .So before reaching for a Bloom filter, consider: how rare is the thing you’re looking for? If the answer is “not very” you may just be wasting CPU cycles hashing your way to scanning most of the shards anyway…Alternative Approach: Data Partitioning 
    A simple solution for fields that are too common for Bloom filters is to partition your data by the values of those fields. Instead of probabilistic filtering, you group data by field values into separate shards.Going back to our  example, partitioned shards would look like:Shard_1: tenant=ducks-corp, 2023-01-01T00:00:00Z to 2023-01-01T06:00:00Z
Shard_2: tenant=ducks-inc , 2023-01-01T00:00:00Z to 2023-01-01T06:00:00Z
...
Now when you query | where tenant_name == "ducks-inc", the system only needs to scan shards tagged with . It can skip everything else no probabilistic guessing needed.This approach works best for  fields with a small, fixed number of possible values like tenant names, regions, or event types. Partitioning by high-cardinality fields like user IDs or UUIDs would create too many tiny shards, making the search operation inefficient (we will probably cover shard merging in a future post).Beyond Membership: What Else Can We Prune? 
    Here’s a challenge: what about the following query which a Bloom filters can’t handle at all?@AuthLogs
| where FailedAttempts > 10
Think about it for a moment. Bloom filters are designed for exact membership testing (“is X in the set?”), but this query asks “show me all of the logs with a value greater than 10.” How would you skip irrelevant shards?Hint: Just like Bloom filters, you would need to store some metadata about the numeric values in a shard.The answer: for each numeric field, store the  range:Shard_1: FailedAttempts: min=0, max=5
Shard_2: FailedAttempts: min=3, max=15
Shard_3: FailedAttempts: min=12, max=25
Now  can immediately skip Shard_1 (max=5), while  can skip Shard_3 (min=12).Here’s another puzzle, what about this query?@AuthLogs
| where UserAgent contains "Chrome/91"
How would you efficiently skip shards that definitely don’t contain that substring? Bloom filters work for exact matches, but substring searches are trickier…Throughout our examples, we’ve made an important assumption that’s worth calling out: . Once written, they don’t change. This assumption breaks down when you need to update or delete data, which brings us to our next topic.Cuckoo Filters: When Elements Need to Leave the Nest 
    Bloom filters have one big limitation: they don’t forget. Once you add an element, you can’t remove it, because different elements might “share” the same bits. Clearing bits for one element could accidentally wipe out another leading to .
One workaround is to use a , which maintains a counter for each bit position rather than a single bit. When adding an element, you increment the counters; when removing, you decrement them. An element exists if all its positions have counts greater than zero. But this comes at a cost, as each position now requires multiple bits to store the counter.That’s where Cuckoo filters come in as a more elegant alternative, named after the cuckoo bird’s charming habit of tossing other birds eggs out of the nest.
Unlike Bloom filters, which use a bit array, Cuckoo filters use a fixed-size hash table to store fingerprints: small, fixed-size representations of the original items. Each fingerprint has two possible “homes” in the table, determined by hash functions. When both are full, the filter evicts an existing fingerprint to its alternate location, just like the cuckoo evicts its nest-mates, and repeats this process until it finds space.Instead of a bit array, Cuckoo filters use a fixed-size hash table that stores short “fingerprints”, which are small hashes derived from the inserted values. These fingerprints are much shorter than the original items, which helps save space. Each fingerprint has two possible positions in the table, chosen using two different hash functions. If both positions are already occupied, the filter selects one of the existing fingerprints, evicts it (just like the cuckoo evicts its nest-mates) and moves it to its alternate location. If that spot is also full, the process continues by evicting again until an empty slot is found or the filter gives up after a fixed number of attempts.Because each fingerprint is tied to a specific spot, deletion is possible by simply removing it the fingerprint if you find it in one of the expected slots.Deletion of elements (ideal for expiring old data)Lower false positive rates compared to Bloom filtersComparable or better space efficiencyThe trade-off? potentially slower insertions due to the evictions logic and slightly slower lookup.Typically for security monitoring purposes you might need to answer questions like:“How many unique IP addresses attempted to authenticate to our VPN in the last 24 hours?”@VPNLogs
| where timestamp > ago(24h)
| summarize unique_ips = dcount(source_ip)
“How many distinct hosts communicated with domains on our watchlist this week?”@DNSLogs
| where timestamp > ago(7d) and query.domain in (<watchlist_domains>)
| summarize unique_hosts = dcount(source_host)
“How many different user accounts accessed our internal data-sensitive database this month?”@DBLogs
| where timestamp > ago(30d) and db_name == "sensitive_data_db"
| summarize unique_users = dcount(actor.username)
These seem like simple questions, but at scale, they become challenging.
The naive approach to counting unique items is straightforward, collect items into a set and return the size:The problem with this approach is that the memory requirements grow linearly with the number of unique elements. In a large scale data system, we can expect millions of unique IP addresses, hundreds of thousands of unique user accounts, and tens of thousands of unique hostnames. So you need to keep track of all of them, plus apart from the size of the raw data there is a significant overhead from the hash-set data structure itself.The real problem isn’t just the memory for a single count. In practice, you’re running dozens of these queries simultaneously:Different time windows (hourly, daily, weekly, monthly)Different log sources (VPN, auth, DNS, network traffic)Different groupings (by region, department, risk level)What seemed like a simple counting problem quickly consumes gigabytes of memory.Finally distributing exact counting across multiple machines requires coordination to avoid double-counting elements which can be tricky as well.Enter HyperLogLog++: Counting Without Remembering 
    HyperLogLog++ solves this using a different approach. Instead of “remembering” every element, it tries to estimate how many unique elements there are using the statistical properties of hash functions. The estimates are pretty accurate while using a tiny, fixed amount of memory.The high-level idea is hashing each element and looking for rare patterns in the binary representation. The rarer the pattern you’ve observed, the more elements you’ve likely processed.Think of it like estimating the population of a city by sampling random people and asking where they were born. If you ask 100 people and find that the most remote birthplace is someone from a tiny village 500 miles away, you can infer that the city probably has a pretty large population. The logic behind it is that the odds of randomly finding someone from such a remote place is low unless there are many people to sample from.
Another classic analogy is coin flips: if someone tells you they flipped 5 heads in a row, you might guess they’ve done around 32 flips total, since the probability of getting 5 consecutive heads is about \(\frac{1}{32}\). The longer the streak of heads, the more flips they’ve likely made.HyperLogLog works similarly but with binary patterns. Here’s the intuition:Hash everything consistently: Every element gets run through a hash function, giving us a random-looking binary stringCount leading zeros: Look at how many zeros appear at the start of each hashTrack the maximum: Keep track of the longest run of leading zeros you’ve ever seenEstimate from extremes: The longer the maximum run of zeros, the more unique elements you’ve probably processedSo similar to the coin flip analogy, if you’ve seen a hash starting ith 5 zeros  it safe to assume you’ve processed roughly \(2^5 = 32\) different elements since the probability of any single hash starting with 5 zero is about \(\frac{1}{32}\). This of course only works if your hash function produces uniformly random bits so each bit position should be 0 or 1 with equal probability, independent of the input data or other bit positions just like coin flips.You’re probably thinking now that relying on a single “maximum” doesn’t sound like a good idea, just like I thought when I first read about it. You might get lucky and see a very rare pattern early, leading to a massive overestimate, or unlucky and never see rare patterns, leading to underestimation. HyperLogLog++ addresses this problem by using multiple independent estimates and combining them to get a much more stable result.The HyperLogLog++ Algorithm 
    Instead of keeping one maximum, HyperLogLog++ maintains many buckets, each tracking the maximum leading zeros for a subset of elements. This provides multiple independent estimates that can be averaged for better accuracy.
Here’s how it actually works: using a good hash function Use the first  bits to choose a bucket ( total buckets), and count leading zeros in the . For example for the hash  and , we split it as  so the bucket index is 10 ( in binary) and we count 2 leading zeros in the remaining part. If this is the longest run of zeros seen for this bucket, update it Combine all bucket values using harmonic mean and bias correctionThe formula for the  of a set of \(n\) positive real numbers \(x_1, x_2, \dots, x_n\) is:$$
H = \frac{n}{\sum_{i=1}^{n} \frac{1}{x_i}}
$$Why use harmonic mean when estimating the count?
Each bucket value represents the maximum leading zeros observed, which corresponds to an estimated count of \({2^{buckets}}\) elements. Say you have 4 buckets with values \([2, 2, 2, 6]\), representing estimated counts of \([4, 4, 4, 64]\) elements respectively.Using arithmetic mean: \(\frac{4 + 4 + 4 + 64}{4} = 19\)Using harmonic mean: \(\frac{4}{\frac{1}{4} + \frac{1}{4} + \frac{1}{4} + \frac{1}{64}} \approx 5.1\)As you can see the harmonic mean is much less sensitive to that one outlier bucket that got lucky with a rare pattern, giving a more stable estimate.The actual formula the algorithm use is:$$
\frac{\alpha \cdot m^2}{\sum 2^{-{buckets}}}
$$Based on the harmonic mean but adds:An extra factor of \(m\) (so \(m^2\) instead of m) - to scale from “average per bucket” to “total count”The \(\alpha\) constant - used to correct mathematical biases in the harmonic mean estimation and its value depends on the number of buckets.So for the 4 buckets from the example before with an \(\alpha = 0.568\) will actually get \(\frac{0.568 \times 4^2}{\frac{1}{2^1} + \frac{1}{2^1} + \frac{1}{2^1} + \frac{1}{2^8}} \approx 11.9\) total elements.Note: there’s no predefined alpha for 4 buckets as using HLL with such a small number is not supported in the original algorithmThis raw estimate has systematic biases, especially when most buckets are still empty (value 0). HyperLogLog++ detects this and switches to a more accurate method for small datasets, plus uses pre-computed correction tables to fix predictable errors across different cardinality ranges.
    HyperLogLog with 1,024 buckets estimating 1,000 unique elements. Each bucket represents the maximum number of leading zeros + 1 seen. This "lucky" run achieved 0.2% error, showing how bucket values distribute across the hash space. Try playing with this online calculatorHere’s a simplified rust implementation:Choosing the Right Precision 
    For most applications, \(4,096\) buckets (\(2^{12}\)) hit the sweet spot of good accuracy with minimal memory overhead. You can play with different configurations using this HyperLogLog calculator which also has a nice visualization.To see how significant the memory reduction can be, here’s an example: Say you’re tracking 1 million unique users from authentication logs each username is 10 characters long on average.Using HLL++ with 4,096 buckets requires approximately 32KB of memory. According to , the standard error of the cardinality can be calculated using:
$$
\text{SE} \approx \frac{1.04}{\sqrt{m}} \rightarrow \frac{1.04}{\sqrt{4096}} \approx 0.01625
$$
An error of \(1.625\%\) which in our example is \(\pm 16,250\), it means the estimated cardinality will most likely fall between 983,750 and 1,016,250.Now let’s write a small Rust program to see how much memory we would need to store 1 million unique usernames each 10 characters long using a hash-set for exact count:Now let’s see how much memory that actually takes with :The measurement shows 93.4 MB total memory usage. This includes overhead from String allocations, HashSet internal structure, and the format! macro. While the code could obviously be optimized, that’s a \(\frac{93.4 * 1024^2}{32 * 1024} = 2988.8\)x memory reduction for a small accuracy loss – a trade-off worth taking for most applications.When HyperLogLog++ Stumbles 
    HyperLogLog++ has some important limitations worth knowing:: For datasets with fewer than ~100 unique elements, the probabilistic nature introduces more error than it’s worth. A simple hash set would be more accurate and use similar memory.: In distributed systems, you often need to combine cardinality estimates from multiple sources. While you can merge HyperLogLog++ structures (by taking the maximum value for each bucket), the error accumulates with each merge operation.: Unlike exact approaches, you can’t ask “have I seen element X before?”. You can only get total counts (making it unsuitable for deduplication tasks).: HyperLogLog++ assumes your hash function produces truly random bits. If your data has patterns that survive hashing (like sequential IDs), accuracy can suffer. This is rare with good hash functions, but good to know.This algorithm is the basis for cardinality estimation for most search engines for example:Further Reading on HyperLogLog:We’ve explored how probabilistic data structures like Bloom filters and HyperLogLog++ can be used for shard pruning and cardinality estimation in large-scale log processing systems, trading small amounts of accuracy for massive gains in memory efficiency and query performance.If you’re interested in learning more about probabilistic structures, here are some more useful ones: Count-Min Sketches estimate item frequencies, MinHash enables fast set similarity, and Quantile Sketches provide accurate percentile calculations. We may explore them in future posts.Probabilistic structures are just one part of building a scalable log search system. We’ve already looked at query planning and optimization in distributed search in our blog post “Hidden Complexities of Distributed SQL”. Future posts will cover other critical challenges like high-throughput indexing for real-time ingestion, shard merging strategies to improve search efficiency by minimizing number of shards queried, tokenization and indexing design choices for different search capabilities, and distributed query coordination. All essential for systems that process terabytes of logs every day.]]></content:encoded></item><item><title>New linter: cmplint</title><link>https://github.com/fillmore-labs/cmplint</link><author>/u/___oe</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 8 Jun 2025 13:00:18 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[ is a Go linter (static analysis tool) that detects comparisons against the address of newly created values, such as  or . These comparisons are almost always incorrect, as each expression creates a unique allocation at runtime, usually yielding false or undefined results. _, err := url.Parse("://example.com") // ❌ This will always be false - &url.Error{} creates a unique address. if errors.Is(err, &url.Error{}) { log.Fatal("Cannot parse URL") } // ✅ Correct approach: var urlErr *url.Error if errors.As(err, &urlErr) { log.Fatalf("Cannot parse URL: %v", urlErr) } Also, it detects errors like: defer func() { err := recover() if err, ok := err.(error); ok && // ❌ Undefined behavior. errors.Is(err, &runtime.PanicNilError{}) { log.Print("panic called with nil argument") } }() panic(nil) which are harder to catch, since they actually pass tests. See also the blog post and  tool for a deep-dive.Pull request for golangci-lint here, let's see whether this is a linter or a “detector”.]]></content:encoded></item><item><title>Binfmtc – binfmt_misc C scripting interface</title><link>https://www.netfort.gr.jp/~dancer/software/binfmtc.html.en</link><author>todsacerdoti</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 12:38:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Introducing the binfmt_misc C scripting interface
      "I love C". 
      "I enjoy writing in C".
      "I don't feel well when I have passed a day without coding a line of C". 
      "My conversation with my wife simply doesn't flow without the C language". 
      "I want to write everything in C, even the everyday scripting, 
      but due to the steps required for writing make files and other things,
      I tend to choose interpreted languages".
      A good news for the C programmers suffering from these symptoms.
      binfmtc is a hack to allow you to use the C language for cases where 
      script languages such as perl and shell were the languages of choice.
    
      Also included is a "real csh" as an example, allowing you to use the 
      C language style for executing everyday sysadmin work.
      Please experience the real C shell, which has a slightly different 
      tint to the original C shell we've been used to for more than 10 years.
    
      Examples of execution in C, assembly, and C++.
      The last entry is the one for real csh.
    
      Simply add a magic keyword 
      and add execution permission to the C-script.
      Every time you invoke the script, the compiler will compile and 
      execute the program for you.
    
      For sid add the following line to /etc/apt/sources.list
      and do .
    deb http://www.netfort.gr.jp/~dancer/tmp/20050523 ./
    
      By registering magic through Linux binfmt_misc, 
      binfmtc-interpreter will be invoked every time a C script is invoked.
      binfmtc-interpreter will parse the specified script file, 
      and will invoke gcc with the required options, and compile 
      to a temporary binary,
      and invoke the binary.
    
      Do you actually find it, ... useful?
    $Id: binfmtc.html.en,v 1.11 2006/04/16 03:03:16 dancer Exp $]]></content:encoded></item><item><title>Increase storage on nodes</title><link>https://www.reddit.com/r/kubernetes/comments/1l6alc7/increase_storage_on_nodes/</link><author>/u/Tiiibo</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 8 Jun 2025 12:06:42 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I have a k3s cluster with 3 worker nodes (and 3 master nodes). Each worker node has 30G storage. I want to deploy prometheus and grafana in my cluster for monitoring. I read that 50G is recommended. even though i have 30x3, will the storage be spread or should i have 50G per node minimum? Regardless, I want to increase my storage on all nodes. I deployed my nodes via terraform. can i just increase the storage value number or will this cause issues? How should I approach this, whats the best solution? Downtime is not an issue since its just a homelab, i just dont want to break my entire setup]]></content:encoded></item><item><title>Migrations with mongoDB</title><link>https://www.reddit.com/r/golang/comments/1l69ncb/migrations_with_mongodb/</link><author>/u/ParanoidPath</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 8 Jun 2025 11:09:58 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[do you handle migrations with mongo? if so, how? I dont see that great material for it on the web except for one or two medium articles.]]></content:encoded></item><item><title>How setup crosscompiling for Windows using MacOS and Windows SDK</title><link>https://www.reddit.com/r/golang/comments/1l67dzz/how_setup_crosscompiling_for_windows_using_macos/</link><author>/u/pepiks</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 8 Jun 2025 08:36:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I tried crosscompile for Windows on MacOS , but I don't have headers file like windows.h. I need to this Windows SDK, but official version is bundle for Windows (executable file):What I found is NET 9.0 and NET 8.0 LTS for MacOS, but I am not sure this will be correct as Windows can use WinAPI, it is somehow evolved in UWP and NET framework is behemot itself which are few way to create app for Windows.I am not sure which one is correct to get working crosscompiling on my laptop for Windows machine using MacOS.The simplest solution is using Windows, but as I work on 3 platforms (Windows, MacOS, Linux) depending on what I am currently doing is not convient.]]></content:encoded></item><item><title>Gaussian integration is cool</title><link>https://rohangautam.github.io/blog/chebyshev_gauss/</link><author>beansbeansbeans</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 08:35:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>What can be done about the unoptimized kube-system workloads in GKE?</title><link>https://www.reddit.com/r/kubernetes/comments/1l674os/what_can_be_done_about_the_unoptimized_kubesystem/</link><author>/u/rcrgkbe</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 8 Jun 2025 08:18:00 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/rcrgkbe ]]></content:encoded></item><item><title>The last six months in LLMs, illustrated by pelicans on bicycles</title><link>https://simonwillison.net/2025/Jun/6/six-months-in-llms/</link><author>swyx</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 07:38:37 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[#Also in March, OpenAI launched the "GPT-4o  native multimodal image generation’ feature they had been promising us for a year.This was one of the most successful product launches of all time. They signed up 100 million new user accounts in a week! They had a single hour where they signed up a million new accounts, as this thing kept on going viral again and again and again.I took a photo of my dog, Cleo, and told it to dress her in a pelican costume, obviously.But look at what it did—it added a big, ugly sign in the background saying Half Moon Bay.I didn’t ask for that. My artistic vision has been completely compromised!This was my first encounter with ChatGPT’s new memory feature, where it consults pieces of your previous conversation history without you asking it to.I told it off and it gave me the pelican dog costume that I really wanted.But this was a warning that we risk losing control of the context.As a power user of these tools, I want to stay in complete control of what the inputs are. Features like ChatGPT memory are taking that control away from me.I don’t like them. I turned it off.]]></content:encoded></item><item><title>FAA to eliminate floppy disks used in air traffic control systems</title><link>https://www.tomshardware.com/pc-components/storage/the-faa-seeks-to-eliminate-floppy-disk-usage-in-air-traffic-control-systems</link><author>daledavies</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 07:11:34 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The head of the Federal Aviation Administration just outlined an ambitious goal to upgrade the U.S.’s air traffic control (ATC) system and bring it into the 21st century. According to NPR, most ATC towers and other facilities today feel like they’re stuck in the 20th century, with controllers using paper strips and floppy disks to transfer data, while their computers run Windows 95. While this likely saved them from the disastrous CrowdStrike outage that had a massive global impact, their age is a major risk to the nation’s critical infrastructure, with the FAA itself saying that the current state of its hardware is unsustainable.“The whole idea is to replace the system. No more floppy disks or paper strips,” acting FAA administrator Chris Rocheleau told the House Appropriations Committee last Wednesday. Transportation Secretary Sean Duffy also said earlier this week,” This is the most important infrastructure project that we’ve had in this country for decades. Everyone agrees — this is non-partisan. Everyone knows we have to do it.” The aviation industry put up a coalition pushing for ATC modernization called Modern Skies, and it even ran an ad telling us that ATC is still using floppy disks and several older technologies to keep our skies safe.Unfortunately, upgrading the ATC system isn’t as simple as popping into your nearby Micro Center and buying the latest and greatest gaming PC. First and foremost, some systems can never be shut down because it is crucial for safety. Because of this, you can’t just switch off one site to swap out ancient components for newer ones. Aside from that, the upgrades to this critical infrastructure should be resistant to hacking and other vulnerabilities, as even a single breach could cripple the nation, costing time, money, and lives.The FAA is pouring a lot of money into maintaining its old ATC systems, as they have to keep running 24/7. Nevertheless, age will eventually catch up no matter how much repair, upkeep, or overhaul you do. Currently, the White House hasn’t said what this update will cost. The FAA has already put out a Request For Information to gather data from companies willing to take on the challenge of upgrading the entire system. It also announced several ‘Industry Days’ so companies can pitch their tech and ideas to the Transportation Department.Duffy said that the Transportation Department aims to complete the project within four years. However, industry experts say this timeline is unrealistic. No matter how long it takes, it’s high time that the FAA upgrades the U.S.’s ATC system today after decades of neglect.]]></content:encoded></item><item><title>Design a Web Crawler - System Design Interview</title><link>https://blog.algomaster.io/p/design-a-web-crawler-system-design-interview</link><author>Ashish Pratap Singh</author><category>dev</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c560e48-2f72-4659-9f69-f4269895b979_2452x1640.png" length="" type=""/><pubDate>Sun, 8 Jun 2025 04:56:03 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[A  (also known as a ) is an automated bot that systematically browses the internet, following links from page to page to discover and collect web content.Traditionally, web crawlers have been used by  to discover and index web pages. In recent years, they’ve also become essential for training large language models (LLMs) by collecting massive amounts of publicly available text data from across the internet.At its core, crawling seems simple:Start with a list of known URLs (called )However, designing a crawler that can operate at , processing billions or even trillions of pages, is anything but simple. It introduces several complex engineering challenges like:How do we prioritize which pages to crawl first?How do we ensure we don’t overload the target servers?How do we avoid redundant crawling of the same URL or content?How do we split the work across hundreds or thousands of crawler nodes?In this article, we’ll walk through the end-to-end design of a scalable, distributed web crawler. We’ll start with the requirements, map out the high-level architecture, explore database and storage options, and dive deep into the core components.Before we start drawing boxes and arrows, let's define what our crawler needs to do.1.1 Functional Requirements Given a URL, the crawler should be able to download the corresponding content. Save the fetched content for downstream use. Parse the HTML to discover hyperlinks and identify new URLs to crawl. Prevent redundant crawling and storage of the same URL or content. Both URL-level and content-level deduplication should be supported. Follow site-specific crawling rules defined in  files, including disallowed paths and crawl delays.Handle Diverse Content Types: Support HTML as a primary format, but also be capable of recognizing and handling other formats such as PDFs, XML, images, and scripts. Support recrawling of pages based on content volatility. Frequently updated pages should be revisited more often than static ones.1.2 Non-Functional Requirements The system should scale horizontally to crawl billions of pages across a large number of domains. The crawler should avoid overwhelming target servers by limiting the rate of requests to each domain. The architecture should allow for easy integration of new modules, such as custom parsers, content filters, storage backends, or processing pipelines.Robustness & Fault Tolerance: The crawler should gracefully handle failures whether it's a bad URL, a timeout, or a crashing worker node without disrupting the overall system. The crawler should maintain high throughput (pages per second), while also minimizing fetch latency.In a real system design interview, you may only be expected to address a subset of these requirements. Focus on what’s relevant to the problem you’re asked to solve, and clarify assumptions early in the discussion.2.1 Number of Pages to CrawlAssume we aim to crawl a subset of the web, not the entire internet, but a meaningful slice. This includes pages across blogs, news sites, e-commerce platforms, documentation pages, and forums.Additional Metadata (headers, timestamps, etc.): ~10 KBTotal Data Volume = 1 billion pages × 110 KB = ~110 TBThis estimate covers only the raw HTML and metadata. If we store additional data like structured metadata, embedded files, or full-text search indexes, the storage requirements could grow meaningfully.Let’s assume we want to complete the crawl in . = 1 billion / 10 ≈  ≈ 1150 pages/sec 110 KB/page × 1150 pages/sec = ~126 MB/secThis means our system must be capable of:Making over 1150 HTTP requests per secondParsing and storing content at the same rateEvery page typically contains several outbound links, many of which are unique. This causes the  (queue of URLs to visit) to grow rapidly. Average outbound links per page: 5New links discovered per second =  1150 (pages per second) * 5 = 5750The URL Frontier's needs to handle thousands of new URL submissions per second. We’ll need efficient , , and  to handle this at scale.]]></content:encoded></item><item><title>&lt;Blink&gt; and &lt;Marquee&gt; (2020)</title><link>https://danq.me/2020/11/11/blink-and-marquee/</link><author>ghssds</author><category>dev</category><category>hn</category><pubDate>Sun, 8 Jun 2025 04:17:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
              I was chatting with a fellow web developer recently and made a joke about the  and
               tags, only to discover that he had no idea what I was talking about. They’re a part of web history that’s fallen off the radar and younger developers are
              unlikely to have ever come across them. But for a little while, back in the 90s, they were a big deal.
            
              Invention of the  element is often credited to Lou Montulli, who wrote pioneering web browser Lynx before being joining Netscape in 1994. He insists that he didn’t write any
              of the code that eventually became the first implementation of . Instead, he claims: while out at a bar (on the evening he’d first meet his wife!), he
              pointed out that many of the fancy new stylistic elements the other Netscape engineers were proposing wouldn’t work in Lynx, which is a text-only browser. The fanciest conceivable
              effect that would work across both browsers would be making the text flash on and off, he joked. Then another engineer – who he doesn’t identify – pulled a late night hack session and
              added it.
            
              And so it was that when Netscape Navigator 2.0 was released in 1995 it added support for
              the  tag. Also animated  and the first inklings of JavaScript, which collectively
              would go on to  the “personal website” experience for years to come. Here’s how you’d use it:
            <BLINK>This is my blinking text!</BLINK>
              With no attributes, it was clear from the outset that this tag was supposed to be a joke. By the time  was
              published as a a recommendation two years later, it was  as being a joke. But the Web of the late 1990s
              saw it used . If you wanted somebody to notice the “latest updates” section on your personal home page, you’d wrap a  tag around the title (or,
              if you were a sadist, the entire block).
            
              In the same year as Netscape Navigator 2.0 was released, Microsoft released Internet Explorer
              2.0. At this point, Internet Explorer was still very-much playing catch-up with the features the Netscape team had implemented, but clearly some senior Microsoft engineer took a
              look at the  tag, refused to play along with the joke, but had an innovation of their own: the  tag! It had a whole suite of attributes to control the scroll direction, speed, and whether it looped or bounced backwards and forwards. While
               encouraged disgusting and inaccessible design as a joke,  did it on purpose.
            <MARQUEE>Oh my god this still works in most modern browsers!</MARQUEE>
              But here’s the interesting bit: for a while in the late 1990s, it became a somewhat common practice to wrap content that you wanted to emphasise with animation in  a
               and a  tag. That way, the Netscape users would see it flash, the  users
              would see it scroll or bounce. Like this:
            <MARQUEE><BLINK>This is my really important message!</BLINK></MARQUEE>
              The web has always been built on Postel’s Law: a web browser should assume that it won’t understand everything it reads,
              but it should provide a best-effort rendering for the benefit of its user anyway. Ever wondered why the modern  element is a block rather than a self-closing
              tag? It’s so you can embed  it code that an earlier browser – one that doesn’t understand  – can read (a browser’s default state when seeing a
              new element it doesn’t understand is to ignore it and carry on). So embedding a  in a  gave you the best of both worlds, right?
              
              Better yet, you were safe in the knowledge that anybody using a browser that didn’t understand  of these tags could . Used properly, the
              web is about . Implement for everybody, enhance for those who support the shiny features. JavaScript and  can be applied with the same rules, and doing so pays dividends in maintainability and accessibility (though, sadly, that doesn’t stop people writing
              sites that needlessly  these technologies).
            
              I remember, though, the first time I tried Netscape 7, in 2002. Netscape 7 and its close descendent are, as far as I can tell, the only web browsers to support  and . Even then, it was picky about the order in which they were presented and the elements wrapped-within them. But support was
              good enough that some people’s personal web pages suddenly began to exhibit the most ugly effect imaginable: the combination of both scrolling and flashing text.
            ]]></content:encoded></item><item><title>Bill Atkinson, Hypercard Creator and Original Mac Team Member, Dies at Age 74</title><link>https://apple.slashdot.org/story/25/06/08/016210/bill-atkinson-hypercard-creator-and-original-mac-team-member-dies-at-age-74?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 8 Jun 2025 01:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[ AppleInsider reports:

The engineer behind much of the Mac's early graphical user interfaces, QuickDraw, MacPaint, Hypercard and much more, William D. "Bill" Atkinson, died on June 5 of complications from pancreatic cancer... 
Atkinson, who built a post-Apple career as a noted nature photographer, worked at Apple from 1978 to 1990. Among his lasting contributions to Apple's computers were the invention of the menubar, the selection lasso, the "marching ants" item selection animation, and the discovery of a midpoint circle algorithm that enabled the rapid drawing of circles on-screen. 
He was Apple Employee No. 51, recruited by Steve Jobs. Atkinson was one of the 30 team members to develop the first Macintosh, but also was principle designer of the Lisa's graphical user interface (GUI), a novelty in computers at the time. He was fascinated by the concept of dithering, by which computers using dots could create nearly photographic images similar to the way newspapers printed photos. He is also credited (alongside Jobs) for the invention of RoundRects, the rounded rectangles still used in Apple's system messages, application windows, and other graphical elements on Apple products. 
Hypercard was Atkinson's main claim to fame. He built the a hypermedia approach to building applications that he once described as a "software erector set." The Hypercard technology debuted in 1987, and greatly opened up Macintosh software development.
 
In 2012 some video clips of Atkinson appeared in some rediscovered archival footage. (Original Macintosh team developer Andy Hertzfeld uploaded "snippets from interviews with members of the original Macintosh design team, recorded in October 1983 for projected TV commercials that were never used.") 

Blogger John Gruber calls Atkinson "One of the great heroes in not just Apple history, but computer history."

 If you want to cheer yourself up, go to Andy Hertzfeld's Folklore.org site and (re-)read all the entries about Atkinson. Here's just one, with Steve Jobs inspiring Atkinson to invent the roundrect. Here's another (surely near and dear to my friend Brent Simmons's heart) with this kicker of a closing line: "I'm not sure how the managers reacted to that, but I do know that after a couple more weeks, they stopped asking Bill to fill out the form, and he gladly complied." 

Some of his code and algorithms are among the most efficient and elegant ever devised. The original Macintosh team was chock full of geniuses, but Atkinson might have been the most essential to making the impossible possible under the extraordinary technical limitations of that hardware... In addition to his low-level contributions like QuickDraw, Atkinson was also the creator of MacPaint (which to this day stands as the model for bitmap image editorsâ — âPhotoshop, I would argue, was conceptually derived directly from MacPaint) and HyperCard ("inspired by a mind-expanding LSD journey in 1985"), the influence of which cannot be overstated.

 I say this with no hyperbole: Bill Atkinson may well have been the best computer programmer who ever lived. Without question, he's on the short list. What a man, what a mind, what gifts to the world he left us.]]></content:encoded></item><item><title>Joining Apple Computer (2018)</title><link>https://www.folklore.org/Joining_Apple_Computer.html</link><author>tosh</author><category>dev</category><category>hn</category><pubDate>Sat, 7 Jun 2025 20:32:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[40 years ago today, I joined Apple Computer on April 27, 1978. It was a big turning point in my life and I am glad I said "Yes".
I was working on my PhD in neuroscience with Doug Bowden at the University of Washington Regional Primate Research Center. Jef Raskin, a professor and friend from my undergraduate days at UC San Diego, called and urged me to join him at an exciting new startup called Apple Computer. 
I told him I had to finish my PhD, a required credential for researching brains and consciousness. But Jef would not take "No" for an answer, and sent me roundtrip airplane tickets with a note: "Just visit for a weekend, no strings attached." My dad lived in nearby Los Gatos so I decided to visit.
I don't know what Jef told Steve Jobs about me, but Steve spent the entire day recruiting me. He introduced me to all 30 employees at Apple Computer. They seemed intelligent and passionate, and looked like they were having fun, but that was not enough to lure me away from my graduate studies.
Toward the end of the day, Steve took me aside and told me that any hot new technology I read about was actually two years old. "There is a lag time between when someting is invented, and when it is available to the public. If you want to make a difference in the world, you have to be ahead of that lag time. Come to Apple where you can invent the future and change millions of people's lives." 
Then he gave me a visual: "Think how fun it is to surf on the front edge of a wave, and how not-fun to dog paddle on the tail edge of the same wave." That image persuaded me, and within two weeks I had quit my graduate program, moved to Silicon Valley, and was working at Apple Computer. I never finished my neuroscience degree, and my dad was mad at me for wasting ten years of college education that he helped to pay for. I was pretty nervous, but knew I had made the right choice.
Steve Jobs and I became close friends. We went for long walks at Castle Rock State Park, shared meals and wide-ranging conversations about life and design. We bounced ideas off each other. Sometimes he would start a conversation with "Here's a crazy idea...", and the idea would go back and forth and evolve into a serious discussion, or occasionally a workable design. Steve listened to me and challenged me. His support at Apple allowed me to made a difference in the world. 
I wanted to port the UCSD Pascal system to the Apple II. We needed to build software in a cumulative fashion with libraries of reusable modules, and Apple BASIC didn't even have local variables. My manager said "No", but I went over his head to Steve. Steve thought Apple users were fine with BASIC and 6502 assembly language, but since I argued so passionately, he would give me two weeks to prove him wrong. Within hours I boarded a plane to San Diego, worked like crazy for two weeks, and returned with a working UCSD Pascal System that Apple ended up using to bootstrap the Lisa development.
After the UCSD Pascal system shipped, Steve asked me to work on on Apple's new Lisa project. The Apple II had optional game paddle knobs, but software writers could not count on them because not every user had them. I convinced project manager Tom Whitney that the Lisa computer needed to include a mouse in the box so we could write software that counted on a pointing device. Otherwise a graphics editor would have to be designed to be usable with only cursor keys.
The Apple II displayed white text on a black background. I argued that to do graphics properly we had to switch to a white background like paper. It works fine to invert text when printing, but it would not work for a photo to be printed in negative. The Lisa hardware team complained the screen would flicker too much, and they would need faster refresh with more expensive RAM to prevent smearing when scrolling. Steve listened to all the pros and cons then sided with a white background for the sake of graphics.
The Lisa and Macintosh were designed with full bitmap displays. This gave tremendous flexibility in what you could draw, but at a big cost. There were a lot of pixels to set and clear anytime you wanted to draw a character, line, image, or area. I wrote the optimized assembly language QuickDraw graphics primitives that all Lisa and Macintosh applications called to write the pixels. QuickDraw performance made the bitmap display and graphical user interface practical .
To handle overlapping windows and graphics clipping, I wrote the original Lisa Window Manager. I also wrote the Lisa Event Manager and Menu Manager, and invented the pull-down menu. Andy Hertzfeld adapted these for use on the Mac, and with these and QuickDraw, my code accounted for almost two thirds of the original Macintosh ROM. 
I had fun writing the MacPaint bitmap painting program that shipped with every Mac . I learned a lot from watching Susan Kare using my early versions. MacPaint showed people how fun and creative a computer with a graphics display and a mouse could be.The portrait of Steve and me was made by Norman Seeff at Steve's home in December 1983, just before the Mac was introduced. Steve's expression looks like he is calculating how to harness this kid's energy. Some say Steve used me, but I say he harnessed and motivated me, and drew out my best creative energy. It was exciting working at Apple, knowing that whatever we invented would be used by millions of people.The image showing the Mac team is from the cover of Andy Hertzfeld's great little book, "Revolution in the Valley, The Insanely Great Story of How the Mac Was Made." You can also read these stories at Andy's website www.folklore.org.  
Inspired by a mind-expanding LSD journey in 1985, I designed the HyperCard authoring system that enabled non-programmers to make their own interactive media. HyperCard used a metaphor of stacks of cards containing graphics, text, buttons, and links that could take you to another card. The HyperTalk scripting language implemented by Dan Winkler was a gentle introduction to event-based programming. Steve Jobs wanted me to leave Apple and join him at Next, but I chose to stay with Apple to finish HyperCard. Apple published HyperCard in 1987, six years before Mosaic, the first web browser. 
I worked at Apple for 12 years, making tools to empower creative people, and helping Apple grow from 30 employees to 15,000. In 1990, with John Sculley's blessing, I left Apple with Marc Porat and Andy Hertzfeld to co-found General Magic and help to invent the personal communicator.
The road I took 40 years ago has made all the difference. I still follow research in consciousness, but I am more than satisfied with the contributions I was able to make with my years at Apple. I am grateful to Jef Raskin and Steve Jobs for believing in me and giving me the opportunity to change the world for the better.
  ]]></content:encoded></item><item><title>Self-Host and Tech Independence: The Joy of Building Your Own</title><link>https://www.ssp.sh/blog/self-host-self-independence/</link><author>articsputnik</author><category>dev</category><category>hn</category><pubDate>Sat, 7 Jun 2025 17:51:51 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[After watching the two PewDiePie videos where he learned about installing Arch (something considered quite hard, even for Linux enthusiasts) and building three products (camera for the dog, weather/drinking/meditation device, and who knows what comes next) based on open-source, 3D-printed parts, I started wondering about building things yourself, self-hosting, and tech independence. Something dear to my heart for a while.If people ask me how they should start writing or how to get a job, I always say to buy a domain first. Secondly, host your own blog website if you have the technical skills (although it’s not so hard anymore). Because all of this compounds over time. Of course, you can start with a ready-made blog and a URL not yours, but if you want to do it long term, I saw many people changing from WordPress to Medium to Substack to Ghost, so what’s next? Over that time, sometimes they didn’t migrate their long-effort blog posts but started new.Every time they had a new domain. To me, that’s so sad. Of course, you have learned a lot, and sometimes it’s also good to start new, but imagine instead if that happened over 10 years. If you compare that 10-year blog that has the same domain, keeping hard-earned backlinks, showcasing your long-term investment with old blog posts, even though they might not be as good as current ones (but doesn’t that happen all the time), what a huge difference that would be?As someone who has hosted my own stuff for quite a while, and has been adding more every year, I thought I would write a short article about it.Lately, I also went into Homelab and built my own Home Server with SSH, backup, photos, Gitea, etc. Setting up my own configuration for Reverse Proxy and SSL Certificates for my Homeserver, creating SSL certificates, setting up SSH keys to SSH into without a login—all great things you learn along the way.Initially everything seems hard, but once you know how, it’s kind of obvious and less hard. It’s also, as ThePrimeagen says, that there is always a big part of  where one tells themselves, “Oh that can’t be that hard”. But then you realize it’s much harder than you thought. But once you overcome the first hurdles, it’s really rewarding, and once working, it just works!Most of what inspires me to do more is the joy of using something you built yourself, and usually not paying for it. Maybe this is also because of the subscription hell we are living in, where every single app or service can’t be used without a subscription.When I got into vim, and especially Neovim, all of a sudden I lived in the terminal and knew some of the commands that usually only Linux wizards or nerds know, but now I am one myself :) But with great pride. Find more on my journey on my PKM Workflow Blog.Tech Independence is something I learned from Derek Sivers, and basically means that you do not depend on any particular company or software.The premise is that by learning some of the fundamentals, in this case Linux, you can host most things yourself. Not because you need to, but because you want to, and the feeling of using your own services just gives you pleasure. And you learn from it. Derek goes deep in his article. He self-hosts email, contacts & calendar, and your own backup storage. But you can start small. We always believe we just have to use what’s out there to buy, but there are other ways.Start by buying your own domain today. Put some thought into the name, but don’t overcomplicate it. If you have any success or links whatsoever, you can always move the domain later if you don’t like it (and forward existing blogs to a new domain with not much lost). But you can’t do it if you don’t have your own domain or own hosted server.Most of it is Open-Source and comes when you dabble in Linux. As the story of PewDiePie shows, once you learn Linux, you want to build everything yourself and not pay for anything 🙃.Open-source and open-source code is beautiful. It’s much more than just using someone else’s software, but it’s all the millions of people who just give away their work for free. It’s a community of people working for everyone. By putting it on GitHub, people can give feedback (issues) or contribute (Pull Requests), and you as the owner can or cannot listen to it. It’s your choice. Like in the real world.But most of all, everyone can use your code for free. Some nuances on the licensing, but if you have MIT or some other permissive License, everyone can use it.Actually, my whole writing experience started because I could use an open-source BI tool that at work we pay a huge amount of money for. That quick brew install and run locally fascinated me since then, and I haven’t let go of it. And all my writing on this blog is essentially around open-source data engineering, which is just a beautiful thing.I understand that everyone needs to make money, but in a perfect world, everyone would just work collaboratively on open-source software to make the world a better place. And for everyone to profit. Like Linux.Linux runs the world. There is almost no digital device that we use that is not running Linux or part of it. It’s amazing what Linus Torvalds created. He would probably be the richest person on earth if he had monetized it, but then again, would it be so popular? Probably not. And as he has mentioned, he is very well off now, despite not monetizing it. Isn’t that a great outcome too?Linus Torvalds did not only create Linux, but also **git. A version control tool that changed the world and any software engineer is using. But he only built it for his own needs, to version control Linux. And because he hated existing solutions back then. That makes him such a pleasant guy, although he admits he’s not a people person himself 😅.As I said, sharing what you work on, for everyone to see, will only benefit others to learn, but even more so you. As you get potential contributions or other forks that build something else on top of it.You get feedback and connecting with like-minded people. If nothing else, this is probably the most rewarding part of open-source. That you meet new people that you would have never met otherwise.I share almost all of my knowledge and code, but most of the time I use it for myself and am not really expecting contributions. Or I actively don’t encourage anyone, as it makes it harder for myself. But I want to share so others can learn from it, copy it, or just give me feedback in case I do something stupid.And this journey of sharing my knowledge so openly is just a great feeling. And also where I believe most of the trust from people comes from. If someone shares their knowledge and learning, aren’t we inclined to initially like that person? It doesn’t mean anything per se, but if you have been in need of a small software or script and you didn’t know how, and then you find a full-blown solution. In these occasions, you can’t be more thankful to the person who openly shared their code.And this person has an instant place in your heart. You don’t even need to, but you can, pay them.My Tech Stack (Thanks You!)For example, I use open-source tools for most of my online presence. For example, I’m immensely thankful for Jacky Zhao who built the Quartz, an open-source Obsidian Publish alternative that I use to this day to share my Obsidian notes. He has since moved on to a newer version, but I still use the GoHugo v3 version, but isn’t that the beauty? From now on, I manage and maintain the v3 version myself, but based on everything he built.I use GoatCounter to have anonymized stats for my sites. It does not take any hidden pixels or spy on people, but I get a very elegant way of seeing  for my websites. I’m immensely thankful to Martin Tournoij for sharing that for free and even running it for small websites.I’m using Listmonk, an open-source newsletter list, where I’m immensely thankful to Kailash Nadh who created and still maintains it for everyone who uses it. Such a simple installation and nice solution to run a simple newsletter list.And later, I wanted to automatically send an email whenever I wrote a new blog, and I’m immensely thankful to Stephan Heuel who created listmonk-rss that just does that. And he even wrote the most helpful documentation so that it worked for my blog, setting up GitHub Actions on the first try.These are just a few of My Tech Stack that I use, and I am immensely thankful for any of these. That’s why I find it’s only fair to share what I am building in the open too, so everyone else can profit too.There are many more tools, especially if you are into Homelabs; there are a plethora of apps that you can just install. Some of which I use and have installed on my Homelab and playing around with:: Digital document management system that scans, indexes, and organizes your physical documents with OCR and tagging capabilities: Self-hosted Google Photos alternative with AI-powered face recognition, automatic tagging, and privacy-focused photo management: Network-wide ad blocker that acts as a DNS sinkhole to block advertisements and tracking domains across all devices on your network: Web-based reverse proxy management tool with SSL certificate automation and easy domain routing for self-hosted services: Self-hosted audiobook and podcast server with mobile apps, progress tracking, and library management features: Comprehensive e-book management suite for organizing, converting, and serving your digital library with web-based reading interface: Decentralized file synchronization tool that keeps folders in sync across multiple devices without cloud dependencies: Lightweight, self-hosted Git service with web interface, issue tracking, and collaboration tools for code repositoriesBtw, I just bought a cheap and old client server and refurbished it for my homelab at home. You don’t need to spend a huge amount of money to buy the latest and shiniest server. Usually you can do a lot with old hardware and running a great operating system on it.As you might have noticed by now, not only do you get a lot of value out of it, but it also takes some work. But to me, that’s where I get my joy. One of my principles and things I like to do most over anything else is learning. And what is a better way to learn than building something you can actually use?Besides, you also get lots of . That’s why Derek calls it tech independence, because you are not depending on the big players such as Google, Apple, and others to implement your features or tweak them to your needs. You also don’t get a heart attack if Google turns off your favorite app such as Google Inbox and many others I loved but got cut off. Or if they simply raise the price.I hope you enjoyed my little rant. There’s much more to be said, but for now, that’s it. Check my dotfiles to see any of my tools or Linux tools I use, check out my free blogs on data engineering, my second brain where I share more than 1000 notes, interconnected, or my book, that I’m writing in the open and releasing chapter by chapter as I go.One common denominator that I have noticed for a while, besides software running on Linux, is that open-source or content sharing is running on Markdown. As all written content on GitHub or on all of my websites and content, even the newsletter (that’s why I have chosen Listmonk), is based on Markdown. Meaning no converting formatting from one editor’s Rich Text to another (e.g., check out Markdown vs Rich Text if that interests you), or find anything else on my Website or GitHub.Thanks for reading this far. And have a great day. If you enjoyed it, I would love to discuss or hear your experience on Bluesky.]]></content:encoded></item><item><title>Ask Slashdot: How Important Is It For Programmers to Learn Touch Typing?</title><link>https://ask.slashdot.org/story/25/06/07/0811223/ask-slashdot-how-important-is-it-for-programmers-to-learn-touch-typing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sat, 7 Jun 2025 17:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[Once upon a time, long-time Slashdot reader tgibson learned how to type on a manual typewriter, back in an 8th grade classroom. 
And to this day, they write, "my bias is to nod approvingly at touch typists and roll my eyes at those who need to stare at the keyboard while typing..." But how true is that for computer professionals today?

After 15 years I left industry and became a post-secondary computer science educator. Occasionally I rant to my students about the importance of touch-typing as a skill to have as a software engineer. 

But I've been out of the game for some time now. Those of you hiring or working with freshly-minted software engineers, what's your take? 

One anonymous Slashdot reader responded:

Oh, you mean the kid in the next cubicle that has said "Hey Siri" 297 times this morning? I'll let you know when he starts typing. A minor suggestion to office managers... please purchase a very quiet keyboard. Fellow cube-mates who are accomplished typists would consider that struggling audibly to be akin to nails on a blackboard... 
Share your own thoughts in the comments. 

How important is it for programmers to learn touch typing?]]></content:encoded></item><item><title>My experiment living in a tent in Hong Kong&apos;s jungle</title><link>https://corentin.trebaol.com/Blog/8.+The+Homelessness+Experiment</link><author>5mv2</author><category>dev</category><category>hn</category><pubDate>Sat, 7 Jun 2025 16:40:09 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>&apos;For Algorithms, a Little Memory Outweighs a Lot of Time&apos;</title><link>https://developers.slashdot.org/story/25/06/07/0714256/for-algorithms-a-little-memory-outweighs-a-lot-of-time?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sat, 7 Jun 2025 16:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[MIT comp-sci professor Ryan Williams suspected that a small amount of memory "would be as helpful as a lot of time in all conceivable computations..." writes Quanta magazine. 

"In February, he finally posted his proof online, to widespread acclaim..."


Every algorithm takes some time to run, and requires some space to store data while it's running. Until now, the only known algorithms for accomplishing certain tasks required an amount of space roughly proportional to their runtime, and researchers had long assumed there's no way to do better. Williams' proof established a mathematical procedure for transforming any algorithm — no matter what it does — into a form that uses much less space. 
What's more, this result — a statement about what you can compute given a certain amount of space — also implies a second result, about what you cannot compute in a certain amount of time. This second result isn't surprising in itself: Researchers expected it to be true, but they had no idea how to prove it. Williams' solution, based on his sweeping first result, feels almost cartoonishly excessive, akin to proving a suspected murderer guilty by establishing an ironclad alibi for everyone else on the planet. It could also offer a new way to attack one of the oldest open problems in computer science. 

"It's a pretty stunning result, and a massive advance," said Paul Beame, a computer scientist at the University of Washington.
 


Thanks to long-time Slashdot reader mspohr for sharing the article.
]]></content:encoded></item><item><title>Washington Post&apos;s Privacy Tip: Stop Using Chrome, Delete Meta Apps (and Yandex)</title><link>https://tech.slashdot.org/story/25/06/07/035249/washington-posts-privacy-tip-stop-using-chrome-delete-metas-apps-and-yandex</link><author>miles</author><category>dev</category><category>hn</category><pubDate>Sat, 7 Jun 2025 16:33:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
			
		 	
				Meta's Facebook and Instagram apps "were siphoning people's data through a digital back door for months," writes a Washington Post tech columnist, citing researchers who found no privacy setting could've stopped what Meta and Yandex were doing, since those two companies "circumvented privacy and security protections that Google set up for Android devices.

"But their tactics underscored some privacy vulnerabilities in web browsers or apps. These steps can reduce your risks."

Stop using the Chrome browser. Mozilla's Firefox, the Brave browser and DuckDuckGo's browser block many common methods of tracking you from site to site. Chrome, the most popular web browser, does not... For iPhone and Mac folks, Safari also has strong privacy protections. It's not perfect, though.  No browser protections are foolproof. The researchers said Firefox on Android devices was partly susceptible to the data harvesting tactics they identified, in addition to Chrome. (DuckDuckGo and Brave largely did block the tactics, the researchers said....)Delete Meta and Yandex apps on your phone, if you have them. The tactics described by the European researchers showed that Meta and Yandex are unworthy of your trust. (Yandex is not popular in the United States.)    It might be wise to delete their apps, which give the companies more latitude to collect information that websites generally cannot easily obtain, including your approximate location, your phone's battery level and what other devices, like an Xbox, are connected to your home WiFi.

Know, too, that even if you don't have Meta apps on your phone, and even if you don't use Facebook or Instagram at all, Meta might still harvest information on your activity across the web.]]></content:encoded></item><item><title>Bill Atkinson has died</title><link>https://daringfireball.net/linked/2025/06/07/bill-atkinson-rip</link><author>romanhn</author><category>dev</category><category>hn</category><pubDate>Sat, 7 Jun 2025 16:19:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[From his family, on Atkinson’s Facebook page:We regret to write that our beloved husband, father, and
stepfather Bill Atkinson passed away on the night of Thursday,
June 5th, 2025, due to pancreatic cancer. He was at home in
Portola Valley in his bed, surrounded by family. We will miss him
greatly, and he will be missed by many of you, too. He was a
remarkable person, and the world will be forever different because
he lived in it. He was fascinated by consciousness, and as he has
passed on to a different level of consciousness, we wish him a
journey as meaningful as the one it has been to have him in our
lives. He is survived by his wife, two daughters, stepson,
stepdaughter, two brothers, four sisters, and dog, Poppy.One of the great heroes in not just Apple history, but computer history. If you want to cheer yourself up, go to Andy Hertzfeld’s Folklore.org site and (re-)read all the entries about Atkinson. Here’s just one, with Steve Jobs inspiring Atkinson to invent the roundrect. Here’s another (surely near and dear to my friend Brent Simmons’s heart) with this kicker of a closing line: “I’m not sure how the managers reacted to that, but I do know that after a couple more weeks, they stopped asking Bill to fill out the form, and he gladly complied.”Some of his code and algorithms are among the most efficient and elegant ever devised. The original Macintosh team was chock full of geniuses, but Atkinson might have been the most essential to making the impossible possible under the extraordinary technical limitations of that hardware. Atkinson’s genius dithering algorithm was my inspiration for the name of Dithering, my podcast with Ben Thompson. I find that effect beautiful and love that it continues to prove useful, like on the Playdate and apps like BitCam.I say this with no hyperbole: Bill Atkinson may well have been the best computer programmer who ever lived. Without question, he’s on the short list. What a man, what a mind, what gifts to the world he left us.]]></content:encoded></item></channel></rss>